<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"/><meta name="theme-color" content="#222"/><meta http-equiv="X-UA-COMPATIBLE" content="IE=edge,chrome=1"/><meta name="renderer" content="webkit"/><link rel="icon" type="image/ico" sizes="32x32" href="/assets/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"/><link rel="alternate" href="/rss.xml" title="何必要叹气呢？" type="application/rss+xml"><link rel="alternate" href="/atom.xml" title="何必要叹气呢？" type="application/atom+xml"><link rel="alternate" type="application/json" title="何必要叹气呢？" href="https://www.htired.top/feed.json"/><link rel="preconnect" href="https://s4.zstatic.net"/><link rel="preconnect" href="https://at.alicdn.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish:400,400italic,700,700italic%7CFredericka%20the%20Great:400,400italic,700,700italic%7CNoto%20Serif%20JP:400,400italic,700,700italic%7CNoto%20Serif%20SC:400,400italic,700,700italic%7CInconsolata:400,400italic,700,700italic&display=swap&subset=latin,latin-ext" media="none" onload="this.media&#x3D;&#39;all&#39;"><link rel="stylesheet" href="/css/app.css?v=0.4.11"><link rel="modulepreload" href="/js/chunk-6AROBX2M.js"></link><link rel="modulepreload" href="/js/chunk-CDFWGKAZ.js"></link><link rel="modulepreload" href="/js/chunk-NGTPZFEZ.js"></link><link rel="modulepreload" href="/js/chunk-T5KHDXD7.js"></link><link rel="modulepreload" href="/js/chunk-UCNSAKJE.js"></link><link rel="modulepreload" href="/js/chunk-WIQECBEN.js"></link><link rel="modulepreload" href="/js/comments-YMZC2DEK.js"></link><link rel="modulepreload" href="/js/copy-tex-5ZQCB5LC.js"></link><link rel="modulepreload" href="/js/index.esm-UQTDDMFT.js"></link><link rel="modulepreload" href="/js/post-GXBO3POO.js"></link><link rel="modulepreload" href="/js/quicklink-Q3LCUN34.js"></link><link rel="modulepreload" href="/js/search-HYBAVWN7.js"></link><link rel="modulepreload" href="/js/siteInit.js"></link><link rel="modulepreload" href="/js/waline-2AUA5EA7.js"></link><link rel="stylesheet" href="/css/comments-SJZAII77.css" media="none" onload="this.media&#x3D;&#39;all&#39;"></link><link rel="stylesheet" href="/css/siteInit.css" media="none" onload="this.media&#x3D;&#39;all&#39;"></link><link rel="stylesheet" href="/css/waline-ZFRMIHOE.css" media="none" onload="this.media&#x3D;&#39;all&#39;"></link><link rel="preload" href="https://github.com/htired/MyPic/blob/main/img/6c1bc1deb312449195a8c6a802e916c1.jpg?raw=true" as="image" fetchpriority="high"><link rel="preload" href="https://github.com/htired/MyPic/blob/main/img/81d1da7e1ba142629967fea196c73e0e.jpg?raw=true" as="image" fetchpriority="high"><link rel="preload" href="https://github.com/htired/MyPic/blob/main/img/t1.png?raw=true" as="image" fetchpriority="high"><link rel="preload" href="https://raw.githubusercontent.com/htired/MyPic/main/img/1b87e2c5880511ebb6edd017c2d2eca2.png" as="image" fetchpriority="high"><link rel="preload" href="https://github.com/htired/MyPic/blob/main/img/792a4b6934fc491b81342206e3192e59.png?raw=true" as="image" fetchpriority="high"><link rel="preload" href="https://github.com/htired/MyPic/blob/main/img/4fc551ee880a11ebb6edd017c2d2eca2.jpg?raw=true" as="image" fetchpriority="high"><meta name="keywords" content="深度学习"/><meta name="description" content="送君南浦，伤如之何？"/><link rel="canonical" href="https://www.htired.top/2024/09/14/deeplearning/deeplearning/"><title>deeplearning</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">deeplearning</h1><div class="meta"><span class="item" title="创建时间：2024-09-14 12:11:06"><span class="icon"><i class="ic i-calendar"></i></span><span class="text">发表于</span><time itemprop="dateCreated datePublished" datetime="2024-09-14T12:11:06+08:00">2024-09-14</time></span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i></span><span class="text">本文字数</span><span>165k</span><span class="text">字</span></span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i></span><span class="text">阅读时长</span><span>2:30</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span><span class="line"></span><span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Htired Love</a></li></ul><ul class="right" id="rightNav"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div class="pjax" id="imgs"><ul><li class="item" style="background-image: url(&quot;https://github.com/htired/MyPic/blob/main/img/6c1bc1deb312449195a8c6a802e916c1.jpg?raw=true&quot;);"></li><li class="item" style="background-image: url(&quot;https://github.com/htired/MyPic/blob/main/img/81d1da7e1ba142629967fea196c73e0e.jpg?raw=true&quot;);"></li><li class="item" style="background-image: url(&quot;https://github.com/htired/MyPic/blob/main/img/t1.png?raw=true&quot;);"></li><li class="item" style="background-image: url(&quot;https://raw.githubusercontent.com/htired/MyPic/main/img/1b87e2c5880511ebb6edd017c2d2eca2.png&quot;);"></li><li class="item" style="background-image: url(&quot;https://github.com/htired/MyPic/blob/main/img/792a4b6934fc491b81342206e3192e59.png?raw=true&quot;);"></li><li class="item" style="background-image: url(&quot;https://github.com/htired/MyPic/blob/main/img/4fc551ee880a11ebb6edd017c2d2eca2.jpg?raw=true&quot;);"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"></use><use xlink:href="#gentle-wave" x="48" y="3"></use><use xlink:href="#gentle-wave" x="48" y="5"></use><use xlink:href="#gentle-wave" x="48" y="7"></use></g></svg></div><main><div class="inner"><div class="pjax" id="main"><div class="article wrap"><div class="breadcrumb" itemListElement itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i><span><a href="/">首页</a></span><i class="ic i-angle-right"></i><span class="current" itemprop="itemListElement" itemscope="itemscope" itemtype="https://schema.org/ListItem"><a href="/categories/deeplearning/" itemprop="item" rel="index" title="分类于深度学习"><span itemprop="name">深度学习<meta itemprop="position" content="0"/></span></a></span></div><article class="post block" itemscope="itemscope" itemtype="http://schema.org/Article" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.htired.top/2024/09/14/deeplearning/deeplearning/"/><span hidden="hidden" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><meta itemprop="image" content="/assets/avatar.jpg"/><meta itemprop="name" content="htired"/><meta itemprop="description" content="路还很长，这不是最终的结果, 送君南浦，伤如之何？"/></span><span hidden="hidden" itemprop="publisher" itemscope="itemscope" itemtype="http://schema.org/Organization"><meta itemprop="name" content="何必要叹气呢？"/></span><div class="body md" itemprop="articleBody"><h1 id="深度学习概论"><a class="anchor" href="#深度学习概论">#</a> 深度学习概论</h1>
<h2 id="什么是神经网络"><a class="anchor" href="#什么是神经网络">#</a> 什么是神经网络</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405101502162.png?lastModify=1726287112" alt="image-20240405101502162" /></p>
<ol>
<li>右边的网路实现了左边这个函数的功能</li>
<li>神经元就是输入面积，完成线性运算，取不小于 0 的值，最后得到输出预测价格</li>
<li>x：输入。y：输出（预测的价格）</li>
</ol>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405102143493.png?lastModify=1726287112" alt="image-20240405102143493" /></p>
<ol>
<li>只需输入 x，所有的中间过程 它都会自己完成，预测 y</li>
</ol>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405102445942.png?lastModify=1726287112" alt="image-20240405102445942" /></p>
<ol>
<li>中间三个神经元，在一个神经网络中被叫做 “隐藏单元”，输入都同时来自四个特征</li>
<li>神经元自己定义名称，只给四个输入特征随便怎么计算。</li>
<li>神经网络非常善于计算从 x 到 y 的精准映射函数</li>
</ol>
<h2 id="用神经网络进行监督学习"><a class="anchor" href="#用神经网络进行监督学习">#</a> 用神经网络进行监督学习</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405103903771.png?lastModify=1726287112" alt="image-20240405103903771" /></p>
<ul>
<li>1，2 经常使用的是标准神经网络</li>
<li>3 经常使用的是卷积神经网络（CNN）</li>
<li>4，5，对于序列数据，经常使用 RNN（循环神经网络）</li>
<li>6 混合的神经网络结构</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405104347994.png?lastModify=1726287112" alt="image-20240405104347994" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405104710669.png?lastModify=1726287112" alt="image-20240405104710669" /></p>
<h2 id="深度学习为什么会兴起"><a class="anchor" href="#深度学习为什么会兴起">#</a> 深度学习为什么会兴起</h2>
<h3 id="原因一"><a class="anchor" href="#原因一">#</a> 原因一</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405105058692.png?lastModify=1726287112" alt="image-20240405105058692" /></p>
<p>想达到性能更好的效果，两个条件</p>
<ul>
<li>需要训练一个规模足够大的神经网络，以发挥数据规模量巨大的优点</li>
<li>需要很多的数据量</li>
</ul>
<blockquote>
<p>x 轴为带标签的数据：Amount of label data</p>
<p>在训练样本时候，有输入 x 和标签 y：(x, y)</p>
<p>符号约定：m（训练集的规模）</p>
<hr />
<p>训练集不大的一快，各种算法的相对排名并不是很确定：效果取决于手工设计的组件，会决定最终的表现</p>
<p>最终的性能，更多取决于手工设计组件的技能，以及算法处理方面的一些细节</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405110213875.png?lastModify=1726287112" alt="image-20240405110213875" /></p>
<ul>
<li>
<p>使用 sigmoid 函数的机器学习的问题是：在箭头所指的方向，sigmoid 函数的斜率 梯度会接近 0，所以学习会变得非常缓慢</p>
<blockquote>
<p>因为用梯度下降法时，梯度接近 0 时候参数会变化的很慢，学习也会变得很慢</p>
</blockquote>
</li>
</ul>
<p>通过改变激活函数，神经网络用这个函数：修正线性单元（ReLU）</p>
<blockquote>
<p>它的梯度对于所有正值的输入，输出都是 1，因此梯度不会逐渐趋向 0。</p>
<p>而在左边为 0</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405110644317.png?lastModify=1726287112" alt="image-20240405110644317" /></p>
<p>转换。最终算法创新所带来的影响是增加计算速度。</p>
<h3 id="原因二"><a class="anchor" href="#原因二">#</a> 原因二</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405111001233.png?lastModify=1726287112" alt="image-20240405111001233" /></p>
<p>反复重复，迭代愈来愈快</p>
<h2 id="课程大纲"><a class="anchor" href="#课程大纲">#</a> 课程大纲</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240405111432402.png?lastModify=1726287112" alt="image-20240405111432402" /></p>
<h1 id="神经网络基础"><a class="anchor" href="#神经网络基础">#</a> 神经网络基础</h1>
<h2 id="二分分类"><a class="anchor" href="#二分分类">#</a> 二分分类</h2>
<h3 id="概念"><a class="anchor" href="#概念">#</a> 概念</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240411091241653.png?lastModify=1726287112" alt="image-20240411091241653" /></p>
<p>计算机保存一张图片，要保存三个独立矩阵</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240411091312578.png?lastModify=1726287112" alt="image-20240411091312578" /></p>
<ul>
<li>若输入图片是 64×64 像素的，就有三个 64×64 的矩阵</li>
</ul>
<p>要把这些像素亮度值放入到一个特征向量中，就要把这些像素值都提出来放入一个特征向量 <code>x</code></p>
<ul>
<li>用 1 列  n 行矩阵表示 255,231....（提取像素值） 即可</li>
</ul>
<p>若图片是 64×64 那么向量  <code>x</code>  的总维度 n=nx = 64×64×3</p>
<p>他以图片的特征向量  <code>x</code>  作为输入，预测输出的结果标签  <code>y</code>  是 1 还是 0</p>
<h3 id="符号"><a class="anchor" href="#符号">#</a> 符号</h3>
<ul>
<li>
<p>(x,y) 表示一个独立的样本</p>
</li>
<li>
<p><code>x</code>  是 nx 维的特征向量：x∈Rnx</p>
</li>
<li 0,1="">
<p>标签  <code>y</code>  值为 0 或 1：y∈</p>
</li>
<li>
<p>训练集由 m 个训练样本组成</p>
</li>
<li>
<p>(x (1),y (1)) 表示样本一的输入和输出</p>
<blockquote>
<p>(x (1),y (1)),(x (2),y (2))...... 表示整个训练集</p>
</blockquote>
</li>
<li>
<p>有时为了强调：mtrain 为训练样本的个数，mtest 为测试集的样本数</p>
</li>
<li>
<p>矩阵 X 的定义训练样本作为行向量堆叠，而不是列向量堆叠</p>
<blockquote>
<p>构建神经网络时用左边这个约定形式会让构建过程简单很多</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240411093744770.png?lastModify=1726287112" alt="image-20240411093744770" /></p>
<p>当使用 Python 实现的时候：X.shape 用来输出矩阵的维度 即 (nx,m)</p>
</blockquote>
</li>
<li>
<p>矩阵 Y 是 1×m 的矩阵</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240411094023083.png?lastModify=1726287112" alt="image-20240411094023083" /></p>
<p>Y.shape 用来输出矩阵的维度 即 (1,m)</p>
</blockquote>
</li>
</ul>
<h2 id="logistic-回归"><a class="anchor" href="#logistic-回归">#</a> logistic 回归</h2>
<p>这是一个学习算法，用在监督学习问题中输出  <code>y</code>  标签是 <strong>0 或 1</strong>。这是一个二元分类问题</p>
<p>已知的输入特征向量 x 可能是一张图，希望把识别出这是不是猫图，需要一个算法，可以给出一个预测值 y^</p>
<blockquote>
<p>y^ 就是对 y 的预测：y^ 就是一个概率，当输入特征 x 满足条件时 y 就是 1。即：y^=P (y=1|x)</p>
<p>​	也就是若 x 是图片，y^ 告诉你这是一张猫图的概率</p>
</blockquote>
<p>x∈Rnx，已知 Logistic 回归的参数是 w∈Rnx，b∈R。如何计算输出预测 y^</p>
<ul>
<li>
<p>可以这样试：y^=wTx+b，输入 x 的线性函数</p>
<blockquote>
<p>事实上，若做线性回归就是这么算的</p>
<p>这不是个非常好的二元分类算法，因为希望 y^ 是 y=1 的概率，所以 y^ 应该介于 0 和 1 之间</p>
<p>实际上很难实现，因为 wTx+b 可能比 1 大很多，或者甚至是负值，这样的概率是没有意义的</p>
</blockquote>
</li>
</ul>
<p>在 Logistic 回归中，输出变成 y^=σ(wTx+b)：等于 sigmoid 函数作用到这个量上</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240411100705495.png?lastModify=1726287112" alt="image-20240411100705495" /></p>
<blockquote>
<p>σ(z)=11+e−z</p>
<p>z→+∞,σ→1</p>
<p>z→−∞,σ→0</p>
</blockquote>
<p>当实现 Logistic 回归时，学习参数 w 和 b，所以 y^ 变成了对 y=1 概率的比较好的估计</p>
<blockquote>
<p>b 对应一个拦截器</p>
</blockquote>
<h2 id="logistic-回归损失函数"><a class="anchor" href="#logistic-回归损失函数">#</a> logistic 回归损失函数</h2>
<p>为了训练 logistic 回归模型的参数 w 以及 b，需要定义一个 cost 函数</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240411104749757.png?lastModify=1726287112" alt="image-20240411104749757" /></p>
<p>为了让方程更加的详细，需要说明上面定义的 y^  是对一个训练样本 x 来说的，对于每个训练样本使用带有圆括号的上标，方便引用说明，区分样本</p>
<blockquote>
<p>训练样本 (i) 对应的预测值是 y^(i)=σ(wTx (i)+b)</p>
<p>σ(z(i))=11+e−z(i)</p>
<p>用 (i) 指明数据表示 x 或者 y 或者某个和第 i 个训练样本有关</p>
</blockquote>
<h4 id="损失函数"><a class="anchor" href="#损失函数">#</a> 损失函数</h4>
<p>Loss (error) function: L(y<sup>,y)=12(y</sup>−y)2</p>
<p>损失函数用来衡量 y^ 和 y 的实际值有多接近，但用上述函数，梯度下降法则不太好用</p>
<p>Loss (error) function: L(y<sup>,y)=−(ylogy</sup>+(1−y)log(1−y^))</p>
<blockquote>
<p>若 y=1，为了让 L (y^,y) 尽可能小：L (y<sup>,y)=−logy</sup>，也就是 y^ 尽可能大：y^→1</p>
<p>若 y=0，为了让 L (y^,y) 尽可能小：L (y<sup>,y)=−log(1−y</sup>)，也就是 y^ 尽可能小：y^→0</p>
</blockquote>
<p>损失函数只适用这样的单个训练样本</p>
<h4 id="成本函数"><a class="anchor" href="#成本函数">#</a> 成本函数</h4>
<p>Cost function: 衡量的是在全体训练样本上的表现</p>
<blockquote>
<p>J(w,b)=1mΣi=1m⁡L(y<sup>(i),y(i))=−1mΣi=1m⁡[y(i)logy</sup>(i)+(1−y(i))log(1−y^(i))])</p>
</blockquote>
<p>成本函数基于参数的总成本</p>
<p>所以需要找到合适的成本使得 w,b 尽可能小</p>
<h2 id="梯度下降法"><a class="anchor" href="#梯度下降法">#</a> 梯度下降法</h2>
<p>使用梯度下降法来训练或学习训练集上的参数 w,b</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413093054654.png?lastModify=1726287112" alt="image-20240413093054654" /></p>
<blockquote>
<p>梯度下降法的形象化说明：w 可以是更高的维度，但是为了更好地绘图，我们定义 w 和 b，都是单一实数。</p>
<p>所做的就是找到<strong>使得代价函数（成本函数）J (w,b) 函数值是最小值，对应的参数 w 和 b。</strong></p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413093451190.png?lastModify=1726287112" alt="image-20240413093451190" /></p>
<blockquote>
<p>不断地迭代下降，直到走到全局最优解或者接近全局最优解的地方</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413093528300.png?lastModify=1726287112" alt="image-20240413093528300" /></p>
</blockquote>
<p>假定成本函数只有一个参数 w，即用一维曲线代替多维曲线，这样 可以更好画出图像。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413093850281.png?lastModify=1726287112" alt="image-20240413093850281" /></p>
<p>迭代公式：w:=w−αdJ (w) dw</p>
<blockquote>
<p>:= 表示更新参数</p>
<p>学习率 α 可以控制每一次迭代或者梯度下降法的步长</p>
<p>dw=dJ (w) dw 表示对参数 w 的更新或者变化量（dw 作为导数的变量名）</p>
</blockquote>
<p>如下图：由于 dw&gt;0，每一次迭代后 w 更新后会逐步减少（也就是往左边走），直至逼近最小值点</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413094426020.png?lastModify=1726287112" alt="image-20240413094426020" /></p>
</blockquote>
<p>如下图：由于 dw&lt;0，每一次迭代后 w 更新后会逐步增大（也就是往右边走），直至逼近最小值点</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413094524860.png?lastModify=1726287112" alt="image-20240413094524860" /></p>
</blockquote>
<p>总结：无论初始化的位置是在左边还是在右边，梯度下降法会朝着全局最小值方向移动</p>
<p>对于：J (w,b)</p>
<blockquote>
<p>w:=w−α∂J(w,b)∂w</p>
<p>b:=b−α∂J(w,b)∂b</p>
<p>dw=∂J(w,b)∂w</p>
<p>db=∂J(w,b)∂b</p>
</blockquote>
<h2 id="计算图"><a class="anchor" href="#计算图">#</a> 计算图</h2>
<p>一个神经网络的计算，都是 <a href="logistic.ipynb">logistic.ipynb</a> 按照前向或反向传播过程组织的。</p>
<ul>
<li>
<p>首先计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。</p>
<blockquote>
<p>反向操作：用来计算出对应的梯度或导数。</p>
</blockquote>
</li>
</ul>
<p>计算图解释了为什么我们用这种方式组织这些计算过程。</p>
<p>例如：J (a,b,c)=3 (a+bc)=3 (5+3×2)=33</p>
<blockquote>
<p>u=bc</p>
<p>v=a+u</p>
<p>J=3v</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413103847826.png?lastModify=1726287112" alt="image-20240413103847826" /></p>
<p>若需要计算导数，则从右向左（蓝色）的过程是用于计算导数<strong>最自然的方式</strong>。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413104205386.png?lastModify=1726287112" alt="image-20240413104205386" /></p>
</blockquote>
<h2 id="使用计算图求导"><a class="anchor" href="#使用计算图求导">#</a> 使用计算图求导</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240413103847826.png?lastModify=1726287112" alt="image-20240413103847826" /></p>
<p>dJdv=?=3</p>
<blockquote>
<p>J=3v</p>
<p>v=11→11.001</p>
<p>J=33→33.003</p>
<p>最终结果是 J 的增量上升了 v 的增量的三倍</p>
</blockquote>
<p>如果你想计算最后输出变量的导数，使用你最关心的变量对 v 的导数，那么我们就做完了一步反向传播，在这个流程图中是一个反向步</p>
<p>dJda=3=dJdvdvda</p>
<blockquote>
<p>a=5→5.001</p>
<p>v=11→11.001</p>
<p>J=33→33.003</p>
</blockquote>
<p>如果你改变了 a，那么也会改变 v，通过改变 v，也会改变 J</p>
<ul>
<li>a 增加了，v 也会增加，取决于 dvda，然后 v 的变化同理会导致 J 增加</li>
</ul>
<p>如下图表示了如何计算，dJdv 就是 J 对变量 v 的导数，它可以帮助你计算 dJda，所以这是另一步反向传播计算。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414151900105.png?lastModify=1726287112" alt="image-20240414151900105" /></p>
<blockquote>
<p>符号约定：在实现反向传播时候，通常会有一个<strong>最终输出值</strong>是你要关心的，最终的输出变量  <code>FinalOutputVar</code> ，你真正想要关心或者说优化的。</p>
<ul>
<li>在上述情况是 J=FinalOutputVar</li>
</ul>
<p>所以有很多计算尝试计算输出变量的导数：</p>
<ul>
<li>&quot;dvar&quot;（程序）或 &quot;dJdvar&quot; 表示 dFinalOutputVardvar</li>
</ul>
<blockquote>
<p>例如：dJda=da</p>
</blockquote>
</blockquote>
<p>dJdu=3=dJdv3dvdu1</p>
<blockquote>
<p>u=6→6.001</p>
<p>v=11→11.001</p>
<ul>
<li>dudv=1，因为 u 增加 0.001 时，v 会增加 0.001</li>
</ul>
<p>J=33→33.003</p>
<p>首先 u 会影响 v，然后 v 会影响 J</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414152407927.png?lastModify=1726287112" alt="image-20240414152407927" /></p>
<p>dJdb=dJdu3dudb2=6</p>
<blockquote>
<p>b=3→3.001</p>
<p>u=bc=6→6.002</p>
<p>注：当 u 增加 0.002 时，J 增加 0.006，因为：dJdu=3</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414153048628.png?lastModify=1726287112" alt="image-20240414153048628" /></p>
<p>即：计算所有这些导数时，最有效率的办法是从右到左计算，跟着这个蓝色箭头走。</p>
<ul>
<li>特别是当我们第一次计算对 v 的导数时，之后在计算对 a 导数就可以用到。</li>
<li>然后对 u 的导数，比如说 dJdu，可以帮助计算对 b 的导数，然后对 c 的导数。</li>
</ul>
<h2 id="logistic-回归中的梯度下降法"><a class="anchor" href="#logistic-回归中的梯度下降法">#</a> logistic 回归中的梯度下降法</h2>
<p>怎样通过计算偏导数来实现逻辑回归的梯度下降算法</p>
<p><code>logistic</code>  回归中的算法</p>
<p>z=wTx+b</p>
<p>y^=a=σ(z)=11+e−z</p>
<p>Loss (error) function: L(a,y)=−(ylog(a)+(1−y)log(1−a))</p>
<p>cost function：J(w,b)=1mΣi=1m⁡L(y<sup>(i),y(i))=−1mΣi=1m⁡[y(i)logy</sup>(i)+(1−y(i))log(1−y^(i))])</p>
<blockquote>
<p>只考虑一个样本的情况。其中 a 是逻辑回归的输出，y 是样本的标签值</p>
<p>那么：L (a,y)=J (w,b)</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414155515530.png?lastModify=1726287112" alt="image-20240414155515530" /></p>
<blockquote>
<p>w=[w1,w2] ，wT=[w1w2]</p>
<p>x=[x1,x2]</p>
<p>wT⋅x=[w1w2]⋅[x1,x2]=w1x1+w2x2</p>
</blockquote>
<p>为了使得 logistic 回归中<strong>最小化</strong>代价函数 L (a,y)，需要做的仅仅是修改参数 w 和 b 的值。</p>
<blockquote>
<p>w:=w−α∂J(w,b)∂w</p>
<p>b:=b−α∂J(w,b)∂b</p>
</blockquote>
<p>所以要反向计算导数。</p>
<p>我们想要计算出的代价函数 L (a,y) 的导数，首先我们需要反向计算出 &quot;da&quot;=dL (a,y) da</p>
<blockquote>
<p>L(a,y)=−(ylog(a)+(1−y)log(1−a))</p>
<p>&quot;da&quot; = −ya+1−y1−a</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414162113374.png?lastModify=1726287112" alt="image-20240414162113374" /></p>
<p>再向后一步，计算 dz=dLdz=a−y</p>
<blockquote>
<p>a=σ(z)=11+e−z</p>
<p>dLdz=dLdadadz</p>
<ul>
<li>dadz=e−z(1+e−z)2=a(1−a)</li>
<li>dLda=−ya+1−y1−a</li>
</ul>
<p>dz=a(1−a)⋅(−ya+1−y1−a)=a−y</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414163547628.png?lastModify=1726287112" alt="image-20240414163547628" /></p>
<p>进行最后一步反向推导，也就是计算 w 和 b 变化对代价函数 L 的影响</p>
<ul>
<li>∂L∂w1 = &quot;dw1&quot; = dzdw1dLdz=x1dz</li>
<li>dw2=x2dz</li>
<li>db=dz</li>
</ul>
<p>然后更新 w1、w2 、b</p>
<blockquote>
<p>w1:=w1−αdw1</p>
<p>w2:=w2−αdw2</p>
<p>b:=b−αdb</p>
</blockquote>
<h2 id="m-个样本的梯度下降"><a class="anchor" href="#m-个样本的梯度下降">#</a> m 个样本的梯度下降</h2>
<p>J(w,b)=1mΣi=1m⁡L(a(i),y(i))</p>
<blockquote>
<p>a(i)=y^(i)=σ(z(i))=σ(wTx(i)+b)</p>
</blockquote>
<p>dw1=∂∂w1J(w,b)=1mΣi=1m⁡∂∂wiL(a(i),y(i))</p>
<blockquote>
<p>在上一节，展示了单个样本的 dw1 (i)−(x (i),y (i))</p>
<p>在此处，只需要对每个样本求解，然后求平均，这会得到全局梯度值</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414170331521.png?lastModify=1726287112" alt="image-20240414170331521" /></p>
<blockquote>
<p>上述只应用了一次梯度下降，因此需要重复以上内容很多次以应用多次梯度下降</p>
</blockquote>
<p>但这种计算中有两个缺点</p>
<ul>
<li>
<p>也就是说应用此方法在逻辑回归上你需要编写两个 for 循 环。</p>
<blockquote>
<p>第一个 for 循环是一个小循环遍历 m 个训练样本，</p>
<p>第二个 for 循环是一个遍历所有特征的 for 循环。</p>
<p>这个例子中我们只有 2 个特征，所以 n 等于 2 并且 nx 等于 2。</p>
<p>但如果你有更多特征，你开始编写你的因此 dw1、dw2，你有相似的计算从 dw3 一直下去到 dwn。所以需要一个 for 循环遍历所有 n 个特征。</p>
</blockquote>
</li>
</ul>
<p>解决办法：向量化技术（Vectorization），它可以允许你的代码摆脱这些显式的 for 循环。</p>
<h2 id="向量化vectorization"><a class="anchor" href="#向量化vectorization">#</a> 向量化（Vectorization）</h2>
<p>向量化是非常基础的去除代码中 for 循环的艺术</p>
<ul>
<li>在深度学习安全领域、深度学习实践中，你会经常发现自己训练大数据集，因为深度学习算法处理大数据集效果很棒，所以你的代码运行速度非常重要，</li>
<li>否则如果在大数据集上，你的代码可能花费很长时间去运行， 你将要等待非常长的时间去得到结果。</li>
<li>所以在深度学习领域，运行向量化是一个关键的技巧。</li>
</ul>
<p>在 logistic 回归中，需要去计算 z=wTx+b</p>
<blockquote>
<p>w=[....],x=[....],x∈Rnx,w∈Rnx</p>
</blockquote>
<p>使用非向量化技术，如下 python 代码：</p>
<blockquote>
<pre><code>z = 0
for i in range(n - 1):
 z += w[i]*x[i]
z += b
</code></pre>
</blockquote>
<p>使用向量化技术：</p>
<blockquote>
<pre><code>z = np.dot(w, x) + b
</code></pre>
<p>np.dot(w, x)：wTx</p>
</blockquote>
<p>在 Jupyter 中实验：</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414172855418.png?lastModify=1726287112" alt="image-20240414172855418" /></p>
<p>大规模的深度学习使用了 GPU（图像处理单元）实现，在上述只有 CPU</p>
<ul>
<li>
<p>CPU 和 GPU 都有并行化的指令，他们有时候会叫做 SIMD 指令，这个代表了一个单独指令多维数据，</p>
<blockquote>
<p>这个的基础意义是，如果你使用了 built-in 函数，像 np.function 或者并不要求你实现循环的函数，它可以让 python 的充分利用<strong>并行化</strong>计算，这是事实在 GPU 和 CPU 上面计算，GPU 更加擅长 SIMD 计算，但是 CPU 事实上也不是太差，可能没有 GPU 那么擅长。</p>
</blockquote>
</li>
</ul>
<h2 id="向量化的更多例子"><a class="anchor" href="#向量化的更多例子">#</a> 向量化的更多例子</h2>
<p>当在写神经网络程序时，或者在写逻辑 (logistic) 回归，或者其他神经网络模型时，应该避免写循环 (loop) 语句，可以使用比如 numpy 的内置函数或者其他办法去计算。</p>
<h3 id="求和"><a class="anchor" href="#求和">#</a> 求和</h3>
<blockquote>
<p>u=Av</p>
<p>ui=Σj=1⁡Aijvi</p>
<p>u=np.zeros((n,1))</p>
<p>for i…</p>
<p>​	for j...</p>
<p>​			u[i]+=A[i][j]∗v[j]</p>
<hr />
<p>对应：u=np.dot (A,v)</p>
</blockquote>
<h3 id="指数"><a class="anchor" href="#指数">#</a> 指数</h3>
<p>v=[v1...vn]→x=[ev1...evn]</p>
<pre><code>u = np.zeros((n, 1))
for i in range(n):
    u[i] = math.exp(v[i])
</code></pre>
<p>对应向量化版本：</p>
<pre><code>import numpy as np
u = np.exp(v)
#---------------------
np.log(v)
np.abs(v)
np.maximun(v, o)
</code></pre>
<h3 id="logistic-回归的梯度下降"><a class="anchor" href="#logistic-回归的梯度下降">#</a> logistic 回归的梯度下降</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240414175113030.png?lastModify=1726287112" alt="image-20240414175113030" /></p>
<blockquote>
<p>注意：上述 dz (i)=a (i)−y (i)</p>
</blockquote>
<h2 id="向量化-logistic-回归"><a class="anchor" href="#向量化-logistic-回归">#</a> 向量化 logistic 回归</h2>
<p>z(1)=wTx(1)+b,a(1)=σ(z(1))</p>
<p>z(2)=wTx(2)+b,a(2)=σ(z(2))</p>
<p>z(3)=wTx(3)+b,a(3)=σ(z(3))</p>
<p>X=[||⋯|x(1)x(2)⋯x(m)||⋯|],(nx,m)―Rnx×m</p>
<p>Z</p>
<p>=[z(1)z(2)⋯z(m)]</p>
<p>①=wTX+[bb⋯b] ①</p>
<p>=[wTx(1)+bwTx(2)+b⋯wTx(n)+b]</p>
<blockquote>
<p>wT： 1×nx</p>
</blockquote>
<p>X 是把所有训练样本堆叠起来得到的（一个一个横向堆积起来）</p>
<p>同理，将 z (i) 横向堆叠起来就得到了 Z</p>
<p>对于 ①：</p>
<blockquote>
<pre><code>Z = np.dot(w.T, x) + b
</code></pre>
<p>广播（broadcasting）：b 为实数，python 会自动将 b 扩展成一个 1×m 的行向量</p>
</blockquote>
<p>同理，将 a (i) 横向堆叠起来就得到了 A</p>
<p>A=[a(1)a(2)⋯a(m)]=σ(z)</p>
<h2 id="向量化-logistic-回归的梯度输出"><a class="anchor" href="#向量化-logistic-回归的梯度输出">#</a> 向量化 logistic 回归的梯度输出</h2>
<p>如何同时计算 m 个数据的梯度，并且实现一个非常高效的逻辑回归算法 (Logistic Regression)。</p>
<p>dz(1)=a(1)−y(1)	dz(2)=a(2)−y(2)</p>
<p>dz=[dz(1)dz(2)⋯dz(m)]</p>
<p>A=[a(1)a(2)⋯a(m)]	Y=[y(1)y(2)⋯y(m)]</p>
<p>→dz=A−Y=[a(1)−y(1)a(2)−y(2)⋯a(m)−y(m)]</p>
<blockquote>
<p>dw=0</p>
<p>dw+=x(1)dz(1)</p>
<p>dw+=x(2)dz(2)</p>
<p>⋯</p>
<p>dw+=x(m)dz(m)</p>
<p>dw/=m</p>
</blockquote>
<blockquote>
<p>db=0</p>
<p>db+=dz(1)</p>
<p>db+=dz(2)</p>
<p>⋯</p>
<p>db+=dz(m)</p>
<p>db/=m</p>
</blockquote>
<p>将它们向量化</p>
<p>db=1mΣi=1m⁡dz(i)=1mnp.sum(dz)</p>
<p>dw=1mXdzT</p>
<p>=1m[||⋯|x(1)x(2)⋯x(m)||⋯|][dz(1)dz(2)..dz(m)]</p>
<p>=1m[x(1)dz(1)+⋯+x(m)dz(m)]</p>
<p>没有向量化的代码：</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240415191527588.png?lastModify=1726287112" alt="image-20240415191527588" /></p>
</blockquote>
<p>向量化后的代码</p>
<blockquote>
<p>for item in range(1000):</p>
<p>Z=wTX+b</p>
<p>A=σ(Z)</p>
<p>dz=A−Y</p>
<p>dw=1mXdzT</p>
<p>db=1mnp.sum(dz)</p>
<p>w:=w−αdw</p>
<p>b:=b−αdb</p>
</blockquote>
<ul>
<li>J：只是手推各个偏导值所要定义出来的，在代码计算中已经求出来各个参数的偏导公式，也就是所需要的最终迭代出 w，b（最小 J 所需要的参数）</li>
</ul>
<h2 id="python-中的广播"><a class="anchor" href="#python-中的广播">#</a> Python 中的广播</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240415192947756.png?lastModify=1726287112" alt="image-20240415192947756" /></p>
<p>要计算每列 Carbs 在每列总体含量的百分比，如何不适用 for 循环</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240415194017914.png?lastModify=1726287112" alt="image-20240415194017914" /></p>
<p>下  <code>A.sum(axis = 0)</code>  中的参数 axis。axis 用来指明将要进行的运算是沿着哪个轴执行，在 numpy 中，0 轴是垂直的，也就是<strong>列</strong>，而 1 轴是水平的，也就是<strong>行</strong>。</p>
<p>那么一个 3×4 的矩阵是怎么和 1×4 的矩阵做除法的呢？</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240415194523447.png?lastModify=1726287112" alt="image-20240415194523447" /></p>
<p><strong>普遍原则</strong>：一个维度相同，则另一个维度自动扩展至相同</p>
<blockquote>
<p>(1,n)→(m,n)</p>
<p>(m,1)→(m,n)</p>
</blockquote>
<h2 id="关于-python-_-numpy-向量的说明"><a class="anchor" href="#关于-python-_-numpy-向量的说明">#</a> 关于 python _ numpy 向量的说明</h2>
<p><strong>数组</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240415195938886.png?lastModify=1726287112" alt="image-20240415195938886" /></p>
<p><strong>矩阵</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240415200042366.png?lastModify=1726287112" alt="image-20240415200042366" /></p>
<p>不完全确定一个向量的维度：</p>
<blockquote>
<pre><code>assert(a.shape == (5, 1))
</code></pre>
</blockquote>
<p>使用  <code>reshape</code>  将数组转化为矩阵</p>
<blockquote>
<pre><code>a = a.reshape((5, 1))
</code></pre>
</blockquote>
<h2 id="logistic-损失函数的解释"><a class="anchor" href="#logistic-损失函数的解释">#</a> logistic 损失函数的解释</h2>
<p>y^=σ(wTx+b)</p>
<p>σ(z)=11+e−z</p>
<p>y^=P (y=1|x)：给定训练样本 x 条件下 y 等于 1 的概率。</p>
<p>If y = 1：P(y|x)=y^</p>
<p>If y = 0：P(y|x)=1−y^</p>
<p>合并：P (y|x)=y<sup>y(1−y</sup>)(1−y)</p>
<p>log(P(y|x))=logy<sup>y(1−y</sup>(1−y))=ylogy<sup>+(1−y)log(1−y</sup>)=−L(y^,y)</p>
<blockquote>
<p>用 log 的原因：求导方便</p>
</blockquote>
<p>因此：最小化损失函数 L (y^,y) 就是最大化 log (P (y|x))</p>
<p>成本函数？</p>
<p>假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积</p>
<blockquote>
<p>logP( labels in training set) =logΠi=1m⁡P(y(i)|x(i))=Σi=1m⁡logP(y(i)|x(i))=Σi=1m−L(y^(i),y(i))</p>
</blockquote>
<p>在最大似然估计中，求出一组参数，使得 logP (labels in training set) 取<strong>最大值</strong>，也就是下面的成本函数<strong>最小</strong></p>
<p>对于：Cost function：J (w,b)=1mΣi=1m⁡L (y^(i),y (i))</p>
<blockquote>
<p>目的是让成本函数最小化，所以不是直接用最大似然概率，要去掉这里的负号，最后为了方便，可以对成本函数进行适当的缩放，我们就在前面加一个额外的常数因子 1m，</p>
</blockquote>
<h1 id="浅层神经网络"><a class="anchor" href="#浅层神经网络">#</a> 浅层神经网络</h1>
<h2 id="神经网络概述"><a class="anchor" href="#神经网络概述">#</a> 神经网络概述</h2>
<p>logistic 回归模型</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416190743200.png?lastModify=1726287112" alt="image-20240416190743200" /></p>
<p>公式：xwb}→z=wTx+b→a=σ(z)→L (a,y)</p>
<p>你可以把许多 sigmoid 单元堆叠起来形成一个神经网络。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416191120532.png?lastModify=1726287112" alt="image-20240416191120532" /></p>
<blockquote>
<p>上角标 [1]：第一层神经网络</p>
<p>下角标：对应层数的第几个节点</p>
</blockquote>
<p>xw[1]b[1]}→z[1]=w[1]x+b[1]→a[1]=σ(z[1])</p>
<p>→z[2]=w[2]x+b[2]→a[2]=σ(z[2])→L(a[2],y)</p>
<p>同样，在神经 网络中我们也有<strong>从后向前的计算导数</strong></p>
<p>xdw[1]db[1]}→dz[1]=d(w[1]x+b[1])←da[1]=dσ(z[1])</p>
<p>←dz[2]=d(w[2]x+b[2])←da[2]=dσ(z[2])←dL(a[2],y)</p>
<h2 id="神经网络表示"><a class="anchor" href="#神经网络表示">#</a> 神经网络表示</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416192119419.png?lastModify=1726287112" alt="image-20240416192119419" /></p>
<blockquote>
<p>Input layer：神经网络的输入</p>
<p>Hidden layer：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入 x 也包含了目标输出 y，所以 术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的</p>
<ul>
<li>也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。</li>
</ul>
<p>Output layer：负责产生预测值</p>
</blockquote>
<p>a [0]=x：a 表示激活的意思，意味着网络中不同层的值会传递到它们后面的层中</p>
<p>a [1]=[a [1] a [2] a [3] a [4]]：有四个结点或者单元，或者称为四个隐藏层单元</p>
<p>y=a [2]：明确指出这些值来自哪一层。</p>
<blockquote>
<p>计算网络的层数：输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层</p>
<ul>
<li>将输入层称为第零层</li>
</ul>
<p>在技术上：三层的神经网络</p>
<p>在传统的符号使用：两层的神经网络</p>
</blockquote>
<p>对于隐藏层 w [1] 和 b [1]</p>
<ul>
<li>
<p>w[1]：(4, 3)</p>
<blockquote>
<p>4：有四个结点或隐藏层单元</p>
<p>3：有三个输入特征</p>
</blockquote>
</li>
<li>
<p>b[1]：(4, 1)</p>
</li>
</ul>
<p>对于输出层  w [2] 和 b [2]</p>
<ul>
<li>
<p>w[2]：(1, 4)</p>
<blockquote>
<p>1：有一个结点或隐藏层单元</p>
<p>4：有四个输入特征</p>
</blockquote>
</li>
<li>
<p>b[2]：(1, 1)</p>
</li>
</ul>
<h2 id="计算一个神经网络的输出"><a class="anchor" href="#计算一个神经网络的输出">#</a> 计算一个神经网络的输出</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416200730109.png?lastModify=1726287112" alt="image-20240416200730109" /></p>
<p>对于：</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416192119419.png?lastModify=1726287112" alt="image-20240416192119419" /></p>
</blockquote>
<p>z1[1]=w1[1]Tx+b1[1],a1[1]=σ(z1[1])</p>
<p>z2[1]=w2[1]Tx+b2[1],a2[1]=σ(z2[1])</p>
<p>z3[1]=w3[1]Tx+b3[1],a3[1]=σ(z3[1])</p>
<p>z4[1]=w4[1]Tx+b4[1],a4[1]=σ(z4[1])</p>
<blockquote>
<p>其中 wi [1]:(3,1)</p>
</blockquote>
<p>z[1]=[z1[1]z2[1]z3[1]z4[41]]=[⋯w1[1]T⋯⋯w2[1]T⋯⋯w3[1]T⋯⋯w4[1]T⋯]⏟(4,3)⏞w[1]∗[x1x2x3]+[b1[1]b2[1]b3[1]b4[1]]⏞b[1]</p>
<p>a[1]=[a[1]a[2]a[3]a[4]]=σ(z[1])</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416202655650.png?lastModify=1726287112" alt="image-20240416202655650" /></p>
<p>a [2]→y^ 这部分类似于 Logistic 回归</p>
<h2 id="多样本向量化"><a class="anchor" href="#多样本向量化">#</a> 多样本向量化</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416212204729.png?lastModify=1726287112" alt="image-20240416212204729" /></p>
<p>对于单样本：x→a [2]=y^</p>
<p>对于 m 个训练样本：</p>
<blockquote>
<p>x(1)→a<a href="1">2</a>=y(1)^</p>
<p>x(2)→a<a href="2">2</a>=y(2)^</p>
<p>…</p>
<p>x(m)→a<a href="m">2</a>=y(m)^</p>
</blockquote>
<ul>
<li>a<a href="i">2</a>：i ：训练样本 i</li>
</ul>
<p>对于非向量化：</p>
<p>z1[1]=w1[1]Tx+b1[1],a1[1]=σ(z1[1])</p>
<p>z2[1]=w2[1]Tx+b2[1],a2[1]=σ(z2[1])</p>
<p>for i = 1 to m:</p>
<p>​	z<a href="i">1</a>=w[i]x(i)+b[1]</p>
<p>​	a<a href="i">1</a>=σ(z<a href="i">1</a>)</p>
<p>​	z<a href="i">2</a>=w[i]a<a href="i">1</a>+b[2]</p>
<p>​	a<a href="i">2</a>=σ(z<a href="i">2</a>)</p>
<p>向量化：</p>
<p>Z[1]=W[1]X+b[1]</p>
<p>A[1]=σ(Z[1])</p>
<p>Z[2]=W[2]A[1]+b[2]</p>
<p>A[2]=σ(z[2])</p>
<blockquote>
<p>X=[||⋯|x(1)x(2)⋯x(m)||⋯|],(nx,m)―Rnx×m</p>
<p>Z[1]=[||⋯|z<a href="1">1</a>z<a href="2">1</a>⋯z<a href="m">1</a>||⋯|]</p>
<p>A[1]=[||⋯|a<a href="1">1</a>a<a href="2">1</a>⋯a<a href="m">1</a>||⋯|]</p>
</blockquote>
<ul>
<li>每一列表示<strong>每个样本</strong>：每一行表示<strong>每一个节点（隐藏单元）</strong></li>
</ul>
<h2 id="向量化实现的解释"><a class="anchor" href="#向量化实现的解释">#</a> 向量化实现的解释</h2>
<p>对于 Z [1]=W [1] X+b [1]：</p>
<p>z<a href="1">1</a>=W[1]x(1)+b[1],z<a href="2">1</a>=W[1]x(2)+b[1],z<a href="3">1</a>=W[1]x(3)+b[1]</p>
<blockquote>
<p>b[1]=0</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416215515638.png?lastModify=1726287112" alt="image-20240416215515638" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416215911652.png?lastModify=1726287112" alt="image-20240416215911652" /></p>
<h2 id="激活函数"><a class="anchor" href="#激活函数">#</a> 激活函数</h2>
<p>例如：如下 σ() 就是激活函数</p>
<blockquote>
<p>​	z<a href="i">1</a>=w[1]x(i)+b[1]</p>
<p>​	a<a href="i">1</a>=σ(z<a href="i">1</a>)</p>
<p>​	z<a href="i">2</a>=w[2]a<a href="i">1</a>+b[2]</p>
<p>​	a<a href="i">2</a>=σ(z<a href="i">2</a>)</p>
</blockquote>
<p>对于通常情况下：使用 g (z [1])，g 可以是非线性函数</p>
<p>tanh 函数或者双曲正切函数是总体上都优于 sigmoid 函数的激活函数</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416221746150.png?lastModify=1726287112" alt="image-20240416221746150" /></p>
</blockquote>
<p>g (z [1])=tanh (z [1])=ez−z−zez+z−z 效果总是优于 sigmoid 函数。</p>
<blockquote>
<p>因为函数值域在 - 1 和 + 1 的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用 tanh 函数代替 sigmoid 函数中心化数据，使得数据的平均值更接近 0 而不是 0.5.</p>
</blockquote>
<p>例外：在<strong>二分类</strong>的问题中，对于<strong>输出层</strong>，因为 y 的值是 0 或 1，所以想让 y^ 的 数值介于 0 和 1 之间，而不是在 - 1 和 + 1 之间。所以需要使用 sigmoid 激活函数。</p>
<p>对上述例子：对隐藏层使用 tanh 激活函数，输出层使用 sigmoid 函数</p>
<p>二者共同缺点：，在 z 特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会<strong>接近于 0</strong>，导致降低梯度下降的速度。</p>
<p>对于修正线性单元的函数 (ReLu)：a=max (0,z)</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416222433583.png?lastModify=1726287112" alt="image-20240416222433583" /></p>
<p>从实际上来说，当使用𝑧的导数时，z=0 的导数是没有定义的。 但是当编程实现的时候，z 的取值刚好等于 0.00000001，这个值相当小，所以，在实践中， 不需要担心这个值，z 是等于 0 的时候，假设一个导数是 1 或者 0 效果都可以。</p>
</blockquote>
<p>经验：如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</p>
<blockquote>
<p>这是很多激活函数的<strong>默认</strong>选择，如果在隐藏层上不确定使用哪个激活函数，那么通常 会使用 Relu 激活函数。有时，也会使用 tanh 激活函数</p>
<p>但 Relu 的一个<strong>缺点</strong>是：当 z 是负值的时候，导数等于 0。</p>
</blockquote>
<p>另一个版本的 Relu 被称为  <code>Leaky Relu</code> ：a=max(0.01z,z)</p>
<blockquote>
<p>当 z 是负值时，这个函数的值不是等于 0，而是轻微的倾斜</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240416223120623.png?lastModify=1726287112" alt="image-20240416223120623" /></p>
</blockquote>
<h2 id="为什么需要非线性激活函数"><a class="anchor" href="#为什么需要非线性激活函数">#</a> 为什么需要非线性激活函数</h2>
<p>z[1]=w[1]x+b[1]</p>
<p>a[1]=g(z[1])→a[1]=z[1]</p>
<ul>
<li>g (z)=z：线性激活函数</li>
</ul>
<p>z[2]=w[2]a[1]+b[2]</p>
<p>a[2]=g(z[2])→a[2]=z[2]</p>
<p>那么这个模型的输出 y 或仅仅只是输入特征 x 的线性组合。</p>
<p>a[1]=z[1]=w[1]x+b[1]</p>
<p>a[2]=z[2]=w[2]a[1]+b[2]</p>
<p>a[2]=w<a href="w%5B1%5Dx+b%5B1%5D">2</a>+b[2]</p>
<p>​	=w[2]w[1]x+(w[2]b[1]+b[2])</p>
<p>​	=w′x+b′</p>
<p>如果使用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入<strong>线性组合</strong>再输出。</p>
<p>无论你的神经网络有多少层一直在做的<strong>只是计算线性函数</strong>，所以不如直接去掉全部隐藏层</p>
<blockquote>
<p>使用线性函数在隐藏层，无论使用多少个隐藏层更一个隐藏层是一样的。</p>
<p>也就是说没有激活函数，本质上都是单层的神经网络，只有了激活函数，才会扩展至多层的神经网络</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417204309478.png?lastModify=1726287112" alt="image-20240417204309478" /></p>
<p>相当于 Logistic 回归</p>
</blockquote>
<p>只有一个地方可以使用线性激活函数 g (z)=z，就是你在做机器学习中的回归问题。y 是一 个实数</p>
<blockquote>
<p>比如你想预测房地产价格，y 就不是二分类任务 0 或 1，而是一个实数，从 0 到正无穷。</p>
<p>如果 y 是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个实数</p>
<ul>
<li>从负无穷到正无穷。</li>
</ul>
</blockquote>
<p>总而言之，不能在隐藏层用线性激活函数，可以用 ReLU 或者 tanh 或者 leaky ReLU 或者其他的非线性激活函数</p>
<blockquote>
<p>唯一可以用线性激活函数的通常就是输出层。</p>
</blockquote>
<h2 id="激活函数的导数"><a class="anchor" href="#激活函数的导数">#</a> 激活函数的导数</h2>
<p>在神经网络中使用反向传播的时候，你真的需要计算激活函数的斜率或者导数。针对以下四种激活，求其导数如下：</p>
<h3 id="sigmoid-activation-function"><a class="anchor" href="#sigmoid-activation-function">#</a> sigmoid activation function</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417205155948.png?lastModify=1726287112" alt="image-20240417205155948" /></p>
<p>g′(z)=ddzg(z)=11+e−z(1−11+e−z)=g(z)(1−g(z))</p>
<blockquote>
<p>当 z=10,g (z)≈1,ddzg (z)≈0</p>
<p>当 z=−10,g (z)≈0,ddzg (z)≈0</p>
<p>当 z=0,g (z)=12,ddzg (z)=12</p>
</blockquote>
<p>在神经网络中：a=g (z)</p>
<p>g′(z)=ddzg(z)=a(1−a)</p>
<h3 id="tanh-activation-function"><a class="anchor" href="#tanh-activation-function">#</a> Tanh activation function</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417205805278.png?lastModify=1726287112" alt="image-20240417205805278" /></p>
<p>g(z)=tanh(z)=ez−e−zez+e−z</p>
<p>ddzg(z)=1−(tanh(z))2</p>
<blockquote>
<p>当 z=10,g (z)≈1,ddzg (z)≈0</p>
<p>当 z=−10,g (z)≈−1,ddzg (z)≈0</p>
<p>当 z=0,g (z)=0,ddzg (z)=1</p>
</blockquote>
<p>在神经网络中：a=g (z)</p>
<p>g′(z)=ddzg(z)=1−a2</p>
<h3 id="rectified-linear-unit-relu"><a class="anchor" href="#rectified-linear-unit-relu">#</a> Rectified Linear Unit (ReLU)</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417210229339.png?lastModify=1726287112" alt="image-20240417210229339" /></p>
<p>g(z)=max(0,z)</p>
<p>g′(z)={0 if z&lt;01 if z&gt;0undefined if z=0</p>
<blockquote>
<p>通常在 z=0 的时候给定其导数 1,0；当然 z=0 的情况很少</p>
</blockquote>
<h3 id="leaky-linear-unit-leaky-relu"><a class="anchor" href="#leaky-linear-unit-leaky-relu">#</a> Leaky linear unit (Leaky ReLU)</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417210618365.png?lastModify=1726287112" alt="image-20240417210618365" /></p>
<p>g(z)=max(0.01z,z)</p>
<p>g′(z)={0.01 if z&lt;01 if z&gt;0undefined if z=0</p>
<blockquote>
<p>通常在 z=0 的时候给定其导数 1,0.01；当然 z=0 的情况很少</p>
</blockquote>
<h2 id="神经网络的梯度下降矩阵链式求导调整维度"><a class="anchor" href="#神经网络的梯度下降矩阵链式求导调整维度">#</a> 神经网络的梯度下降 (矩阵链式求导调整维度)</h2>
<p><strong>单隐层</strong>神经网络会有 W [1],b [1],W [2],b [2] 这些参数</p>
<p>还有个 nx=n [0] 表示输入特征的个数，n [1] 表示隐藏单元个数，n [2] 表示输出单元个数。</p>
<blockquote>
<p>W[1]:(n[1],n[0])</p>
<p>b[1]:(n[1],1)</p>
<p>W[2]:(n[2],n[1])</p>
<p>b[2]:(n[2],1)</p>
</blockquote>
<p>假设你在做二分类任务，那么你的成本函数等于：</p>
<p>Cost function：J(W[1],b[1],W[2],b[2])=1mΣi=1m⁡L(y^,y)</p>
<blockquote>
<p>a[2]=y</p>
</blockquote>
<p>每次梯度下降都会循环计算以下预测值：</p>
<p>y^(i),(i=1,2,...,m)</p>
<p>dW[1]=∂J∂W[1],db[1]=∂J∂b[1]</p>
<p>dW[2]=∂J∂W[2],db[2]=∂J∂b[2]</p>
<p>W[1]:=W[1]−αdW[1]</p>
<p>b[1]:=b[1]−αdb[1]</p>
<p>W[2]:=W[2]−αdW[2]</p>
<p>b[2]:=b[2]−αdb[2]</p>
<p>正向传播（Forward propagation）：</p>
<p>Z[1]=W[1]X+b[1]</p>
<blockquote>
<p>Z[1]:(n[1],m)</p>
</blockquote>
<p>A[1]=g<a href="Z%5B1%5D">1</a></p>
<blockquote>
<p>A [1] 的维度和 Z [1] 的维度相等</p>
</blockquote>
<p>Z[2]=W[2]A[1]+b[2]</p>
<p>A[2]=g<a href="Z%5B2%5D">2</a></p>
<p>反向传播（back propagation）：</p>
<p>dZ[2]=A[2]−Y,Y=[y(1)y(2)⋯y(m)]</p>
<p>dW[2]=1mdZ[2]A[1]T</p>
<blockquote>
<p>A [1]: 第一层的输出，类似与第 0 层的特征向量 X</p>
<p>为什么要转置：</p>
<ul>
<li>dZ[2]:(n[2],m)</li>
<li>A[1]:(n[1],m)</li>
</ul>
<p>dW[2]:(n[2],n[1])</p>
</blockquote>
<p>db[2]=1mnp.sum(dZ[2],axis=1,keepdims=True)</p>
<blockquote>
<p>每一列表示<strong>每个样本</strong>：每一行表示<strong>每一个节点（隐藏单元）</strong></p>
</blockquote>
<p>dZ[1]=W[2]TdZ[2]⏟(n[1],m)∗g[1]′(Z[1])</p>
<blockquote>
<p>∗：元素乘积</p>
<p>dLdZ[1]=dLdZ[2]dZ[2]dA[1]dA[1]dZ[1]</p>
<ul>
<li>dZ[1]:(n[1],m)</li>
<li>dZ[2]:(n[2],m)</li>
<li>dZ[2]dA[1]=W[2]:(n[2],n[1])</li>
<li>dA[1]dZ[1]=g[1]′(Z[1]):(n[1],m)</li>
</ul>
<p>会发现维度对不上：(n [2],m)∗(n [2],n [1])∗(n [1],m)</p>
<p>需要调整维度：(n [1],n [2])∗(n [2],m)∗(n [1],m)</p>
</blockquote>
<p>dW[1]=1mdZ[1]xT</p>
<blockquote>
<p>dZ[1]:(n[1],m)</p>
<p>x:(n[1],m)</p>
</blockquote>
<p>db[1]⏟(n[1],1)=1mnp.sum(dZ[1],axis=1,keepdims=True)</p>
<h2 id="直观理解反向传播"><a class="anchor" href="#直观理解反向传播">#</a> 直观理解反向传播</h2>
<h3 id="logistic-regression"><a class="anchor" href="#logistic-regression">#</a> Logistic regression</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417221211324.png?lastModify=1726287112" alt="image-20240417221211324" /></p>
<p>L(a,y)→a=σ(z)</p>
<blockquote>
<p>da=ddaL(a,y)=dda(−yloga−(1−y)log(1−a))</p>
<p>​	=−ya+1−y1−a</p>
</blockquote>
<p>a=σ(z)→z=wTx+b</p>
<blockquote>
<p>dz=a−y</p>
<p>​	=∂L∂adadz</p>
<p>​	dadz=g′(z)</p>
</blockquote>
<p>z=wTx+b→{xwb</p>
<blockquote>
<p>dw=dz⋅x</p>
<p>db=dz</p>
</blockquote>
<h3 id="对于双层神经网络维度匹配"><a class="anchor" href="#对于双层神经网络维度匹配">#</a> 对于双层神经网络（维度匹配）</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417223247262.png?lastModify=1726287112" alt="image-20240417223247262" /></p>
<blockquote>
<p>不对 x 求导，因为监督学习的输入 x 是固定的</p>
</blockquote>
<p>dz[2]=a[2]−y</p>
<p>dW[2]=dz[2]a[1]T</p>
<blockquote>
<p>转置是因为矩阵链式求导维度要匹配</p>
</blockquote>
<p>db[2]=dz[2]</p>
<p>dz[1]⏟(n[1],1)=W[2]Tdz[2]⏟(n[1],1)∗g[1]′(z[1])⏟(n[1],1)</p>
<blockquote>
<p>∗：元素乘积</p>
<p>dLdz[1]=dLdz[2]dz[2]da[1]da[1]dz[1]</p>
<ul>
<li>dz[1]:(n[1],1)</li>
<li>dz[2]:(n[2],1)</li>
<li>dz[2]da[1]=W[2]:(n[2],n[1])</li>
<li>da[1]dz[1]=g[1]′(z[1]):(n[1],1)</li>
</ul>
<p>会发现维度对不上：(n [2],1)∗(n [2],n [1])∗(n [1],1)</p>
<p>需要调整维度：(n [1],n [2])∗(n [2],1)∗(n [1],1)</p>
</blockquote>
<p>实现反向传播有个技巧，就是要保证矩阵的维度相互<strong>匹配</strong>。</p>
<p>dW[1]=dz[1]xT</p>
<blockquote>
<p>dz[1]:(n[1],1)</p>
<p>x:(n[1],1)</p>
</blockquote>
<p>db[1]=dz[1]</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/backprop_kiank.png?lastModify=1726287112" alt="backprop_kiank" /></p>
<p>使用前一链规则，计算当前规则：</p>
<blockquote>
<p>dW[1]=dz[1]∂z[1]∂W[1]</p>
<p>使用上一个梯度来计算当前梯度</p>
<p>da[l−1]⏟(n[l−1],1)=W[l]T⏟(n[l−1],n[l])dz[l]⏟(n[l],1)</p>
</blockquote>
<h3 id="向量化实现-除以-m-解释"><a class="anchor" href="#向量化实现-除以-m-解释">#</a> 向量化实现 (除以 M 解释)</h3>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>dz[2]=a[2]−y</td>
<td>dZ[2]=A[2]−Y</td>
</tr>
<tr>
<td>dW[2]=dz[2]a[1]T</td>
<td>dW[2]=1mdZ[2]A[1]T</td>
</tr>
<tr>
<td>db[2]=dz[2]</td>
<td>db[2]=1mnp.sum(dZ[2],axis=1,keepdims=True)</td>
</tr>
<tr>
<td>dz[1]⏟(n[1],1)=W[2]Tdz[2]⏟(n[1],1)∗g[1]′(z[1])⏟(n[1],1)</td>
<td>dZ[1]⏟(n[1],nm)=W[2]TdZ[2]⏟(n[1],m)∗g[1]′(Z[1])⏟(n[1],m)</td>
</tr>
<tr>
<td>dW[1]=dz[1]xT</td>
<td>dW[1]=1mdz[1]xT</td>
</tr>
<tr>
<td>db[1]=dz[1]</td>
<td>db[1]⏟(n[1],1)=1mnp.sum(dZ[1],axis=1,keepdims=True)</td>
</tr>
</tbody>
</table>
<blockquote>
<p>损失函数累加导致误差变大了，损失函数不应该累加，而是使得它接近一个值，所以要求平均</p>
<p>Z 是所有样本的矩阵，所以不需要求平均</p>
<p>而 W 一直是 W，M 个样本的矩阵变成了 A，是之前 a 矩阵维度的 M 倍，所以要除以 M</p>
</blockquote>
<h2 id="随机初始化"><a class="anchor" href="#随机初始化">#</a> 随机初始化</h2>
<p>当训练神经网络时，权重随机初始化是很重要的。</p>
<ul>
<li>对于逻辑回归，把权重初始化为 0 当然也是可以的。</li>
<li>但是对于一个神经网络，如果你把权重或者参数都初始化为 0，那么梯度下降将不会起作用。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240418133101063.png?lastModify=1726287112" alt="image-20240418133101063" /></p>
<p>n[0]=2,n[1]=2</p>
<p>W[1]=[0000],b[1]=[00]</p>
<p>网络输入任何样本，a [1]=a [2]</p>
<blockquote>
<p>这两个激活函数完全一样，因为两个隐含单元计算同样的函数，</p>
</blockquote>
<p>当做反向传播计算时，dz [1]=dz [2]</p>
<blockquote>
<p>这两个隐藏单元会以同样方式初始化</p>
</blockquote>
<p>这样输出的权值也会一模一样，w [2]=[00]</p>
<blockquote>
<p>每一列表示<strong>每个样本</strong>：每一行表示<strong>每一个节点（隐藏单元）</strong></p>
</blockquote>
<p>最终经过每次训练的迭代， 这两个隐含单元仍然是同一个函数</p>
<p>dW=[uvuv],W[1]=W[1]−αdW</p>
<blockquote>
<p>每次迭代后的 W [1] 其中第一行等于第二行</p>
</blockquote>
<p>如果你要初始化成 0，由于所有的隐含单元都是对称的，无论你运行梯度下降多久， 他们一直计算同样的函数。这没有任何帮助，因为你想要两个不同的隐含单元计算不同的 函数，这个问题的解决方法就是随机初始化参数</p>
<p>W[1]=np.random.randn(2,2)∗0.01</p>
<ul>
<li>
<p>为什么是 0.01，而不是 100 或者 1000。我们通常倾向于初始化为很小的随机数</p>
<blockquote>
<p>因为：Z [1]=W [1] X+b [1],a [1]=g<a href="Z%5B1%5D">1</a></p>
<p>若使用 100 的化，导致 W [1] 会很大，Z [1],a [1] 会很大</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240417205155948.png?lastModify=1726287112" alt="image-20240417205155948" /></p>
<p>如果你用 tanh 或者 sigmoid 激活函数，这种情况下你很可能停在 tanh/sigmoid 函数的<strong>平坦的地方</strong></p>
<p>这些地方梯度 很小也就意味着梯度下降会很慢，因此学习也就很慢。</p>
<hr />
<p>若没有使用 sigmoid/tanh 激 活函数在整个的神经网络里，那么问题不是很大。</p>
<p>但如果做二分类并且你的输出单元是 sigmoid 函数，那么不会想让初始参数太大</p>
</blockquote>
</li>
<li>
<p>当训练一个只有一层隐藏层的网络时（这是相对 浅的神经网络，没有太多的隐藏层），设为 0.01 可能也可以。但当你训练一个非常非常深的神经网络，你可能会选择一个不同于的常数而不是 0.01。</p>
</li>
</ul>
<p>b[1]=np.zero((2,1))</p>
<blockquote>
<p>因为 W [1] 是随机初始化的，移开时还是用不同的隐藏单元计算不同的函数</p>
<p>W[2]=np.random.randn(1,2)∗0.01</p>
<p>b[2]=np.zero((1,1))</p>
</blockquote>
<h1 id="深层神经网络"><a class="anchor" href="#深层神经网络">#</a> 深层神经网络</h1>
<h2 id="深层神经网络-2"><a class="anchor" href="#深层神经网络-2">#</a> 深层神经网络</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240418204001553.png?lastModify=1726287112" alt="image-20240418204001553" /></p>
<blockquote>
<p>logistic regression：&quot;shallow&quot;</p>
<p>1 hidden layer：&quot;2 layer&quot; NN</p>
<p>5 hidden layer：&quot;deep&quot;</p>
</blockquote>
<p>提前预测到底需要多深的神经网络，先去尝试逻辑回归，尝试一层然后两层隐含层，然后把隐含层的数量看做是另一个可以自由选择大小的超参数，然后再保留交叉验证数据上评估，或者用你的开发集来评估。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240418205948684.png?lastModify=1726287112" alt="image-20240418205948684" /></p>
<p>我们用 L 表示层数，上图 L=4</p>
<p>n[0]=nx=3,n[1]=5,n[2]=5,n[3]=3,n[4]=nL=1</p>
<p>a [l]=activation in layer l：表示激活函数 g<a href="z%5Bl%5D">l</a></p>
<p>W[l]=wight for z[l]</p>
<h2 id="深层网络中的前向传播"><a class="anchor" href="#深层网络中的前向传播">#</a> 深层网络中的前向传播</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240418211119254.png?lastModify=1726287112" alt="image-20240418211119254" /></p>
<p>第一层需要计算：z [1]=W [1] x+b [1],a [1]=g<a href="z%5B1%5D">1</a></p>
<blockquote>
<p>x 可以看作 a [0]</p>
</blockquote>
<p>第二层需要计算：z [2]=W [2] a [1]+b [2],a [2]=g<a href="z%5B2%5D">2</a></p>
<p>第三层需要计算：z [3]=W [3] a [2]+b [3],a [3]=g<a href="z%5B3%5D">3</a></p>
<p>第四层需要计算：z [4]=W [4] a [3]+b [4],a [4]=g<a href="z%5B4%5D">4</a></p>
<p>基本规则：z [l]=W [l] a [l−1]+b [l],a [l]=g<a href="z%5Bl%5D">l</a></p>
<p>向量化：</p>
<p>for l=1...4</p>
<p>Z[1]=W[1]A[0]+b[1],A[1]=g<a href="A%5B1%5D">1</a></p>
<p>Z[2]=W[2]A[1]+b[2],A[2]=g<a href="A%5B2%5D">2</a></p>
<blockquote>
<p>Z[1]=[||⋯|z<a href="1">1</a>z<a href="2">1</a>⋯z<a href="m">1</a>||⋯|]</p>
<p>每个训练样本横向堆叠</p>
</blockquote>
<p>Y^=g(Z[4])=A[4]</p>
<blockquote>
<p>循环计算每一层</p>
</blockquote>
<h2 id="核对矩阵的维数"><a class="anchor" href="#核对矩阵的维数">#</a> 核对矩阵的维数</h2>
<h3 id="单个样本"><a class="anchor" href="#单个样本">#</a> 单个样本</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240418215120970.png?lastModify=1726287112" alt="image-20240418215120970" /></p>
<p>z[1]=W[1]x+b[1]</p>
<blockquote>
<p>z [1] 是第一个隐藏层的激活函数向量</p>
<p>z[1]:(3,1)→(n[1],1) ;x:(2,1)→(n[0],1)</p>
<p>所以 (3,1) 的矩阵 = ?∗(2,1) 的矩阵</p>
<p>那么 W [1]:(3,2)→(n [1],n [0])</p>
<ul>
<li>即：(3,1)=(3,2)∗(2,1)</li>
<li>权重矩阵的行数对应于当前层的神经元数量</li>
<li>权重矩阵的列数对应于上一层的神经元数量</li>
</ul>
</blockquote>
<p>同理：W [2]:(5,3)→(n [2],n [1])</p>
<ul>
<li>维度：（当前层的维度，上一层的维度）</li>
<li>W[l]:(n[l],n[l−1])</li>
</ul>
<p>如果需要做向量加法，z [1]:(3,1)→(n [1],1)</p>
<ul>
<li>那么 b [1]:(3,1)→(n [1],1)</li>
</ul>
<p>对于 a [l]=g<a href="z%5Bl%5D">l</a>，所以 a [l] 与 z [l] 的维度相等</p>
<p>z[1]⏟(n[1],1)=W[1]⏟(n[1],n[0])x⏟(n[0],1)+b[1]⏟(n[1],1)</p>
<p>对于反向传播：dw,db 的维度应该与 w,b 的维度相等</p>
<p>综上：该层有多少个神经单元，就建多少行，然后每个神经单元都要连到上一层的所有神经单元（作为输入），所以就会有上一层的神经单元数这么多列</p>
<h3 id="向量化"><a class="anchor" href="#向量化">#</a> 向量化</h3>
<p>Z[1]⏟(n[1],m)=W[1]⏟(n[1],n[0])X⏟(n[0],m)+b[1]⏟(n[1],1)</p>
<blockquote>
<p>Z[1]=[||⋯|z<a href="1">1</a>z<a href="2">1</a>⋯z<a href="m">1</a>||⋯|]</p>
<p>对于 b [1]，python 会广播化：b [1]:(n [1],1)→(n [1],m)</p>
</blockquote>
<p>Z[l],A[l]:(n[l],m)</p>
<p>dZ [l],dA [l]:(n [l],m) 与 Z [l],A [l] 的维度相等</p>
<h3 id="总结"><a class="anchor" href="#总结">#</a> 总结</h3>
<p>最好在每次矩阵乘法后加一个 assert 语句，方便找出 shape 错误</p>
<pre><code>    # 隐藏层
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    # 输出层
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))
    assert (W1.shape == (n_h, n_x))
    assert (b1.shape == (n_h, 1))
    assert (W2.shape == (n_y, n_h))
    assert (b2.shape == (n_y, 1))
</code></pre>
<h2 id="为什么使用深层表示"><a class="anchor" href="#为什么使用深层表示">#</a> 为什么使用深层表示？</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419121019718.png?lastModify=1726287112" alt="image-20240419121019718" /></p>
<p>如果在建一个人脸识别或是人脸检测系统，深度神经网络所做的事就是，当你输入一张脸部的照片，然后你可以把深度神经网络的<strong>第一层</strong>， 当成一个特征探测器或者边缘探测器。</p>
<blockquote>
<p>在上述例子，有 20 个隐藏单元的深度神经网络，隐藏单元就是这些图里这些小方块</p>
<p>例如：</p>
<ul>
<li>小方块（第一行第一列）就是一个隐藏单元，它会去找这张照片里 “|” 边缘的方向。</li>
<li>隐藏单元（第四行第四列），可能是在找（“—”）水平向的边缘在哪 里。</li>
</ul>
</blockquote>
<p>可以先把神经网络的第一层当作看图，然后去找这张照片的各个边缘。</p>
<ul>
<li>
<p>我们可以把照片里组成边缘的像素们放在一起看，然后它可以把被探测到的边缘<strong>组合</strong>成面部的不同部分（第二张大图）。</p>
<blockquote>
<p>比如说，可能有一个神经元会去找眼睛的部分，另外还有别的在找鼻子的部分，然后把这许多的边缘结合在一起，就可以开始检测人脸的不同部分。</p>
</blockquote>
</li>
<li>
<p>最后再把这些部分<strong>放在一起</strong>，比如鼻子眼睛下巴，就可以识别或是探测不同的人脸 （第三张大图）。</p>
</li>
</ul>
<p>边缘探测器：是针对照片中非常小块的面积。</p>
<p>面部探测器：会针对于大一些的区域。</p>
<blockquote>
<p>一般会从比较小的细节入手， 比如边缘，然后再一步步到更大更复杂的区域，比如一只眼睛或是一个鼻子，再把眼睛鼻子装一块组成更复杂的部分。</p>
</blockquote>
<p><strong>语音识别系统</strong>：需要解决的就是如何可视化语音。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419121643093.png?lastModify=1726287112" alt="image-20240419121643093" /></p>
<blockquote>
<p>比如你输入一个音频片段，那么神经网络的第一层可能就会去先开始试着探测比较<strong>低层次的音频波形</strong>的一些特征，</p>
<ul>
<li>比如音调是变高了还是低了，分辨白噪音，咝咝咝的声音，或者音调，</li>
</ul>
<p>可以选择这些相对程度比较低的波形特征，然后把这些波形<strong>组合</strong>在一起 就能去探测<strong>声音的基本单元</strong>。</p>
<p>在语言学中有个概念叫做音位，</p>
<ul>
<li>比如说单词 ca，c 的发音， “嗑” 就是一个音位，a 的发音 “啊” 是个音位，t 的发音 “特” 也是个音位</li>
</ul>
<p>有了基本的声音单元以后，<strong>组合</strong>起来，你就能识别音频当中的<strong>单词</strong>，单词再<strong>组合</strong>起来就能识别<strong>词组</strong>，再到完整的<strong>句子</strong>。</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419121935752.png?lastModify=1726287112" alt="image-20240419121935752" /></p>
<p>深度神经网络的这许多隐藏层中</p>
<p>前几层：较早的前几层能学习一些低层次的<strong>简单</strong>特征</p>
<blockquote>
<p>相对简单的输入函数，比如图像单元的边缘</p>
</blockquote>
<p>后几层：就能把简单的特征结合起来，去探测更加<strong>复杂</strong>的东西。</p>
<blockquote>
<p>很多复杂的事，比如探测面部或是探测单词、短语或是句子。</p>
</blockquote>
<p>深层的网络隐藏单元数量相对较少，隐藏层数目较多</p>
<p>如果浅层的网络想要达到同样的计算结果则需要<strong>指数级</strong>增长的单元数量才能达到。</p>
<p>例如：</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419122932537.png?lastModify=1726287112" alt="image-20240419122932537" /></p>
<blockquote>
<p>说如果只用或门， 还有非门的话，可能会需要几层才能计算异或函数。左图对应的深度：O (log (n))</p>
<p>那么节点的数量和电路部件，或是门的数量并不会很大，也不需要太多门去计算异或。</p>
<hr />
<p>单隐藏层来计算，那么要计算奇偶性，或者异或关系函数就需要这一隐层（上图右方框部分）的单元数呈指数增长才行</p>
</blockquote>
<h2 id="搭建神经网络块"><a class="anchor" href="#搭建神经网络块">#</a> 搭建神经网络块</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419125553425.png?lastModify=1726287112" alt="image-20240419125553425" /></p>
<p>layer l:W[l],b[l]</p>
<p>Forward:Input a[l−1],output a[l]</p>
<blockquote>
<p>z[l]=W[l]a[l−1]+b[l]</p>
<p>a[l]=g<a href="z%5Bl%5D">l</a></p>
</blockquote>
<p>Backward:Input da[l],output da[l−1],dw[l],db[l]</p>
<blockquote>
<p>da [l] 使用来计算 dz [l]，然后 dz [l]→da [l−1]</p>
<p>cache(z[l])</p>
</blockquote>
<p>计算反向传播</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419130355659.png?lastModify=1726287112" alt="image-20240419130355659" /></p>
<p>然后如果实现了这两个函数（正向和反向），然后神经网络的计算过程会是这样的：</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419131029617.png?lastModify=1726287112" alt="image-20240419131029617" /></p>
<h2 id="前向传播和后向传播"><a class="anchor" href="#前向传播和后向传播">#</a> 前向传播和后向传播</h2>
<h3 id="forward-propagation"><a class="anchor" href="#forward-propagation">#</a> Forward propagation</h3>
<p>Input a[l−1]</p>
<p>Output a[l],cache(z[l])</p>
<p>z[l]=W[l]a[l−1]+b[l],a[l]=g<a href="z%5Bl%5D">l</a></p>
<p>向量化</p>
<p>Z[l]=W[l]A[l−1]+b[l],A[l]=g<a href="Z%5Bl%5D">l</a></p>
<h3 id="backward-propagation"><a class="anchor" href="#backward-propagation">#</a> Backward propagation</h3>
<p>Input da[l]</p>
<p>Output da[l−1],dW[l],db[l]</p>
<p>dz[l]=da[l]∗g[l]′(z[l])</p>
<blockquote>
<p>dz[l]⏟(n[l],1)=W[l+1]Tdz[l+1]⏟(n[1],1)∗g[l]′(z[l])⏟(n[1],1)</p>
</blockquote>
<p>dW[l]=dz[l]a[l−1]T</p>
<p>db[l]=dz[l]</p>
<p>da[l−1]⏟(n[l−1],1)=W[l]T⏟(n[l−1],n[l])dz[l]⏟(n[l],1)</p>
<p>向量化</p>
<p>dZ[l]=dA[l]∗g[l]′(Z[l])</p>
<blockquote>
<p>dZ[l]⏟(n[l],m)=W[l+1]TdZ[l+1]⏟(n[1],m)∗g[l]′(Z[l])⏟(n[1],m)</p>
</blockquote>
<p>dW[l]=1mdZ[l]A[l−1]T</p>
<p>db[l]=1mnp.sum(dZ[l],axis=1,keepdims=True)</p>
<p>dA[l−1]⏟(n[l−1],m)=W[l]T⏟(n[l−1],n[l])dZ[l]⏟(n[l],m)</p>
<h3 id="summary"><a class="anchor" href="#summary">#</a> Summary</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419141203989.png?lastModify=1726287112" alt="image-20240419141203989" /></p>
<h2 id="参数-vs-超参数"><a class="anchor" href="#参数-vs-超参数">#</a> 参数 VS 超参数</h2>
<p>参数：W [1],b [1],W [2],b [2],W [3],b [3]⋯</p>
<p>超参数：比如算法中的 learning rate 𝑎（学习率）、iterations (梯度下降法循环的数量)、𝐿（隐藏层数目）、n [l]（隐藏层单元数目）、choice of activation function（激活函数的选择）都需要 你来设置</p>
<p>这些数字实际上控制了最后的参数 w 和 b 的值，所以它们被称作超参数。</p>
<blockquote>
<p>还有其他超参数：如 momentum、mini batch size、regularization parameters 等等。</p>
</blockquote>
<p>寻找参数的最优值</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240419143053117.png?lastModify=1726287112" alt="image-20240419143053117" /></p>
<blockquote>
<p>不断调整 α 的值，观察效果，成本函数等等</p>
</blockquote>
<p>通常，必须尝试很多不同的值，并走这个循环，试试各种参数。</p>
<blockquote>
<p>试试看 5 个隐藏层，这个数目的隐藏单元，实现模型并观察是否成功，然后再迭代。</p>
</blockquote>
<p>总结：超参数需要经验来设置值，新手需要一个个试验，看成本函数提升还是降低从而调参（<strong>炼丹</strong>）</p>
<h1 id="改善深层神经网络"><a class="anchor" href="#改善深层神经网络">#</a> 改善深层神经网络</h1>
<h2 id="训练验证测试集"><a class="anchor" href="#训练验证测试集">#</a> 训练，验证，测试集</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420145741377.png?lastModify=1726287112" alt="image-20240420145741377" /></p>
<p>应用深度学习是一个典型的迭代过程，需要多次<strong>循环往复</strong>，才能为应用程序找到一个称心的神经网络，因此循环该过程的效率是决定项目进展速度的一个关键因素</p>
<p>而创建高质量的训练数据集，验证集和测试集也有助于提高循环效率。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420150840952.png?lastModify=1726287112" alt="image-20240420150840952" /></p>
<p>通常会将这些数据划分成几部分</p>
<ul>
<li>
<p>训练集</p>
</li>
<li>
<p>简单交叉验证集（验证集）</p>
<blockquote>
<p>验证不同算法，检验哪种算法更有效</p>
</blockquote>
</li>
<li>
<p>测试集</p>
</li>
</ul>
<p>接下来开始对训练执行算法，通过<strong>验证集或简单交叉验证集</strong>选择最好的模型。经过充分验证，我们选定了最终模型，然后就可以在测试集上进行评估了，为了无偏评估算法的运行状况。</p>
<h3 id="三七分"><a class="anchor" href="#三七分">#</a> 三七分</h3>
<p>常见的就是是将所有数据<strong>三七分</strong></p>
<blockquote>
<p>70% 验证集，30% 测试集</p>
<p>也可以按照 60% 训练，20% 验证和 20% 测试集来划分。</p>
<p>若只有 100 条，1000 条或者 1 万条数据，那么上述比例划分是非常合理的。</p>
</blockquote>
<h3 id="大数据"><a class="anchor" href="#大数据">#</a> 大数据</h3>
<p>在<strong>大数据</strong>时代，我们现在的数据量可能是百万级别，那么<strong>验证集和测试集</strong>占数据总量的比例会趋向于变得更小。</p>
<blockquote>
<p>因为验证集的目的就是验证不同的算法，检验哪种算法更有效</p>
<p>因此，验证集要足够大才能评估，比如 2 个甚至 10 个不同算法，并迅速判断出哪种算法更有效。</p>
<p>我们可能不需要拿出 20% 的数据作为验证集。</p>
</blockquote>
<p>比如我们有 100 万条数据，那么取 1 万条数据便足以进行评估，找出其中表现最好的 1-2 种算法。</p>
<p>同样地，根据最终选择的分类器，<strong>测试集</strong>的主要目的是正确评估分类器的<strong>性能</strong>（最终的成本偏差）</p>
<ul>
<li>所以，如果拥有百万数据，我们只需要 1000 条数据，便足以评估单个分类器，并且准确评估该分类器的性能。</li>
</ul>
<blockquote>
<p>假设我们有 100 万条数据，其中 1 万条作为验证集，1 万条作为测试集，100 万里取 1 万，比例是 1%，</p>
<ul>
<li>即：训练集占 98%，验证集和测试集各占 1%。</li>
</ul>
<p>对于数据量过百万的应用，训练集可以占到 99.5%，验证和测试集各占 0.25%，或者验证集占 0.4%，测试集占 0.1%。</p>
</blockquote>
<h3 id="总结-2"><a class="anchor" href="#总结-2">#</a> 总结</h3>
<p>通常将样本分成训练集，验证集和测试集三部分</p>
<ul>
<li>数据集规模相对较小，适用传统的划分比例</li>
<li>数据集规模较大的，验证集和测试集要小于数据总量的 20% 或 10%。</li>
</ul>
<h3 id="分布不匹配"><a class="anchor" href="#分布不匹配">#</a> 分布不匹配</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420151855576.png?lastModify=1726287112" alt="image-20240420151855576" /></p>
<p>Training set：可能是从网络上抓下来的图片</p>
<blockquote>
<p>分辨率很高，很专业， 后期制作精良</p>
</blockquote>
<p>Dev/test sets：用户上传的照片可能是用手机随意拍摄的，像素低，比较模糊</p>
<p>针对这种情况，根据经验，建议大家要确保<strong>验证集和测试集</strong>的数据来自<strong>同一分布</strong></p>
<blockquote>
<p>因为要用验证集来<strong>评估</strong>不同的模型，尽可能地优化性能。</p>
</blockquote>
<p>就算没有测试集也不要紧</p>
<ul>
<li>测试集的目的是对最终所选定的神经网络系统做出<strong>无偏估计</strong></li>
</ul>
<p>如果不需要无偏估计，也可以不设置测试集。</p>
<p>所以如果只有<strong>验证集</strong>，没有测试集。要做的就是，在<strong>训练集</strong>上训练，尝试<strong>不同</strong>的模型框架，在<strong>验证集</strong>上评估这些模型，然后迭代并选出适用的模型。</p>
<blockquote>
<p>因为验证集中已经<strong>涵盖测试集</strong>数据，其不再提供无偏性能评估。</p>
</blockquote>
<p>在实际应用中，人们只是把测试集当成简单交叉验证集使用，并没有完全实现该术语的功能</p>
<blockquote>
<p>因为他们把验证集数据<strong>过度拟合</strong>到了测试集中（“训练验证集”），导致太<strong>灵敏</strong></p>
</blockquote>
<h2 id="偏差-方差"><a class="anchor" href="#偏差-方差">#</a> 偏差、方差</h2>
<p>偏差和方差的权衡问题</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420153211309.png?lastModify=1726287112" alt="image-20240420153211309" /></p>
<p><strong>高偏差</strong>（high bias）的情况，我们称为 “<strong>欠拟合</strong>” （underfitting）</p>
<blockquote>
<p>如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合</p>
<p>像这种接近线性的分类器，数据拟合度低。</p>
</blockquote>
<p><strong>方差较高</strong>（high variance），数据<strong>过度拟合</strong>（overfitting）</p>
<blockquote>
<p>如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集</p>
</blockquote>
<p>“<strong>适度拟合</strong>”（just right）</p>
<blockquote>
<p>复杂程度适中，数据拟合适度的分类器， 这个数据拟合看起来更加合理</p>
</blockquote>
<p><strong>线性高偏差（欠拟合），过度拟合高方差</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420153919888.png?lastModify=1726287112" alt="image-20240420153919888" /></p>
<p>理解偏差和方差的两个<strong>关键数据</strong></p>
<ul>
<li><strong>训练集</strong>误差（Train set error）和<strong>验证集</strong>误差（Dev set error）</li>
</ul>
<blockquote>
<p>例如：假定训练集误差是 1%，为了方便论证，假定验证集误差是 11%</p>
<p>可以看出训练集设置得非常好，而验证集设置相对较差</p>
<p>我们可能<strong>过度拟合</strong>了训练集</p>
<p>在某种程度上，验证集并<strong>没有充分利用</strong>交叉验证集的作用</p>
<p>像这种情况，我们称之为 “高方差”。</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420154525903.png?lastModify=1726287112" alt="image-20240420154525903" /></p>
<table>
<thead>
<tr>
<th>Train set error</th>
<th>Dev set error</th>
<th>result</th>
</tr>
</thead>
<tbody>
<tr>
<td>1%</td>
<td>11%</td>
<td>训练集：过度拟合 &lt;br /&gt; 验证集：没有充分利用</td>
</tr>
<tr>
<td>15%</td>
<td>16%</td>
<td>训练数据的拟合度不高，就是数据<strong>欠拟合</strong>（<strong>偏差较高</strong>）&lt;br /&gt; 相反，验证集：合理。验证集中的错误率只比训练集的多了 1%</td>
</tr>
<tr>
<td>15%</td>
<td>30%</td>
<td>训练集误差是 15%，<strong>high bias</strong>&lt;br /&gt; 验证集的评估结果更糟糕， 错误率达到 30%，<strong>high variance</strong></td>
</tr>
<tr>
<td>0.5%</td>
<td>1%</td>
<td><strong>low bias, low variance</strong></td>
</tr>
</tbody>
</table>
<p><strong>偏差</strong>：刚开始拟合的曲线或者模型的拟合程度高低（训练集）</p>
<ul>
<li>模型预测值与真实值之间的差距</li>
</ul>
<p><strong>方差</strong>：拟合后的曲线和验证集数据的方差大小（验证集错误率 - 数据集错误率）</p>
<ul>
<li><strong>太依赖与数据集。</strong></li>
</ul>
<h3 id="贝叶斯误差最优误差"><a class="anchor" href="#贝叶斯误差最优误差">#</a> 贝叶斯误差（最优误差）</h3>
<p>这些分析都是基于假设预测的，假设<strong>人眼</strong>辨别的错误率接近 0%。一般来说，最优误差也被称为<strong>贝叶斯误差</strong>，所以， 最优误差接近 0%。</p>
<blockquote>
<p>Human: ≈ 0%</p>
<p>Optimum (Bayes) error : ≈ 0%</p>
</blockquote>
<p>如果最优误差或贝叶斯误差非常高，比如 15%。</p>
<blockquote>
<p>对于：分类器（训练误差 15%，验证误差 16%），</p>
<p>15% 的错误率对<strong>训练集</strong>来说也是非常合理的，偏差不高，方差也非常低。</p>
</blockquote>
<h3 id="总结-3"><a class="anchor" href="#总结-3">#</a> 总结</h3>
<p>看训练集误差，是否有偏差问题，然后看错误了有多高</p>
<p>验证集验证时，判断方差是否过高</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420160103849.png?lastModify=1726287112" alt="image-20240420160103849" /></p>
<blockquote>
<p>对于紫色部分：即高偏差，又高方差</p>
<ul>
<li>
<p>高偏差：几乎是一条线性分类器，并未拟合数据。</p>
</li>
<li>
<p>高方差：紫色线中间部分灵活性非常高，却过度拟合了这两个样本</p>
<p>采用曲线函数或二次元函数会产生高方差，因为它曲线灵活性太高以致拟合了这两 个错误样本和中间这些活跃数据。</p>
</li>
</ul>
</blockquote>
<p><strong>欠拟合</strong>：没把大头当回事</p>
<p><strong>过度拟合</strong>：将偶然当太真</p>
<h2 id="机器学习基础"><a class="anchor" href="#机器学习基础">#</a> 机器学习基础</h2>
<p>初始模型训练完成后：我首先要知道算法的偏差高不高</p>
<ul>
<li>
<p>偏差较高，试着评估训练集或训练数据的性能</p>
</li>
<li>
<p>如果偏差的确很高， 甚至无法拟合训练集，那么你要做的就是选择一<strong>个新的网络</strong></p>
<blockquote>
<p>比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>偏差高：欠拟合，增加模型复杂度，先提升训练集上的性能</p>
</blockquote>
<p>尝试更大网络结构和更长的时间来解决贝叶斯误差，直到可以很好地拟合训练集，至少可以 拟合或者过拟合训练集。</p>
<p>一旦偏差降低到可以接受的数值，检查一下<strong>方差</strong>有没有问题</p>
<p>需要查看验证集性能，从训练集推断出验证集是否理想。</p>
<ul>
<li>
<p>若方差高（过拟合）：扩充数据集、<strong>正则化</strong>、或者更合适的神经网络框架</p>
<blockquote>
<p>正则化时会出现<strong>偏差方差权衡</strong>问题，偏差可能略有<strong>增加</strong>，如果网络足够大，增幅通常不会太高</p>
</blockquote>
</li>
</ul>
<p>总之不断地尝试。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420183901023.png?lastModify=1726287112" alt="image-20240420183901023" /></p>
<p>有时有很多选择，减少偏差或方差而不增加另一方。 最终，会得到一个非常规范化的网络。</p>
<h2 id="正则化"><a class="anchor" href="#正则化">#</a> 正则化</h2>
<h3 id="logistic-回归-2"><a class="anchor" href="#logistic-回归-2">#</a> logistic 回归</h3>
<p>过拟合问题题 —— 高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据。</p>
<p>但可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少网络中误差。</p>
<p>用 logistic 回归来实现：</p>
<p>J(w,b)=1mΣi=1m⁡L(y<sup>(i),y(i))→J(w,b)=1mΣi=1m⁡L(y</sup>(i),y(i))+λ2m||w||22</p>
<blockquote>
<p>定义 12m 方便求导</p>
</blockquote>
<p>L2 regularization:||w||22=Σj=1nx⁡wj2=Σj=1nx⁡wTw</p>
<blockquote>
<p>向量参数 w 的欧几里德范数（2 范数）的平方</p>
</blockquote>
<p>为什么不加上 12mb2 呢？</p>
<blockquote>
<p>因为 w 通常是一个高维参数矢量，已经可以表达高偏差问题</p>
<p>w 可能包含有很多参数， 我们不可能拟合所有参数</p>
<p>而  b 只是单个数字，所以 w 几乎涵盖<strong>所有</strong>参数，而不是 b，如果加了参数 b，其实也没太大影响，因为 b 只是众多参数中的<strong>一个</strong></p>
</blockquote>
<p>对于 L1 regularization:λ2mΣi=1nx⁡|wi|=λ2m||w||1</p>
<blockquote>
<p>如果用的是 L1 正则化，w 最终会是<strong>稀疏的</strong></p>
<ul>
<li>也就是说 w 向量中有很多 0</li>
</ul>
<p>有人说这样有利于压缩模型，因为集合中参数均为 0，存储模型所占用的内存更少。</p>
<p>实际上，虽然 L1 正则化使模型变得稀疏，却没有降低太多存储<strong>内存</strong>，所以这并不是 L1 正则化的目的， 至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用 L2 正则化。</p>
</blockquote>
<p>λ 是正则化参数</p>
<blockquote>
<p>通常使用验证集或交叉验证集来配置这个参数，尝试各种各样的数据，寻找最好的参数</p>
<p>要考虑训练集之间的权衡，把参数设置为较小值，这样可以避免过拟合</p>
</blockquote>
<h3 id="神经网络"><a class="anchor" href="#神经网络">#</a> 神经网络</h3>
<p>J(w[1],b[1],w[2],b[2],⋯,w[L],b[L])=1mΣi=1m⁡L(y^(i),y(i))+λ2mΣl=1L⁡||W[l]||F2</p>
<p>||W[l]||F2=Σi=1n[l]⁡Σj=1n[l−1]⁡(Wij[l])2</p>
<blockquote>
<p>矩阵中所有元素的平方和</p>
</blockquote>
<p>dWl=(from backprop)+λmW[l]</p>
<blockquote>
<p>原来的导数 + 多出来的一项对  W 的导数</p>
</blockquote>
<ul>
<li>
<p>为什么只需修改 dW [l]，不修改 dZ [l],db [l] 呢？</p>
<blockquote>
<p>J 只多出来了与 W 有关的项</p>
</blockquote>
</li>
</ul>
<p>W[l]:=W[l]−αdW[l]</p>
<p>​	=W[l]−α[(from backprop)+λmW[l]]</p>
<p>​	=(1−αλm)W[l]−α(from backprop)</p>
<blockquote>
<p>系数 1−αλm 小于 1</p>
</blockquote>
<h2 id="为什么正则化有利于预防过拟合呢"><a class="anchor" href="#为什么正则化有利于预防过拟合呢">#</a> 为什么正则化有利于预防过拟合呢？</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420203659448.png?lastModify=1726287112" alt="image-20240420203659448" /></p>
<p>J(w[l],b[l])=1mΣi=1m⁡L(y^(i),y(i))+λ2mΣl=1L⁡||W[l]||F2</p>
<p>添加正则项，它可以<strong>避免数据权值矩阵 W [l] 过大</strong>，这就是弗罗贝尼乌斯范数，为什么压缩 L2 范 数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？</p>
<p>如果正则化 λ 设置得足够大，权重矩阵 W [l]≈0</p>
<blockquote>
<p>当正则化参数 λ 设置得足够大时，会导致 (1−αλm) W [l] 这一项的影响变得显著。</p>
<p>所以当 λ 很大时，这一项的值会接近于零，尤其是当 αλm≫1 时。</p>
</blockquote>
<p>这样，整个更新公式可以近似为：</p>
<p>[ −α⋅(from backprop)]</p>
<blockquote>
<p>这意味着权重 W [l] 的更新主要取决于<strong>反向传播</strong>算法得到的梯度，而<strong>不</strong>受到正则化项的影响。</p>
</blockquote>
<p>在这种情况下，如果网络的损失函数足够小，并且没有受到正则化项的影响，那么权重 W [l]  可能会趋于较小的值，甚至接近于<strong>零</strong></p>
<blockquote>
<p>因为没有其他因素来保持它们的值较大。</p>
</blockquote>
<p>直观理解就是把多隐藏单元的权重设为 0，于是基本上消除了这些<strong>隐藏单元</strong>的许多影响</p>
<ul>
<li>
<p>例如：Z [l]=WA [l−1]+b,W≈=0→Z [l]=b</p>
<blockquote>
<p>这会直接影响神经元的激活值。因此，这个神经元在计算中的作用<strong>被减弱了</strong>或者消失了，这就是正则化导致的影响之一。</p>
<p>该隐藏单元对其他隐藏单元的影响会变小或者消失。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420205130962.png?lastModify=1726287112" alt="image-20240420205130962" /></p>
<p>这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从<strong>过度拟合</strong>的状态更接近左图的<strong>高偏差状态</strong>。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420205221156.png?lastModify=1726287112" alt="image-20240420205221156" /></p>
<p>过度拟合（高方差） → 欠拟合（高偏差）</p>
</blockquote>
<p>但是 λ 会存在一个中间值，于是会有一个接近 <strong>“Just Right”</strong> 的中间状态。</p>
<p>过拟合是由于学习的权重太多导致的，预防过拟合就是减少学习的权重数量（因为加上正则向后，就会让某些权重约为 0，减少权重数量）</p>
<blockquote>
<p>通过正则化，可以减少学习的权重数量，从而减少模型的复杂性，有助于预防过拟合。</p>
</blockquote>
<p>例如：采用 tanh 激活函数</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420211130449.png?lastModify=1726287112" alt="image-20240420211130449" /></p>
<p>λ↑	W[l]↓	z[l]=Wla[l−1]+b[l]↓</p>
<blockquote>
<p>z [l] 最终在这个范围之内，都是相对较小的值，g (z) 大致呈线性，每层几乎都是线性的，和线性回归函数一样。</p>
</blockquote>
<p>如果每层都是线性的，那么整个网络就是一个线性网络，即使是一个非常深的深层网络，因具有线性激活函数的特征，最终我们只能计算线性函数</p>
<blockquote>
<p>因此， 它不适用于非常复杂的决策，以及过度拟合数据集的非线性决策边界，如同过度拟合高方差的情况。</p>
</blockquote>
<h2 id="dropout-正则化"><a class="anchor" href="#dropout-正则化">#</a> dropout 正则化？</h2>
<p>除了 L2 正则化，还有一个非常实用的正则化方法 ——<strong>“Dropout</strong>（随机失活）”</p>
<p>复制这个神经网络，<strong>dropout</strong> 会遍历网络的每一层，并设置消除神经网络中节点的概率。</p>
<blockquote>
<p>要了解 <strong>dropout</strong> ，可以思考与朋友进行以下对话：</p>
<ul>
<li>朋友：“为什么你需要所有神经元来训练你的网络以分类图像？”。</li>
<li>你：“因为每个神经元都有权重，并且可以学习图像的特定特征 / 细节 / 形状。我拥有的神经元越多，模型学习的特征就越丰富！”</li>
<li>朋友：“我知道了，但是你确定你的神经元学习的是不同的特征而不是全部相同的特征吗？”</li>
<li>你：“这是个好问题…… 同一层中的神经元实际上并不关联。应该绝对有可能让他们学习相同的图像特征 / 形状 / 形式 / 细节... 这是多余的。为此应该有一个解决方案。”</li>
</ul>
<p>当你关闭某些神经元时，实际上是在修改模型。<strong>dropout</strong> 背后的想法是，在每次迭代中，你将训练仅使用神经元子集的不同模型。通过 <strong>dropout</strong> ，你的神经元对另一种特定神经元的激活变得不那么敏感，因为另一神经元可能随时关闭。</p>
</blockquote>
<ul>
<li>假设网络中的每一层，每个节点得以保留和消除的概率都是 0.5</li>
<li>设置完节点概率，会消除一些节点，然后删除掉从该节点进出的连线</li>
<li>最后得到一个节点更少，规模更小的网络，然后用 <strong>backprop</strong> 方法进行训练。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420213157523.png?lastModify=1726287112" alt="image-20240420213157523" /></p>
<p><strong>Inverted  dropout（反向随机失活）</strong></p>
<p>d3：一个第三层的 <strong>dropout</strong> 向量</p>
<blockquote>
<p>d3 = np.random.rand(a3.shape[0],a3.shape[1]) &lt; keep-prob</p>
</blockquote>
<ul>
<li>
<p><strong>keep-prob</strong>：保留某个隐藏单元的概率，此处为 0.8</p>
<blockquote>
<p>意味着消除任意一个隐藏单元的概率是 0.2</p>
</blockquote>
</li>
<li>
<p>a3：第三层的激活值</p>
</li>
<li>
<p>d3：一个矩阵，<strong>每个隐藏单元和每个样本</strong>（单元数，样本数），其中 d3 中的对应值为 1 的概率都是 0.8，对应为 0 的概率是 0.2，</p>
</li>
</ul>
<p>a3=np.multiply(a3,d3)</p>
<blockquote>
<p>表示 d3 对应的 a3 位置将 0 的部分抹除掉</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420214352659.png?lastModify=1726287112" alt="image-20240420214352659" /></p>
</blockquote>
<p>向外扩展 a3/=keep−prob</p>
<blockquote>
<p>若第三层有 50 个神经单元，保留和删除它们的概率分别为 80% 和 20%。</p>
<p>这意味着最后被删除或归零的单元平均有 10（50×20%=10）个</p>
</blockquote>
<p>z[4]=W[4]a[3]+b[4]</p>
<p>a [3] 会减少 20%（20% 的神经元会归 0），为了不影响 z [4] 的期望值，需要将 a [3]/0.8，它将会修正或弥补我们所需的那 20%</p>
<blockquote>
<p>因为消除了 20% 的神经元导致总体均值变小，为保证均值不变，将剩余 80% 的神经元 /0.8 增大总体值</p>
</blockquote>
<p>也就是产生的影响为原来的 <strong>keep-prob</strong> 倍，因此要除以 <strong>keep-prob</strong>，使得 <strong>dropout</strong> 后的 a [3] 的期望和原来的一致</p>
<ul>
<li><strong>保证 a [3] 期望不变</strong></li>
</ul>
<p>反向传播对 dA [l] 同理 ？</p>
<blockquote>
<p>本 up 认为在反向传播不需要使用 <strong>dropout</strong></p>
<p>在正向传播中，使用 <strong>dropout</strong> A [L] 会变化，而梯度跟 A [L],W [l] 有关。</p>
<ul>
<li>在反向传播中，使用的是正向传播期间保留下来的激活值</li>
</ul>
</blockquote>
<p>不同的训练样本，清除不同的隐藏单元也不同。 实际上，如果你通过相同训练集多次传递数据，每次训练数据的梯度不同，则随机对不同隐藏单元归零，有时却并非如此。</p>
<blockquote>
<p>比如：</p>
<p>需要将<strong>相同</strong>隐藏单元归零，第一次迭代梯度下降 时，把一些隐藏单元归零</p>
<p>第二次迭代梯度下降时，也就是第二次遍历训练集时，对<strong>不同</strong>类型的隐藏层单元归零。</p>
</blockquote>
<p>在测试阶段不使用 dropout 函数</p>
<blockquote>
<p>No dropout</p>
<p>z[1]=W[1]a[0]+b[1],a[1]=g<a href="z%5B1%5D">1</a></p>
<p>z[2]=W[2]a[1]+b[2],a[2]=g<a href="z%5B2%5D">2</a></p>
<p>⋯</p>
</blockquote>
<p>在测试阶段进行预测时，我们不期望输出结果 y^ 是随机的</p>
<ul>
<li>如果测试阶段应用 dropout 函数，预测会受到干扰。</li>
</ul>
<h2 id="理解-dropout"><a class="anchor" href="#理解-dropout">#</a> 理解 dropout</h2>
<p><strong>dropout</strong> 随机删除神经单元，就好像每次迭代之后，神经网络都会变得比以前更小</p>
<blockquote>
<p>因此采用一个较小神经网络好像和使用正则化的效果是一样的</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420225759552.png?lastModify=1726287112" alt="image-20240420225759552" /></p>
<blockquote>
<p>有时这两个单元会被删除， 有时会删除其它单元</p>
</blockquote>
<p>也就是说：紫色的这个单元，它不能依靠任何特征</p>
<p>不应该给任何一个输入加上<strong>太多权重</strong>，因为它可能会被删除。</p>
<ul>
<li>因此紫色单元将通过这种方式积极地传播开，并为单元的四个输入给予一点权重，通过传播所有权重</li>
</ul>
<p><strong>dropout</strong> 将产生收缩权重的平方范数的效果</p>
<blockquote>
<p>实施 dropout 的结果是它会收缩权重，并完成一些预防过拟合的外层正则化。</p>
</blockquote>
<p><strong>dropout</strong> 被正式地作为一种正则化的替代形式</p>
<p>权重衰退的方式不同</p>
<ul>
<li>一个是 L2 范数有超参数 λ 控制</li>
<li>一个是减少神经元从而减少权重</li>
</ul>
<p>例如</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240420231046491.png?lastModify=1726287112" alt="image-20240420231046491" /></p>
<p>其中 W [2] 是最大的权重矩阵 (7,7)，为了<strong>预防</strong>矩阵的<strong>过拟合</strong>，他的 <strong>keep-prob</strong> 值应该相对较低（假设 0.05）</p>
<p>对于其它层，过拟合的程度可能没那么严重，它们的 <strong>keep-prob</strong> 值可能高一些，这里是 0.7。</p>
<p>如果<strong>不必担心</strong>其过拟合的问题，那么 <strong>keep-prob</strong> 可以为 1（保留所有单元）</p>
<p>对于输出层：<strong>keep-prob</strong> 的值为 1</p>
<p>除非算法过拟合，不然是不会使用 <strong>dropout</strong> 的，所以它在其它领域应用得比较少，主要存在于计算机视觉领域，因为通常没有足够的数据，所以一直存在过拟合</p>
<blockquote>
<p>像素多所以导致样本少，没法通过增加输入样本的方法降低过拟合，只能通过正则化</p>
</blockquote>
<p>大缺点：代价函数 J 不再被明确定义</p>
<p><strong>dropout</strong>  在训练过程中随机地丢弃一部分神经元，导致每个样本的实际代价函数 J 都不同。</p>
<blockquote>
<p>这是因为每次训练迭代时，由于 <strong>dropout</strong> 的随机性，网络结构都会发生变化，因此相同的样本可能会对应不同的神经元被激活或者丢弃的情况，从而导致对应的代价函数 J 不同。</p>
</blockquote>
<p>每次迭代由于 <strong>dropout</strong>  的随机性质，可能会导致代价函数 J 的值在训练过程中出现波动</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423133056759.png?lastModify=1726287112" alt="image-20240423133056759" /></p>
</blockquote>
<p>通常会关闭 <strong>dropout</strong> 函数，将 <strong>keep-prob</strong> 的值设为 1，运行代码，确保 J 函数单调递减。然后打开 <strong>dropout</strong> 函数</p>
<p>通过逐步引入 <strong>dropout</strong>，通常可以避免出现较大幅度的向上波动</p>
<blockquote>
<ol>
<li>首先，在训练过程的最开始阶段，将 <strong>dropout</strong> 函数关闭，即将 <strong>keep-prob</strong> 参数设为 1，这样可以确保每次迭代的代价函数 J 是单调递减的。</li>
<li>等模型开始收敛到一个相对稳定的状态后，可以逐步打开 <strong>dropout</strong> 函数，即逐步减小 <strong>keep-prob</strong> 参数的值，引入 <strong>dropout</strong> 的随机性，以增加模型的鲁棒性和泛化能力。</li>
<li>在逐步引入 <strong>dropout</strong> 的过程中，可能需要对学习率等超参数进行微调，以确保模型的训练效果和收敛速度。</li>
</ol>
</blockquote>
<h2 id="其他正则化方法"><a class="anchor" href="#其他正则化方法">#</a> 其他正则化方法</h2>
<h3 id="数据扩增"><a class="anchor" href="#数据扩增">#</a> 数据扩增</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421132552089.png?lastModify=1726287112" alt="image-20240421132552089" /></p>
<p><strong>数据扩增</strong>：可以通过水平翻转图片、裁剪，训练集则可以增大一倍</p>
<h3 id="early-stopping"><a class="anchor" href="#early-stopping">#</a> early stopping</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421135221791.png?lastModify=1726287112" alt="image-20240421135221791" /></p>
<p>参数 W 接近 0</p>
<blockquote>
<p>因为随机初始化 W 值时，它的值可能都是较小的随机值</p>
</blockquote>
<p>所以在长期训练神经网络之前 W 依然很小，在迭代过程和训练过程中 W 的值会变得越来越大</p>
<blockquote>
<p>由于 dw&lt;0，每一次迭代后 w 更新后会逐步增大（也就是往右边走），直至逼近最小值点</p>
</blockquote>
<p><strong>early stopping</strong> 要做就是在中间点停止迭代过程，我们得到一个 W 值中等大小的弗罗贝尼乌斯范数（||W||F2），与 L2 正则化相似，选择参数 W 范数较小的神经网络</p>
<h4 id="early-stopping-缺点"><a class="anchor" href="#early-stopping-缺点">#</a> <strong>early stopping</strong> 缺点</h4>
<p>对于两个问题</p>
<blockquote>
<p>最优化 J (w,b)</p>
<p>防止过度拟合</p>
</blockquote>
<p><strong>early stopping</strong> 的主要缺点就是你不能独立地处理这两个问题</p>
<blockquote>
<p>因为提早停止梯度下降，也就是停止了优化代价函数 J</p>
<ul>
<li>因为现在你不再尝试降低代价函数 J，所以代价函数 J 的值可能不够小</li>
</ul>
</blockquote>
<p>同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是要考虑的东西变得更<strong>复杂</strong></p>
<p>若不用 <strong>early stopping</strong>，另一种方法就是 L2 正则化，训练神经网络的时间就可能很长。</p>
<blockquote>
<p>缺点在于，必须尝试很多正则化参数 λ 的值，这也导致搜索大量 λ 值的计算代价太高。</p>
</blockquote>
<h4 id="优点"><a class="anchor" href="#优点">#</a> 优点</h4>
<p>只运行一次梯度下降，你可以找出 W 的较小值，中间值和较大值，而无需尝试 L2 正则化超级参数 λ 的很多值。</p>
<h2 id="归一化输入normalizing-inputs"><a class="anchor" href="#归一化输入normalizing-inputs">#</a> 归一化输入（Normalizing inputs）</h2>
<p>训练神经网络，其中一个<strong>加速训练</strong>的方法就是归一化输入。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421141511239.png?lastModify=1726287112" alt="image-20240421141511239" /></p>
<blockquote>
<p>假设一个训练集有两个特征，输入特征为 2 维</p>
</blockquote>
<p>归一化需要两步骤：</p>
<ol>
<li>
<p>零均值（移动训练集）</p>
<blockquote>
<p>μ=1mΣi=1m⁡x(i)</p>
<p>x:=x−μ</p>
</blockquote>
</li>
<li>
<p>归一化方差</p>
<blockquote>
<p>σ2=1mΣi=1m⁡x(i)2</p>
<p>x/=σ</p>
</blockquote>
</li>
</ol>
<p>要用同样的方法调整测试集，而不是在训练集和测试集上分别预估 μ 和 σ2。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421142655335.png?lastModify=1726287112" alt="image-20240421142655335" /></p>
<p>对于代价函数 J (w,b)=1mΣi=1m⁡L (y^(i),y (i))，若没有使用归一化，则会表现出图 <strong>Unnormalized</strong> 情况</p>
<blockquote>
<p>若：</p>
<ul>
<li>x1:1⋯1000</li>
<li>x2:0⋯1</li>
</ul>
<p>结果：参数 w1 和 w2 值的范围或比率将会非常不同</p>
<ul>
<li>这些数据轴应该是 w1 和 w2，但直观理解，标记为 w 和 b</li>
</ul>
<p>代价函数就有点像狭长的碗一样</p>
</blockquote>
<p>对于归一化特征后， J 看起来更加匀称</p>
<p>对于在代价函数上运行<strong>梯度下降法</strong></p>
<ul>
<li>
<p>左下图（斜着的椭圆）：必须使用一个<strong>非常小</strong>的学习率。</p>
<blockquote>
<p>因为如果是在这个位置，梯度下降 法可能需要多次迭代过程，直到最后找到最小值。</p>
</blockquote>
</li>
<li>
<p>右下图（圆）：不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，你可以在梯度下降法中使用<strong>较大步长</strong>，而不需要像在左下图中那样反复执行。</p>
</li>
</ul>
<blockquote>
<p>前提：特征都在相似范围内</p>
</blockquote>
<p>一般是在 （-1，1） 范围内或相似偏差，这使得代价函数 J 优化起来更简单快速。</p>
<blockquote>
<p>如果假设特征 x1 范围在 0-1 之间，x2 的范围在 (-1,1) 之间，x3 范围在 1-2 之间， 它们是相似范围，所以会表现得很好。</p>
</blockquote>
<p>也就是：特征值处于<strong>相似范围</strong>，归一化就不很重要</p>
<p>如果输入特征处于<strong>不同范围</strong>内，可能有些特征值从 0 到 1 ，有些从 1 到 1000 ，那么归一化特征值就非常重要了</p>
<h2 id="梯度消失梯度爆炸vanishing-exploding-gradients"><a class="anchor" href="#梯度消失梯度爆炸vanishing-exploding-gradients">#</a> 梯度消失 / 梯度爆炸（Vanishing / Exploding gradients）</h2>
<p>梯度消失或梯度爆炸：训练神经网络的时候，<strong>导数</strong>或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421145721773.png?lastModify=1726287112" alt="image-20240421145721773" /></p>
<blockquote>
<p>g(z)=z,b[l]=0</p>
</blockquote>
<p>y^=W[l]W[l−1]W[l−2]⋯W[3]W[2]W[1]x</p>
<blockquote>
<p>z[1]=W[1]x</p>
<p>a[1]=g(z[1])=z[1]</p>
<p>a[2]=g(z[2])=g(w[2]a[1])</p>
<p>⋯</p>
</blockquote>
<p>若：W [l]=[1.5001.5],y^=W [L][1.5001.5] L−1x</p>
<blockquote>
<p>注：假设 W [L] 更 W [l] 相等</p>
</blockquote>
<p>这会导致 y^ 呈指数级增长，比率：1.5L</p>
<p>相反，若 W [l]=[0.5000.5],y^=W [L][0.5000.5] L−1x</p>
<ul>
<li>这会导致激活函数的值将以<strong>指数级下降</strong>，它是与网络层数数量 L 相关的函数，在深度网络中，激活函数以指数级递减。</li>
</ul>
<p>若 W [l] 略大于 1，激活函数将爆炸式增长</p>
<p>若 W [l] 略小于 1，激活函数将以指数级递减</p>
<blockquote>
<p>同理：与层数 L 相关的导数或梯度函数</p>
</blockquote>
<h2 id="神经网络的权重初始化weight-initialization-for-deep-networks"><a class="anchor" href="#神经网络的权重初始化weight-initialization-for-deep-networks">#</a> 神经网络的权重初始化（Weight Initialization for Deep  Networks）</h2>
<p>针对梯度消失和梯度爆炸问题，此节有助于为神经网络更谨慎地选择随机初始化参数</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421154714158.png?lastModify=1726287112" alt="image-20240421154714158" /></p>
<p>z=w1x1+w2x2+⋯+wnxn,b=0</p>
<blockquote>
<p>z=[w1w2⋯wn][x1x2⋯xn]</p>
</blockquote>
<p>为了预防 z 值过大或过小，你可以看到 n 越大，你希望 wi 越小</p>
<p>最合理的方法就是：wi=1n</p>
<p>W[l]=np.random.randn(shape)×np.sqrt(1n[l−1])</p>
<blockquote>
<p>若使用 Relu 激活函数，那么设置为 2m，效果会更好</p>
</blockquote>
<h3 id="xavier-初始化"><a class="anchor" href="#xavier-初始化">#</a> Xavier 初始化</h3>
<p>为什么在 W [l]=np.random.randn (shape)×np.sqrt (1n [l−1]) 中需要乘以 np.sqrt (1n [l−1])</p>
<p><strong>Xavier</strong> 初始化通过保持输入和输出之间的<strong>方差不变</strong>来尽可能避免梯度爆炸 / 消失。</p>
<blockquote>
<p>它假设每个神经元的输入和输出是从一个<strong>均值 μ 为零、方差 σ2 相同</strong>的分布中抽样得到的，而这个分布的方差可以通过输入和输出的维度来估计。</p>
</blockquote>
<p>对于权重矩阵 W [l] 采用高斯分布来初始化：N (0,σ2)</p>
<blockquote>
<p>若 W [l] 方差很大（取值范围非常<strong>广泛</strong>） → l 层的输出值 z 很大</p>
<p>→ 激活函数 a 的斜率很大（l 层的输出 z 就是 a 的输入）</p>
<p>→ l 层权重的梯度变大（l 层权重的梯度与激活函数的斜率成正比）</p>
<p>→ 可能引发梯度爆炸</p>
</blockquote>
<ul>
<li>
<p>主要<strong>零均值：</strong> 高斯分布的均值为零，这意味着在初始化权重时，期望值为零，<strong>不会引入任何偏差。</strong></p>
<blockquote>
<p>若 μ !=0，就会引入一个初始的偏差（记为 <strong>bias_orig</strong>）。假设 a=g (z)，且只有一个输出层。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421174306133.png?lastModify=1726287112" alt="image-20240421174306133" /></p>
<p>y^=Wx+b</p>
<p>当引入偏差之后，y^=Wx+b+bias_orig</p>
<p>也就是在基本预测函数的基础上加上了额外的偏差。</p>
<hr />
<p>即：<strong>本来预测函数与真实有一定的偏差，但是加上了额外的偏差，与真实偏差更大。</strong></p>
<ul>
<li>偏差：模型预测值与真实值之间的差距</li>
</ul>
</blockquote>
</li>
<li>
<p>其次<strong>对称性：</strong> 高斯分布是对称的，这意味着权重的<strong>正值和负值</strong>出现的概率相等，从而可以避免引入偏斜。</p>
</li>
</ul>
<p>那么某一个神经元输出也就是 z=w1x1+w2x2+⋯+wnxn=Σi=1nin⁡wixi,b=0</p>
<p>假设 x 的输入也具有均值 μ=0，方差 γ2</p>
<p>E(z)=Σi=1nin⁡E[wixi]=Σi=1nin⁡E[wi]E[xi]=0</p>
<p>D(z)=E(z2)−E(z)2=Σi=1nin⁡E[wi2xi2]−0=Σi=1nin⁡E[wi2]E[xi2]=ninσ2γ2</p>
<blockquote>
<p>E[w2]=D(w)−(E[w])2=D(w)=σ2</p>
<p>E[x2]=D(x)−(E[x])2=D(x)=γ2</p>
</blockquote>
<p>由于需要保证输入与输出的方差一致：D (z)=γ2</p>
<p>那么 ninσ2=1</p>
<p>若只考虑正向传播，那么 σ=1nin</p>
<blockquote>
<p>也就是 np.sqrt (1n [l−1])</p>
</blockquote>
<p>那么权重矩阵 W [l] 采用高斯分布来初始化：N (0,σ2)</p>
<ul>
<li>其中 σ=1n [l−1]</li>
</ul>
<blockquote>
<p>即：W [l]=np.random.randn (shape)×np.sqrt (1n [l−1])</p>
<ul>
<li>np.random.randn (shape) 默认服从 N (0,1)</li>
</ul>
</blockquote>
<p>若即考虑正向传播，也考虑到反向传播</p>
<p>那 ninσ2=1,noutσ2=1</p>
<blockquote>
<p>不可能同时满足二者</p>
</blockquote>
<p>只需满足：ninσ2+noutσ2=2</p>
<p>即：</p>
<p>(1)σ=2nin+nout</p>
<p>通常：x 也采用高斯分布</p>
<p>若  x 的输入采用均匀分布 U (−a,a)</p>
<blockquote>
<p>同高斯分布的对称性</p>
</blockquote>
<p>那么：σ2=(a−(−a)) 212=a23</p>
<p>带入 (1) 得出初始化值域：</p>
<p>U(−6nin+nout,6nin+nout)</p>
<h2 id="梯度的数值逼近numerical-approximation-of-gradients"><a class="anchor" href="#梯度的数值逼近numerical-approximation-of-gradients">#</a> 梯度的数值逼近（Numerical approximation of gradients）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421224025853.png?lastModify=1726287112" alt="image-20240421224025853" /></p>
<p>f=θ3</p>
<table>
<thead>
<tr>
<th>θ−ε</th>
<th>θ</th>
<th>θ+ε</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.99</td>
<td>1</td>
<td>1.01</td>
</tr>
</tbody>
</table>
<p>更准确的梯度预估：对小三角形 θ 与 θ+ε 两点计算高和宽的比值（单边误差）</p>
<p>对 θ−ε 和 θ+ε 两点处较大三角形的高比上宽（双边误差）</p>
<p>较大三角形的高宽比值更接近于 θ 的导数</p>
<ul>
<li>
<p>若使用右边，斜率偏大</p>
<p>使用左边，斜率偏小</p>
</li>
<li>
<p>证明（中心差分）</p>
<blockquote>
<p>中心差分思想：利用函数在 x 点左右两侧的取值来构造一个区间，并计算这个区间内的平均斜率</p>
<p>近似公式：f′(x)≈f (x+h)−f (x−h) 2h</p>
<hr />
<p>由泰勒公式</p>
<p>f(x+h)=f(x)+hf′(x)+h22f″(x)+h33!f‴(x)+O(h3)</p>
<p>f(x−h)=f(x)−hf′(x)+h22f″(x)−h33!f‴(x)+O(h3)</p>
<p>二者相减得</p>
<p>f(x+h)−f(x−h)=2hf′(x)+h33!f‴(x)+O(h3)</p>
<p>f(x+h)−f(x−h)2h=f′(x)+h23f‴(x)+O(h2)</p>
<p>​	=f′(x)+O(h2)</p>
<hr />
<p>中心差分的精度通常为 O (h2)：步长 h 减小为原来的一半时，误差将减小为原来的四分之一</p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240421232953535.png?lastModify=1726287112" alt="image-20240421232953535" /></p>
<p>双边误差：g (θ)≈f (θ+ε)−f (θ−ε) 2ε，精度：O (ε2)</p>
<blockquote>
<p>带入上述：θ=1,ε=0.01</p>
<p>(1.01)3−(0.99)32(0.01)=3.0001≈3</p>
<p>对于精度 O (ε2)：ε=0.01,ε2=0.0001</p>
</blockquote>
<p>对于单边误差：g (θ)≈f (θ+ε)−f (θ)ε，精度：O (ε)</p>
<blockquote>
<p>同理：(1.01) 3−(1) 3 (0.01)=3.0301,error:0.03</p>
</blockquote>
<p>运行一次双边的时间和运行两次单边的时间一样</p>
<blockquote>
<p>原因：在一次双边误差计算中，需要计算两次：f (x+h),f (x−h)</p>
<p>一次单边误差需要计算：f (x+h)</p>
</blockquote>
<p>所以在执行梯度检验时，使用<strong>双边误差</strong></p>
<blockquote>
<p>如何使用双边误差来判断别人给你的函数 g (θ)，是否正确实现了函数𝑓的偏导</p>
</blockquote>
<h2 id="梯度检验gradient-checking"><a class="anchor" href="#梯度检验gradient-checking">#</a> 梯度检验（Gradient checking）</h2>
<p>利用梯度检验来调试或检验 backprop 的实施是否正确。</p>
<ul>
<li>自己求偏导对不对</li>
</ul>
<p>对于参数：W [1],b [1],⋯,W [L],b [l]。进行重塑</p>
<blockquote>
<p>对于<strong>每一个</strong>参数转换成<strong>一个向量</strong></p>
</blockquote>
<ul>
<li>然后进行<strong>拼接</strong>得到一个巨型向量 θ</li>
<li>J(W[1],b[1],⋯,W[L],b[l])=J(θ)</li>
</ul>
<blockquote>
<p><img loading="lazy" data-src="D:%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%5C%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5Cweek1%5C%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%5Cimages%5Cdictionary_to_vector.png" alt="dictionary_to_vector" /></p>
</blockquote>
<p>同理：dW [1],db [1],⋯,dW [L],db [l]→dθ</p>
<p>J(θ)=J(θ1,θ2,⋯,θL)</p>
<blockquote>
<p>θi：W or b</p>
</blockquote>
<p>遍历对每个 θi 求偏导</p>
<p>dθapprox[i]=J(θ1,θ2,⋯,θi+ε,...)−J(θ1,θ2,⋯,θi−ε,...)2ε</p>
<blockquote>
<p>也就是之前的中心差分求解偏导</p>
<p>也就是 dθapprox [i] 逼近 dθ[i]</p>
</blockquote>
<ul>
<li>ε=10−7 （可能）</li>
</ul>
<p>然后与 dθ[i]=∂J∂θi 进行比较检验，是否逼近</p>
<p>即：||dθapprox−dθ||2||dθapprox||2+||dθ||2 （整个 θ 计算）</p>
<blockquote>
<p>两个向量的欧式距离：||dθapprox−dθ||2</p>
<p>||dθapprox||2+||dθ||2：两个向量的欧几里得范数之和</p>
<ul>
<li>分母预防向量太小或者太大（归一化误差）</li>
</ul>
<p>【注】：这是整个 θ</p>
</blockquote>
<ul>
<li>若得到值为 10−7 或更小： <code>ok</code></li>
<li>若得到值为 10−5 左右： <code>需再次检查这向量所有项</code></li>
<li>若得到值为 10−3 或更大： <code>worry</code></li>
</ul>
<h2 id="梯度检验应用的注意事项gradient-checking-implementation-notes"><a class="anchor" href="#梯度检验应用的注意事项gradient-checking-implementation-notes">#</a> 梯度检验应用的注意事项（Gradient Checking  Implementation Notes）</h2>
<p>首先，不要在训练中使用梯度检验，它只用于调试</p>
<blockquote>
<p>计算所有的 dθapprox [i] 是一个非常漫长的计算过程</p>
<p>梯度下降的每一个迭代过程都不执行它，因为它太<strong>慢</strong>了。</p>
</blockquote>
<p>其次，如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出 bug</p>
<p>若 dθapprox [i] 与 dθ[i] 的值相差很大，要做的就是查找不同的 i 值，看看是哪个导致 dθapprox [i] 与 dθ[i] 的值相差这么多。</p>
<ul>
<li>
<p>因为 dθapprox [i] 与每一项都有关，所以要查找每一项 dθ[i]，确定 dW [l] 还是 db [l]</p>
<blockquote>
<p>dθapprox[i]=J(θ1,θ2,⋯,θi+ε,...)−J(θ1,θ2,⋯,θi−ε,...)2ε</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>例如：</p>
<p>【注】：θ 的各项与 w,b 的各项都是一一对应的</p>
<p>dW [l] 的各项非常接近，可能会发现计算 b,db  存在 bug</p>
</blockquote>
<p>再次，在实施梯度检验时，如果使用正则化，请注意正则项。</p>
<p>若 J (θ)=1mΣi=1m⁡L (y^(i),y (i))+λ2mΣl=1L⁡||W [l]||F2</p>
<blockquote>
<p>dθ：J 对 θ 的导数</p>
</blockquote>
<p>然，梯度检验不能与 <strong>dropout</strong> 同时使用（不然梯度检验没有可靠性了）</p>
<blockquote>
<p>因为每次迭代过程中，<strong>dropout</strong> 会随机消除隐藏层单元的不同子集，难以计算 <strong>dropout</strong>  在梯度下降上的代价函数 J。</p>
<ul>
<li>这是因为每次训练迭代时，由于 <strong>dropout</strong> 的随机性，网络结构都会发生变化，因此相同的样本可能会对应不同的神经元被激活或者丢弃的情况，从而导致对应的代价函数 J 不同。</li>
</ul>
</blockquote>
<p>建议关闭 <strong>dropout</strong>，用梯度检验进行双重检查，在没有 <strong>dropout</strong> 的情况下，你的算法至少是正确的，然后打开 <strong>dropout</strong>。</p>
<p>最后，现实中几乎不会出现这种情况。</p>
<p>当 W,b 接近 0 时，可以很好的计算梯度。但是当 W,b 变大时，它会变得越来越不准确（模型在这一部分存在问题），在这个时候开启<strong>梯度检验</strong>。</p>
<ul>
<li>
<p>在随机初始化过程中，运行梯度检验，然后再训练网络，W 和 b 会有一段时间远离 0</p>
<blockquote>
<p>运行梯度检验：检查是否梯度下降的实现正确</p>
<p>W 和 b 会有一段时间远离 0：因为网络通过梯度下降算法对参数进行更新，以最小化代价函数</p>
</blockquote>
</li>
<li>
<p>如果随机初始化值比较小，反复训练网络之后，再重新运行梯度检验。</p>
<blockquote>
<p>因为反复训练网络之后，通过梯度下降算法对参数进行更新，使得 W,b 变大</p>
</blockquote>
</li>
</ul>
<h2 id="mini-batch-梯度下降"><a class="anchor" href="#mini-batch-梯度下降">#</a> Mini-batch 梯度下降</h2>
<p>优化算法让你的神经网络运行得更快。</p>
<blockquote>
<p>设 X:(nx,m),Y:(1,m)</p>
<p>向量化能够让你相对较快地处理所有 m 个样本。如果 m 很大的话，处理速度<strong>仍然缓慢</strong>。</p>
<ul>
<li>每次进行梯度下降的时候，都要处理 m 个样本</li>
</ul>
<p><strong>向量化部分样本</strong></p>
</blockquote>
<p>X⏟(nx,m)=[x(1) x(2) ⋯ x(1000)⏟X{1}:(nx,1000) ⋯ x(4001) ⋯ x(5000)⏟X{5000}:(nx,1000)])</p>
<p>Y⏟(1,m)=[y(1) y(2) ⋯ y(1000)⏟Y{1}:(1,1000) ⋯ y(4001) ⋯ y(5000)⏟Y{5000}:(1,1000)])</p>
<p><strong>mini-batch</strong>：其中 X 中把 x (1)∼x (1000) 取出来为 X {i}，为第一个训练集</p>
<blockquote>
<p>同时处理整个训练集</p>
</blockquote>
<p><strong>Mini-batch</strong> 梯度下降：每次同时处理的单个的 <strong>mini-batch</strong> X {t} 和 Y {t}，而不是同时处理全部的 X 和 Y 训练集。</p>
<p>的梯度 for t=1,⋯,5000:  Forward prop on X<ruby>t}    Z[1]=W[1]X{t}+b[l],A[1]=g[1](Z[1])    ⋯    Z[L]=W[L]A[L−1]+b[L],A[L]=g[L](Z[L])  Compute cost:J{t}=11000Σi=11000⁡L(y<rp>(</rp><rt>(i),y(i))+λ2⋅Σl=1L⁡||W[l]||F2  Back prop:J{t</rt><rp>)</rp></ruby> 的梯度  Update parameters:W [l]:=W [l]−αdW [l],b [l]:=b [l]−αdb [l]</p>
<blockquote>
<p>也称为进行 &quot;一代（1 epoch）&quot; 训练</p>
</blockquote>
<p><strong>batch</strong> 梯度下降法：一次遍历训练集只能让你做一个梯度下降</p>
<p><strong>Mini-batch</strong> 梯度下降法：一次遍历训练集，能让你做 5000 个梯度下降。</p>
<h2 id="理解-mini-batch-梯度下降法"><a class="anchor" href="#理解-mini-batch-梯度下降法">#</a> 理解 mini-batch 梯度下降法</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423170508993.png?lastModify=1726287112" alt="image-20240423170508993" /></p>
<p>使用 <strong>mini-batch</strong> 后，相当于一个样本集合训练之后，下一次用另一个样本集合，所以<strong>误差可能会增大</strong>，有更多的噪声</p>
<blockquote>
<p>X {1} 和 Y {1} 是可能容易计算的 <strong>mini-batch</strong></p>
<p>X {2} 和 Y {2} 是比较难计算的 <strong>mini-batch</strong></p>
</blockquote>
<p>但是多次迭代整体应该下降的。</p>
<blockquote>
<p>【注】：J {t} 就是整体的 J，无论是一代训练中的 J，还是梯度下降迭代的 J</p>
</blockquote>
<p>对于极端情况：</p>
<p>若 <strong>mini-batch</strong> 的大小为 m （t=1）：<strong>mini-batch</strong> 等于整个训练集：(X {1},Y {1})=(X,Y)</p>
<p>若 <strong>mini-batch</strong> 的大小为 1 （t=m）: 也称随机梯度下降法</p>
<ul>
<li>相较于 <strong>batch</strong>，<strong>SGD</strong> 在 1 epoch 中会更新 m 次 W,b 计算 m 次 J，而 batch 只有一次</li>
</ul>
<blockquote>
<p>每个样本都是独立的 <strong>mini-batch</strong></p>
<p>(X{1},Y{1})=(x(1),y(1)))</p>
<p>(X{2},Y{2})=(x(2),y(2)))</p>
<p>⋯</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423172511127.png?lastModify=1726287112" alt="image-20240423172511127" /></p>
<blockquote>
<p>紫色：<strong>batch</strong></p>
<p>蓝色：<strong>mini-batch</strong></p>
<p><strong>batch</strong> 梯度下降法从某处开始，相对噪声低些，幅度也大一些</p>
<p>在随机梯度下降法中：每次迭代， 只对<strong>一个</strong>样本进行梯度下降</p>
<ul>
<li>
<p>大部分时候你向着全局最小值靠近，有时候你会远离最小值</p>
<blockquote>
<p>因为那个样本恰好给你指的方向不对</p>
</blockquote>
</li>
<li>
<p>因此随机梯度下降法是有很多<strong>噪声</strong>的</p>
</li>
<li>
<p>平均来看，它最终会<strong>靠近最小值</strong>，不过有时候也会方向错误</p>
<blockquote>
<p>因为随机梯度下降法永远不会收敛， 而是会一直在最小值附近波动，但它并不会在达到最小值并停留在此。</p>
</blockquote>
</li>
<li>
<p><strong>梯度够大，可以一直跑，总是会收敛（最后的结果比较好）</strong></p>
<blockquote>
<p><strong>若跑不动了，说明在一个地方卡住了（梯度消失）</strong></p>
</blockquote>
</li>
</ul>
<p>在 <strong>1 epoch</strong> 中：J 是  会波动的</p>
<p>而梯度下降的迭代中，也就是下一个 <strong>1 epoch</strong>，J 会朝着最小值靠近</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423174138067.png?lastModify=1726287112" alt="image-20240423174138067" /></p>
<p>若使用 <strong>batch</strong> 梯度下降法，<strong>mini-batch</strong> 的大小为 m，每个迭代需要处理<strong>大量</strong>训练样本，单次迭代耗时太长。</p>
<p>若使用 <strong>随机梯度下降法</strong>，如果只要处理一个样本，那这个方法很好</p>
<blockquote>
<p>通过减小学习率，噪声会被改善或有所减小</p>
</blockquote>
<ul>
<li>
<p>缺点：<strong>失去所有向量化</strong>带给你的加速</p>
<blockquote>
<p>因为一次性只处理了一个训练样本，这样效率过于低下</p>
</blockquote>
</li>
</ul>
<p>选择不大不小的 <strong>mini-batch</strong> 尺寸</p>
<ul>
<li>
<p>大量向量化</p>
<blockquote>
<p>如果 <strong>mini-batch</strong> 大小为 1000 个样本，就可以对 1000 个样本向量化，比一次性处理多个样本快 得多。</p>
</blockquote>
</li>
<li>
<p>不需要等待整个训练集被处理完就可以开始进行后续工作</p>
<blockquote>
<p>在每个小批量处理后，模型就可以进行<strong>参数更新</strong>，并且在整个训练集处理完成之前，模型就已经开始学习了。</p>
<p>每次训练集允许我们采取 5000 个<strong>梯度下降步骤</strong></p>
</blockquote>
</li>
<li>
<p>每一次迭代，J 不会总朝向最小值靠近，但它比<strong>随机梯度下降</strong>要更持续地靠近最小值的方向，</p>
</li>
</ul>
<p>对于 <strong>mini-batch</strong> 迭代次数比 <strong>batch</strong> 更多，为什么训练比 <strong>batch</strong> 快</p>
<blockquote>
<p><strong>mini-batch</strong> 以每次更少的样本来换取<strong>更多的梯度下降</strong></p>
<p>在 1 epoch 中，<strong>mini-batch</strong> 就已经梯度下降了 t 次，实现更频繁的参数更新</p>
<ul>
<li>每次梯度下降都会进行更新训练</li>
</ul>
</blockquote>
<p>但由于在 <strong>1 epoch</strong> 中，每次迭代只使用了部分数据，每个数据集选择不同，导致 J 会波动，但是 <strong>mini-batch</strong> 的大小适中，不会像 <strong>随机梯度下降法</strong> 一样波动太大</p>
<p>J 也<strong>不一定</strong>在很小的范围内收敛或者波动，若出现这个问题（大范围波动），可以慢慢减少学习率</p>
<h2 id="学习率衰减learning-rate-decay"><a class="anchor" href="#学习率衰减learning-rate-decay">#</a> 学习率衰减 (Learning rate decay)</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423180803234.png?lastModify=1726287112" alt="image-20240423180803234" /></p>
<blockquote>
<p>假设使用 <strong>mini-batch</strong> 下降法</p>
</blockquote>
<p>在迭代过程中会有噪音（蓝色线），不会精确地收敛</p>
<blockquote>
<p>因为 α 为固定值</p>
</blockquote>
<p>若慢慢减少学习率</p>
<ul>
<li>
<p>初期的时候，α 较大，学习相对较快</p>
</li>
<li>
<p>随着 α 变小，更新也会变小，步伐也会变慢变小</p>
<blockquote>
<p>最后你的曲线（绿色线）会在最小值附近的一小块区域里摆动，而不是在训练过程中，大幅度在最小值附近摆动。</p>
</blockquote>
</li>
</ul>
<p>对于 1 epoch = 1 pass through X</p>
<blockquote>
<p>1 epoch = 一次数据集的迭代</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423181733536.png?lastModify=1726287112" alt="image-20240423181733536" /></p>
</blockquote>
<p>α=11+decay−rate ∗ epoch−numα0</p>
<blockquote>
<p><strong>epoch-num</strong>：迭代次数</p>
<p><strong>decay-rate</strong>：衰减率（超参数）</p>
<p>α0：初始学习率</p>
</blockquote>
<p>例如：α0=0.2，<strong>decay-rate</strong> = 1</p>
<blockquote>
<table>
<thead>
<tr>
<th>Epoch</th>
<th>α</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.1</td>
</tr>
<tr>
<td>2</td>
<td>0.067</td>
</tr>
<tr>
<td>3</td>
<td>0.05</td>
</tr>
<tr>
<td>4</td>
<td>0.04</td>
</tr>
</tbody>
</table>
</blockquote>
<p><strong>其他公式</strong>：</p>
<ul>
<li>
<p>指数衰减：α=0.95epoch−num⋅α0</p>
</li>
<li>
<p>α=kepoch−num⋅αo 或者 α=kt⋅αo</p>
</li>
<li>
<p>离散下降（discrete stair  cease）：某个步骤有某个学习率，一会之后， 学习率减少了一半，一会儿减少一半，一会儿又一半</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423182731709.png?lastModify=1726287112" alt="image-20240423182731709" /></p>
</blockquote>
</li>
</ul>
<p><strong>手动衰减</strong>：学习速率变慢了，把 α 调小一点。</p>
<h2 id="指数加权平均数exponentially-weighted-averages"><a class="anchor" href="#指数加权平均数exponentially-weighted-averages">#</a> 指数加权平均数（Exponentially weighted averages）</h2>
<p><strong>指数加权平均</strong>，在统计中也叫做<strong>指数加权移动平均</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423184538476.png?lastModify=1726287112" alt="image-20240423184538476" /></p>
<blockquote>
<p>V0=0</p>
<p>V1=0.9V0+0.1θ1</p>
<p>V2=0.9V1+0.1θ2</p>
<p>⋯</p>
<p>Vt=0.9Vt−1+0.1θt</p>
<p>如红线所示：得到了移动平均值，每日温度的<strong>指数加权平均值</strong>。</p>
</blockquote>
<p>Vt=βVt−1+(1−β)θt</p>
<p>Vt≈11−β 天的温度平均值</p>
<blockquote>
<p>例如：β=0.9,11−0.9=10，也就是 10 天的平均值</p>
<p>β=0.98,11−0.98=50，也就是 50 天的平均值</p>
</blockquote>
<p>β=0.98 时的图（绿线）：</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423185528309.png?lastModify=1726287112" alt="image-20240423185528309" /></p>
<p>高值 β 得到的曲线要平坦一些，原因在于你多平均了几天的温度， 所以这个曲线，波动更小，更加平坦</p>
</blockquote>
<ul>
<li>
<p>但是加权平均值相对于原始数据会向过去的方向偏移，即在时间轴上向右移动。</p>
<blockquote>
<p>因为过去的数据 Vt−1 占比很大，导致当前数据的<strong>增量</strong> (1−β)θt 就会变得很小，所以上升的很缓慢</p>
</blockquote>
</li>
</ul>
<p>若 β=0.5，平均两天的温度，如黄色所示</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423210305104.png?lastModify=1726287112" alt="image-20240423210305104" /></p>
<p>由于仅平均了两天的温度，<strong>平均的数据太少</strong>，所以得到的曲线有更多的<strong>噪声</strong>，有可能出现异常值，但是这个曲线能够更快适应温度变化。</p>
</blockquote>
<h2 id="理解指数加权平均数understanding-exponentially-weighted-averages"><a class="anchor" href="#理解指数加权平均数understanding-exponentially-weighted-averages">#</a> 理解指数加权平均数（Understanding exponentially  weighted averages）</h2>
<p>vt=βvt−1+(1−β)θt</p>
<p>v100=0.9v99+0.1θ100</p>
<p>v99=0.9v98+0.1θ99</p>
<p>v98=0.9v97+0.1θ98</p>
<p>⋯</p>
<p>理解 v100 是什么</p>
<blockquote>
<p>v100=0.1θ100+0.1×0.9θ99+0.1×(0.9)2θ98+0.1×(0.9)3θ97+⋯</p>
</blockquote>
<p>对于每个 θt 数据，如下图</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423212815005.png?lastModify=1726287112" alt="image-20240423212815005" /></p>
</blockquote>
<p>对于 0.1×(0.9) i （指数衰减函数）：</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423212851918.png?lastModify=1726287112" alt="image-20240423212851918" /></p>
</blockquote>
<p>两个函数对应数据相乘，然后求和</p>
<p>不过<strong>所有</strong>的这些系数 0.1×(0.9) i，相加起来为 1 或者逼近 1，我们称之为<strong>偏差修正</strong></p>
<p>对于 vt≈11−β 天的温度平均值</p>
<p>考虑 (0.9) 10≈0.35≈1e</p>
<blockquote>
<p>在上述，ε=0.1</p>
<p>(0.9)10=(1−ε)1ε=1e</p>
</blockquote>
<ul>
<li>只关注了过去 10 天的温度。10 天后，权重下降到不到当日权重的 13</li>
<li>10 天后的天数权值太小，忽略不计</li>
</ul>
<p>考虑 β=0.98，(1−ε) 1ε=1e</p>
<blockquote>
<p>那么 0.9850≈1e</p>
</blockquote>
<ul>
<li>所以前 50 天 0.9850&gt;1e，数值会快速衰减</li>
<li>本质上这是一个下降幅度很大的函数</li>
</ul>
<blockquote>
<p>带入 (1−ε) 1ε=1e</p>
<p>ε=0.02</p>
</blockquote>
<ul>
<li>所以 1ε=50</li>
</ul>
<p>由此：平均了大约 11−β 天的温度（<strong>指数</strong>）</p>
<ul>
<li>ε=1−β</li>
</ul>
<p>【注】：默认小于 1e 就是权值过小</p>
<p>vθ:=βvθ+(1−β)θt</p>
<blockquote>
<p>vθ：用来计算数据的指数加权平均数</p>
</blockquote>
<p>好处在于：它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了</p>
<blockquote>
<p>也就是滚动变量</p>
<hr />
<p>如果你要计算移动窗，你直接算出过去 10 天的总和，过去 50 天的总和，除以 10 和 50 就好，如此往往会得到更好的估测。</p>
<ul>
<li>缺点是， 如果保存所有最近的温度数据，和过去 10 天的总和，必须占用更多的<strong>内存</strong>，执行更加复杂， 计算成本也更加高昂。</li>
</ul>
</blockquote>
<h2 id="指数加权平均的偏差修正bias-correction-in-exponentially-weighted-averages"><a class="anchor" href="#指数加权平均的偏差修正bias-correction-in-exponentially-weighted-averages">#</a> 指数加权平均的偏差修正（Bias correction in  exponentially weighted averages）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240423222427903.png?lastModify=1726287112" alt="image-20240423222427903" /></p>
<blockquote>
<p>β=0.98</p>
</blockquote>
<p>在 β=0.98 的时候，得到的不是绿色的曲线，而是<strong>紫色</strong>的曲线。</p>
<p>可以发现紫色曲线的起点较低，那如何处理呢？</p>
<p>vt=βvt−1+(1−β)θt</p>
<p>v0=0</p>
<p>v1=0.98v0+0.02θ1=0.02θ1</p>
<p>v2=0.98v1+0.02θ2=0.0196θ1+0.02θ2</p>
<blockquote>
<p>计算后的 v2 远远小于 θ1,θ2，</p>
</blockquote>
<ul>
<li>所以 v2 不能很好的估测出前两天的温度</li>
</ul>
<p>在估测初期的时候，不用 vt，而是用 vt1−βt，t 就是当前天数。</p>
<blockquote>
<p>t=2:1−βt=1−(0.098)2=0.0396</p>
<p>对第二天温度的估测：v20.0396=0.0196 θ1+0.02 θ20.0396</p>
</blockquote>
<p>也就是 θ1 和 θ2 的加权平均数，并去除了偏差。</p>
<p>但是随着 t 增大，βt→0，vt1−βt→vt，导致偏差修正几乎没有作用。</p>
<p>因此当 t 较大的时候，紫线基本和绿线重合了。</p>
<p>不过在开始学习阶段， 才开始预测热身练习，偏差修正可以帮助更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。</p>
<p>若关心<strong>初始时期</strong>的偏差，在刚开始计算指数加权移动平均数的时候，<strong>偏差修正</strong>能帮助在<strong>早期</strong>获取更好的估测。</p>
<h2 id="动量梯度下降法gradient-descent-with-momentum"><a class="anchor" href="#动量梯度下降法gradient-descent-with-momentum">#</a> 动量梯度下降法（Gradient descent with Momentum）</h2>
<p>基本思想：计算梯度的指数加权平均数，并利用该梯度更新你的权重，</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240424123022061.png?lastModify=1726287112" alt="image-20240424123022061" /></p>
<p>若使用 <strong>mini-batch</strong> 或者 <strong>batch</strong>，如蓝色所示，摆动幅度比较大，慢慢摆动到最小值，这种上下波动减慢了梯度下降法的速度，无法使用<strong>较大</strong>的学习率</p>
<blockquote>
<p>假设使用较大的学习率（紫色），结果可能会<strong>偏离</strong>函数的范围（W 的变化范围就更大），为了避免摆动过大，你要用一个<strong>较小</strong>的学习率。</p>
</blockquote>
<p>在纵轴上：希望学习慢一点，因为不想要这些摆动</p>
<p>在横轴上：希望加快学习，希望快速从左向右移，移向最小值，移向红点</p>
<p>所以使用动量下降法</p>
<p>vdW=βvdW+(1−β)dW</p>
<p>vdb=βvdb+(1−β)db</p>
<p>W:=W−αvdW,b:=b−αvdb</p>
<blockquote>
<p>也就是在纵轴上减缓幅度</p>
</blockquote>
<p>纵轴：摆动平均值接近于零，所以在纵轴方向，希望放慢一点</p>
<blockquote>
<p>平均过程中，正负数相互抵消（平均更多的值），所以平均值接近于零。</p>
</blockquote>
<p>横轴：所有的微分都指向横轴方向</p>
<blockquote>
<p>因此横轴方向的平均值仍然较大，因此用算法几次迭代后，发现动量梯度下降法，最终纵轴方向的摆动变小了，横轴方向运动<strong>更快</strong></p>
</blockquote>
<p>vdb=βvdb+(1−β)db</p>
<blockquote>
<p>β：摩擦力（动量的衰减率）</p>
<p>vdb：速率</p>
<p>db：加速度（当前迭代点的梯度方向，告诉应该朝哪个方向移动参数以减少损失。）</p>
<p>你拿一个球，db 给了这个球一个加速度，此时球正向山下滚， 球因为加速度越滚越快，而因为 β 稍小于 1，表现出一些摩擦力，所以球不会无限加速下去</p>
<ul>
<li>βvdb：之前速度的衰减部分。如果之前的速度很大，那么这个项会减小当前的速度，类似于摩擦力的作用。</li>
</ul>
</blockquote>
<p>两个超参数：α,β</p>
<blockquote>
<p>β 控制着指数加权平均，常用 0.9</p>
<ul>
<li>平均了过去十天的温度，所以现在平均了前十次迭代的梯度</li>
</ul>
</blockquote>
<p>对于偏差修正：一般不会使用，因为 10 次迭代之后，移动平均已经过了初始阶段</p>
<p>vdb,vbw 的初始值为 0</p>
<p>还有个版本：vdb=βvdb+(1−β) db→vdb=βvdb+db</p>
<p>也就是 vdb 增大了 11−β 倍，只会影响到学习率 α 的最佳值。</p>
<blockquote>
<p>vdb 增大了，会对当前更新影响就会更大，变化幅度就可能更大，需要降低 α</p>
<ul>
<li>前者：<strong>历史梯度</strong>对当前更新的影响程度受到了控制</li>
</ul>
<blockquote>
<p>因为历史梯度的权重较小</p>
</blockquote>
<ul>
<li>后者：历史梯度的影响更大</li>
</ul>
<blockquote>
<p>因为没有权值的限制</p>
</blockquote>
</blockquote>
<h2 id="rmsprop"><a class="anchor" href="#rmsprop">#</a> RMSprop</h2>
<p>全称是 root mean square prop 算法</p>
<blockquote>
<p>快的变慢，慢的变快</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240424134414169.png?lastModify=1726287112" alt="image-20240424134414169" /></p>
<p>假设纵轴为 b，横轴为 W</p>
<blockquote>
<p>减缓 b 方向的学习</p>
<p>增加 W 方向的学习</p>
</blockquote>
<p>在每一个 <strong>mini-batch</strong> 的迭代中</p>
<p>SdW=βSdW+(1−β)(dW)2</p>
<p>Sdb=βSdb+(1−β)(db)2</p>
<p>W:=W−αdWSdW,b:=b−αdbSdb</p>
<blockquote>
<p>dW↓,SdW↓,W↑</p>
<ul>
<li>增加 W 的学习速度</li>
</ul>
<p>db↑,Sdb↑,b↓</p>
<ul>
<li>减少 b 的学习速度</li>
</ul>
</blockquote>
<p>因为垂直方向要比水平方向的大很多</p>
<blockquote>
<p>所以 db 较大，dW 较小</p>
</blockquote>
<p>结果就是纵轴上的更新要被一个较大的数相除，就能消除摆动。而水平方向的更新则被较小的数 相除。</p>
<p>把纵轴和横轴方向分别称为 b 和 W，只是为了方便展示而已。</p>
<p>实际中，会处于参数的<strong>高维度</strong>空间，所以需要消除摆动的<strong>垂直维度</strong>，你需要消除摆动</p>
<ul>
<li>实际上是参数 W1,W2 等的合集</li>
<li><strong>水平维度</strong>可能 W3,W4 等等</li>
</ul>
<p>因此把 W 和 b 分开只是方便说明。</p>
<p>实际中 dW 是一个高维度的参数向量，db 也是一个高维度参数向量</p>
<p>如何确保不除 0，也就是 SdW 的平方根趋于 0，得到的答案就非常大，为了确保数值稳定</p>
<p>dWSdW→dWSdW+ε</p>
<blockquote>
<p>ε 通常为 10−8</p>
</blockquote>
<h2 id="adam-优化算法adam-optimization-algorithm"><a class="anchor" href="#adam-优化算法adam-optimization-algorithm">#</a> Adam 优化算法 (Adam optimization algorithm)</h2>
<p><strong>Adam</strong> 优化算法基本上就是将 <strong>Momentum</strong> 和 <strong>RMSprop</strong> 结合在一起</p>
<p>vdW=0,SdW=0;vdb=0,Sdb=0on iteration t:  vdW=β1vdW+(1−β1)dW,vdb=β1vdb+(1−β1)db←(Momentum)  SdW=β2SdW+(1−β2)(dW)2,Sdb=β2Sdb+(1−β2)(db)2←(RMSprop)  vdWcorrected=vdW1−β1t,vdbcorrected=vdb1−β1t  SdWcorrected=SdW1−β1t,Sdbcorrected=Sdb1−β1t  W:=W−αvdWcorrectedSdWcorrected+ε,b:=b−αvdbcorrectedSdbcorrected+ε</p>
<p>对于超参数：</p>
<p>α:needs to be tunedβ1:0.9→dwβ2:0.999→(dw)2ε:10−8</p>
<p>对于 β1,β2 默认值，一般不会改动。</p>
<ul>
<li>第一矩：β1 计算 dW</li>
<li>第二矩：β2 计算 (dW) 2</li>
</ul>
<h2 id="局部最优的问题the-problem-of-local-optima"><a class="anchor" href="#局部最优的问题the-problem-of-local-optima">#</a> 局部最优的问题 (The problem of local optima)</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240424152244198.png?lastModify=1726287112" alt="image-20240424152244198" /></p>
<blockquote>
<p>对于左图，很容易出现多个不同局部最优的图</p>
</blockquote>
<p>事实上，如果你要创建一个神经网络，通 常梯度为零的点并不是左图中的局部最优点，实际上成本函数的零梯度点，通常是<strong>鞍点</strong>（单个方向），右图。</p>
<p>但是一个具有高维度空间的函数，如果梯度为 0，那么在每个方向，它可能是<strong>凸函数</strong>， 也可能是<strong>凹函数</strong>。</p>
<blockquote>
<p>如果你在 2 万维空间中，那么想要得到局部最优，所有的 2 万个方向都需要是这样，但发生的机率也许很小：2−20000</p>
</blockquote>
<p>因此在高维度空间，更可能碰到鞍点，而不会碰到局部最优。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240424153034273.png?lastModify=1726287112" alt="image-20240424153034273" /></p>
<p>平稳段会减缓学习，平稳段是一块区域，其中导数长时间接近于 0，</p>
<blockquote>
<p>因为梯度等于或接近 0</p>
</blockquote>
<p>曲面很平坦，得花上很长时间慢慢抵达平稳段的这个<strong>点</strong></p>
<p>如果在此处，梯度会从曲面从从上向下下降</p>
<blockquote>
<p>因为左边或右边的随机扰动，而帮助模型摆脱局部最优解或者平稳段。</p>
<p>使得参数在<strong>多个方向</strong>上移动，而不是仅仅局限于一个方向。</p>
</blockquote>
<h2 id="调试处理tuning-process"><a class="anchor" href="#调试处理tuning-process">#</a> 调试处理（Tuning process）</h2>
<p>最重要的超参数：α</p>
<p>其次：β，#hidden units，#mini-batch size</p>
<p>最后：#layers，learning rate decay</p>
<blockquote>
<p>默认一般：β1=0.9,β2=0.999,ε=10−8</p>
</blockquote>
<p><strong>Grid Search</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425130419302.png?lastModify=1726287112" alt="image-20240425130419302" /></p>
<blockquote>
<p>不推荐网格上取点，而是采用随机选择点，用这些随机取的点试验超参数的效果</p>
<ul>
<li>很难提前知道哪个超参数最重要</li>
</ul>
</blockquote>
<p>对于上述左图表格：α 的取值很重要，而 ε 的取整无关紧要</p>
<blockquote>
<p>可知：无论 ε 取何值，α 只有 5 个取值，结果基本上都是一样的。</p>
<p>所以，共有 25 种模型，但进行试验的 α 值只有 5 个</p>
</blockquote>
<p>对于上述右图随机取点：α 是独立的 25 个</p>
<p>实践中，搜索的超参数可能不止两个，会试验大量的更多的值</p>
<p>当给超参数取值时，另一个惯例是采用由<strong>粗糙到精细</strong>的策略。</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425131339117.png?lastModify=1726287112" alt="image-20240425131339117" /></p>
<p>比如发现效果最好的某个点，也许这个点<strong>周围</strong>的其他一些点效果也很好</p>
<p>然后就是放大这块小区域（蓝色矩形）</p>
<p>然后在其中更密集得取值或随机取值，聚集更多的资源</p>
</blockquote>
<p>也就是：在整个的方格中进行<strong>粗略</strong>搜索后，会知道接下来应该聚焦到<strong>更小</strong>的方格中。在更小的方格中，你可以更密集得取点。</p>
<h2 id="为超参数选择合适的范围using-an-appropriate-scale-to-pick-hyperparameters"><a class="anchor" href="#为超参数选择合适的范围using-an-appropriate-scale-to-pick-hyperparameters">#</a> 为超参数选择合适的范围（Using an appropriate scale to  pick hyperparameters）</h2>
<p>对于隐藏单元 n [l] 的取值范围</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425150745067.png?lastModify=1726287112" alt="image-20240425150745067" /></p>
<p>随机在其取点（均匀分布）</p>
</blockquote>
<p>或层数 L 的取值范围</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425150805810.png?lastModify=1726287112" alt="image-20240425150805810" /></p>
</blockquote>
<p>上述随机均匀取值的例子，都是合理的。</p>
<p>而对于某些超参数则不行。</p>
<p>例如 α</p>
<p>假设 α 的最小值为 0.0001，最大值为 1</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425151000092.png?lastModify=1726287112" alt="image-20240425151000092" /></p>
<p>那么在数轴上：有 90% 的概率落在 0.1∼1 之间，应用了 90% 的资源</p>
<p>而 10% 的概率落在 0.0001∼0.1 之间，应用了 10% 的资源</p>
</blockquote>
<p>采用用<strong>对数标尺</strong>搜索超参数的方式会更合理</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425151255319.png?lastModify=1726287112" alt="image-20240425151255319" /></p>
<p>在对数坐标轴上，分别取 0.0001,0.001,0.01,0.1,1</p>
<ul>
<li>在对数轴上均匀随机取点</li>
</ul>
<p>这个每个段都是均匀的</p>
</blockquote>
<ul>
<li>
<p>r=−4∗np.random.rand ()：为 [−4,0] 的均匀分布</p>
</li>
<li>
<p>α=10r</p>
<blockquote>
<p>α∈[10−4,100]</p>
<p>所以左边界为 10−4，右边界为 100</p>
</blockquote>
</li>
</ul>
<p>10a=0.0001→a=log100.0001=−4</p>
<p>对于取值范围：[a,b]</p>
<blockquote>
<p>只需要在边界值取对数就可以得到 a,b 的值</p>
</blockquote>
<p>在 a,b 之间随意均匀的选取 r 值， 将超参数设置为 10r</p>
<ul>
<li>也就是：例如在 [−4,0] 的均匀分布中随机取 r，这样就变成了等概率</li>
</ul>
<p>而对于指数加权值中的 β</p>
<blockquote>
<p>对于 beta 的取值一般是 0.9∼0.999 的某一个值</p>
</blockquote>
<p>然而计算指数的加权平均值时</p>
<ul>
<li>0.9 就像在 10 个值中计算平均值</li>
<li>0.999 就像在 1000 个值中计算平均值</li>
</ul>
<p>所以在取值的时候平均值不能再用线性取值</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425154254779.png?lastModify=1726287112" alt="image-20240425154254779" /></p>
<p>在 0.9∼0.99 取的值的数量小于 0.99∼0.999 取值的数量</p>
</blockquote>
<p>因此要在这些区间探究的数量要<strong>一样</strong>多：采用对数取值</p>
<p>β=0.09,⋯,0.999</p>
<p>1−β=0.1,⋯,0.001</p>
<ul>
<li>r∈[−3,−1]</li>
<li>1−β=10r</li>
<li>β=1−10r</li>
</ul>
<p>只需要在 [−3,−1] 中随机均匀的给 r 取值。</p>
<blockquote>
<p>β:0.9000→0.9005∼10</p>
<p>β:0.999⏟∼1000→0.9995⏟∼2000∼1000</p>
<ul>
<li>因为：11−β</li>
</ul>
<p>需要采取的概率是一样的，取值的数量要保持一致</p>
</blockquote>
<p><strong>给定一个范围，超参数在这个范围的不同区间对算法的效果不尽相同</strong></p>
<p><strong>原则：给不同的效果找尽可能一样多的样本点</strong></p>
<h2 id="超参数调试实践pandas-vs-caviarhyperparameters-tuning-in-practice-pandas-vs-caviar"><a class="anchor" href="#超参数调试实践pandas-vs-caviarhyperparameters-tuning-in-practice-pandas-vs-caviar">#</a> 超参数调试实践：Pandas VS Caviar（Hyperparameters  tuning in practice: Pandas vs. Caviar）</h2>
<p>一些变化，原来的超参数的设定不再好用，所以建议，或许只是重新测试或评估你的超参数，至少每隔几个月一次</p>
<blockquote>
<p>数据变化等等</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425161456767.png?lastModify=1726287112" alt="image-20240425161456767" /></p>
<p>左图：通常是有庞大的数据组，但<strong>没有</strong>许多计算资源或足够的 CPU 和 GPU 的前提</p>
<ul>
<li>边学习边调参，在学习中不断地调整参数，若发现学习率太大了，可能又回归之前的模型</li>
<li>也许过了一阵子，应该简历一个不同的模型（绿色）</li>
</ul>
<p>右图：同时实验多种模型，选择最优的</p>
<h2 id="归一化网络的激活函数normalizing-activations-in-a-network"><a class="anchor" href="#归一化网络的激活函数normalizing-activations-in-a-network">#</a> 归一化网络的激活函数（Normalizing activations in a  network）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425163027196.png?lastModify=1726287112" alt="image-20240425163027196" /></p>
<p>在 logistic 回归中的归一化输入</p>
<blockquote>
<p>μ=1mΣi=1m⁡x(i)</p>
<p>x:=x−μ</p>
<p>σ2=1mΣi=1m⁡x(i)2</p>
<p>x/=σ</p>
</blockquote>
<p>对于更深的模型</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425163303634.png?lastModify=1726287112" alt="image-20240425163303634" /></p>
<blockquote>
<p>不仅输入了 x，而且还有激活值 a [l] 等</p>
</blockquote>
<p>若想加速训练参数如 W [3],b3 等，那么归一化 a [2] 的均值和方差</p>
<p>对于一个隐藏层来讲，经常是归一化 z [2]，再用激活函数</p>
<blockquote>
<p>激活函数的输出会受到限制（例如 <strong>sigmoid</strong> 或者 <strong>tanh</strong> 的输出范围是 0 到 1 或者 -1 到 1），如果在激活函数之后进行归一化，可能会破坏这种限制。</p>
</blockquote>
<p><strong>Batch</strong> 归一化</p>
<p>假设第 l 层一些隐藏单元值：z<a href="i">l</a></p>
<p>μ=1mΣi=1m⁡z(i)σ2=1mΣi=1m⁡z(i)2znorm(i)=z(i)−μσ2+εzi~=γznorm(i)+β</p>
<blockquote>
<p>也就是转化为标准正态分布处理，ε：防止除 0</p>
<p>γ,β：两个超参数，可以设置为<strong>任何值</strong></p>
<p>zi~：隐藏单元的分布不一定是标准正态分布，也许隐藏单元有了不同的分布会有意义</p>
</blockquote>
<p>所以用 zi~ 取代 z (i)，方便神经网络的后续计算</p>
<p>当 γ=σ2+ε,β=μ 的时候，zi~=z (i)</p>
<p>通过对 γ 和 β 合理设定，规范化过程，即这四个等式，从根本来说，只是计算恒等函数， 通过赋予 γ 和 β 其它值，可以使你构造含其它平均值和方差的隐藏单元值。</p>
<p>对于不想隐藏单元值必须是平均值 0 和方差 1。</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425170138481.png?lastModify=1726287112" alt="image-20240425170138481" /></p>
<p>对于 <strong>sigmoid</strong> 激活函数</p>
<p>若采用标准正态分布，则值总是集中在上述加粗的<strong>线性</strong>版本中。</p>
<p>而是想有更大的方差或均值不为 0，使得更好的利用<strong>非线性</strong>的 <strong>sigmoid</strong> 函数</p>
</blockquote>
<h2 id="将-batch-norm-拟合进神经网络fitting-batch-norm-into-a-neural-network"><a class="anchor" href="#将-batch-norm-拟合进神经网络fitting-batch-norm-into-a-neural-network">#</a> 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into  a neural network）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425172825716.png?lastModify=1726287112" alt="image-20240425172825716" /></p>
<p>正向传播的过程：</p>
<p>X→⏞W[1],b[1]Z[1]→⏟BatchNorm(BN)⏞β[1],γ[1]Z[1]~→a[1]=g<a href="Z~%5B1%5D">1</a>⋯</p>
<p><strong>Parameters：</strong></p>
<ul>
<li>
<p>W[1],b[1],⋯,W[L],b[L]</p>
</li>
<li>
<p>β[1],γ[1],⋯,β[L],γ[L]</p>
<blockquote>
<p>这里的 β 与 <strong>Momentum</strong> 等不同，这里是参数</p>
</blockquote>
</li>
</ul>
<p>实践中，<strong>Batch</strong> 归一化通常和训练集的 <strong>mini-batch</strong> 一起使用。</p>
<p>对于每一个 <strong>mini-batch_i</strong></p>
<p>X{i}→⏞W[1],b[1]z[1]→⏟BatchNorm(BN)⏞β[1],γ[1]z[1]~→a[1]=g<a href="z~%5B1%5D">1</a>⋯</p>
<p><strong>Parameters：</strong></p>
<ul>
<li>W[1],b[1],⋯,W[L],b[L]→W[1],⋯,W[L]</li>
<li>β[1],γ[1],⋯,β[L],γ[L]</li>
</ul>
<p>z[l]=W[l]al−1+b[l]</p>
<ul>
<li>消除了 b [l]</li>
</ul>
<p>经过标准化后：z [l]~=γ[l] znorm [l]+β[l]</p>
<p>对于为什么要消除 b [l]</p>
<blockquote>
<p>因为标准化后，激活函数采用 z [l]~，而不是 z [l]</p>
<p>z [l]~ 中含有常数项 β[l] 包含了 b [l]</p>
<ul>
<li>也就是合并</li>
</ul>
</blockquote>
<p>z[l]:(n[l],1)</p>
<ul>
<li>那么 γ[l],β[l]:(n [l],1)</li>
</ul>
<blockquote>
<p>有 n [l] 个隐藏单元</p>
<p>γ[l],β[l] 用来将<strong>每个</strong>隐藏层的均值和方差缩放为网络想要的值</p>
</blockquote>
<p>用 <strong>Batch</strong> 归一化来应用梯度下降法</p>
<p>for t=1⋯MiniBatchesSize  Compute forward prop on X{t}    In each hidden layer,use BN to replace Z[l] with Z[l]~  Use backprop to compute dW[l],db[l],dβ[l],dγ[l]  Update parameters (Work with momentum/RMSprop/Adam)</p>
<h2 id="batch-norm-为什么奏效why-does-batch-norm-work"><a class="anchor" href="#batch-norm-为什么奏效why-does-batch-norm-work">#</a> Batch Norm 为什么奏效？（Why does Batch Norm work?）</h2>
<p>一个原因：有一些从 0 到 1 而不是从 1 到 1000 的特征值，通过归一化所有的输入特征值 x，以获得类似范围的值，可以加速学习。</p>
<p><strong>Batch</strong> 归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有<strong>隐藏单元</strong>的值</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425183839729.png?lastModify=1726287112" alt="image-20240425183839729" /></p>
<blockquote>
<p>对于一个浅层网络（左）或深层网络（右）的训练建立在的猫脸识别检测上</p>
<p>红色圆圈：Cat</p>
</blockquote>
<p>在所有黑猫的图像上训练了数据集，如果现在你要把此网络应用于有色猫， 可能适用的不会很好。</p>
<p>无法期待，在左边训练得很好的模块，<strong>同样</strong>在右边也运行得很好（即使是同一个函数）</p>
<p>也不希望学习算法去发现绿色的决策边界，如果<strong>只看</strong>左边数据的话。</p>
<p>改变数据分布，&quot;<strong>Covariate shift</strong>&quot;：如果已经学习了 x 到 y 的映射，如果 x 的分布改变了，那么可能需要重新训练学习算法。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240425200922616.png?lastModify=1726287112" alt="image-20240425200922616" /></p>
<p>从第三隐藏层的角度来看，他接收之前的 a1 [2],a2 [2],a3 [2],a4 [2]，然后计算使得输出值尽 y^ 可能接近 y</p>
<blockquote>
<p>第三层隐藏层的工作是找到一种方式，使这些值映射到 y^</p>
</blockquote>
<p>由于 W [1],b [1],W [2],b [2] 的影响，a [2] 的值会改变，从第三层隐藏层的角度来看，这些隐藏单元的值在不断地改变。</p>
<p><strong>Batch</strong> 归一化做的，减少了这些隐藏值分布变化的数量。</p>
<blockquote>
<p>重整 z，将 z 进行归一化，使得<strong>均值和方差</strong>保持不变</p>
</blockquote>
<p>无论怎么变化 z [2] 的值，由于进行了归一化，那么<strong>均值和方差</strong>也会是均值 0，方差 1，或 不一定必须是均值 0，方差 1，而是由 β[2],γ[2] 决定的值。</p>
<blockquote>
<p>z[l]~=γ[l]znorm[l]+β[l]</p>
</blockquote>
<ul>
<li>
<p>由均值和方差所限制</p>
</li>
<li>
<p>保持在特定的区域内，避免前面的参数变化导致激活变化过大，进而导致后面层次不好学习的情况</p>
<blockquote>
<p>例如：在没有归一化 z 的时候，z 变化很大，导致激活函数变化很大（例如指数级递增或者递增），后面的层次的相应值变化也会更大，导致后面每一层容易发生梯度爆炸或者消失。</p>
</blockquote>
</li>
<li>
<p><strong>Batch</strong> 归一化减少了输入值改变的问题，它的确使这些值变得<strong>更稳定</strong>，神经网络的之 后层就会有更坚实的基础。</p>
</li>
<li>
<p>它减弱了前层参数的作用与后层参数的作用之间的联系</p>
</li>
</ul>
<p><strong>Batch</strong> 归一化还有一个作用，它有轻微的<strong>正则化</strong>效果</p>
<p><strong>Batch</strong> 中的均值和方差可能有一些噪音（<strong>波动</strong>）</p>
<ul>
<li><strong>mini-batch 大小的影响：</strong> <strong>mini-batch</strong> 的样本数量很较小时，均值和方差可能会受到更大的波动</li>
<li>** 统计估计的不准确性：** 在 <strong>mini-batch</strong> 上计算的均值和方差，而不是在整个数据集上，可能存在一定的误差，导致均值和方差有一些噪音</li>
</ul>
<p>在缩放过程从 z [l] 到 z [l]~，过程也会有一些噪音</p>
<blockquote>
<p>因为使用有些噪音的均值和方差计算得出的</p>
</blockquote>
<p>所以和 <strong>droput</strong> 类似，它往每个隐藏层的激活值上<strong>增加了噪音</strong></p>
<blockquote>
<p><strong>dropout</strong> 有增加噪音的<em>方式</em>，它使一个隐藏的单元，以一定的概率乘以 0，以一定的概率乘以 1</p>
</blockquote>
<p><strong>Batch</strong> 归一化有轻微的正则化效果，因为给隐藏单元<strong>添加了噪音</strong>，这迫使后部单元不过分依赖任何一个隐藏单元</p>
<ul>
<li>
<p>** 均值和方差的估计误差引入的波动：** 会导致每个 <strong>mini-batch</strong> 的 znorm 有一定的误差（变化）</p>
<blockquote>
<p>波动可以视为一种噪音。</p>
<p>当前层不会过分依赖某些特征（防止过拟合），因为每个 <strong>mini-batch</strong> 的样本不同。</p>
</blockquote>
</li>
<li>
<p><strong>对隐藏单元分布的影响：</strong> z [l] 到 z [l]~ 归一化的过程，导致变化在一定的范围之内</p>
<blockquote>
<p>避免了隐藏单元对特定激活值的过度依赖（防止过拟合）</p>
</blockquote>
</li>
</ul>
<p>若采用较大的 <strong>mini-batch</strong>，减少了噪音，也会减少了正则化效果</p>
<blockquote>
<p>也就是样本的数量高，特征相似度高，噪音也就会减少，可能会过拟合数据</p>
</blockquote>
<p>即：模型在训练集上表现得很好（<strong>过度依赖于相似特征</strong>，上述 black cats），但在测试集上表现不佳（例如未见过的数据，colored cats）。方差会增大</p>
<p><strong>Batch</strong> 归一化一次只能处理一个 <strong>mini-batch</strong> 数据，它在 <strong>mini-batch</strong> 上计算均值和方差。（模拟全部均值和方差）</p>
<blockquote>
<p>基于这个 <strong>mini-batch</strong> 中的样本数量计算均值和方差</p>
</blockquote>
<h2 id="测试时的-batch-normbatch-norm-at-test-time"><a class="anchor" href="#测试时的-batch-normbatch-norm-at-test-time">#</a> 测试时的 Batch Norm（Batch Norm at test time）</h2>
<p>μ=1mΣi=1m⁡z(i)σ2=1mΣi=1m⁡z(i)2znorm(i)=z(i)−μσ2+εzi~=γznorm(i)+β</p>
<p>在<strong>测试</strong>的时候，可能没有 <strong>mini-batch</strong> 大规模的样本，例如 6428 或 2056 个样本</p>
<blockquote>
<p>也就是不直接计算均值和方差，样本数量不够</p>
</blockquote>
<p>因此，需要用指数加权平均来估算 μ,σ2</p>
<p>x{1}→μ{1}[l]:θ1</p>
<p>x{2}→μ{2}[l]:θ2</p>
<p>x{3}→μ{3}[l]:θ3</p>
<p>⋯</p>
<blockquote>
<p>对于计算当前气温的指数加权平均，同样用指数加权平均来估算 μ,σ2</p>
</blockquote>
<p>也就是均值和方差是根据之前的 <strong>mini-batch</strong> 计算得到的均值和方差进行指数平均后得到</p>
<p>理论上可以在最终的网络中运行<strong>整个训练集</strong>来得到 μ 和 σ2，但在实际操作中，我们通常运用指数加权平均来追踪在训练过程中看到的 μ 和 σ2 的值。</p>
<blockquote>
<p>因为测试的时候，每个 <strong>mini-batch</strong> 的样本数量小，计算出来的 μ 和 σ2 都会有误差，所以采用指数加权平均来进行估算</p>
</blockquote>
<h2 id="softmax-回归softmax-regression"><a class="anchor" href="#softmax-回归softmax-regression">#</a> Softmax 回归（Softmax regression）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240426102214625.png?lastModify=1726287112" alt="image-20240426102214625" /></p>
<p>有多种可能的分类类型，有种 logistic 回归的一般形式：<strong>Softmax</strong> 回归。能让你在试图识别某一分类时做出预测，或 者说是多种分类中的一个，不只是识别两个分类</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240426102831619.png?lastModify=1726287112" alt="image-20240426102831619" /></p>
<p>输出层变成了四层：n [L]:4,y^:(4,1)</p>
<p>P(other|x)+P(cat|x)+P(dog|x)+P(bc|x)=1</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240426103524136.png?lastModify=1726287112" alt="image-20240426103524136" /></p>
<p>对于最后一层 L</p>
<p>z[L]=W[L]a[L−1]+bL  (4,1)</p>
<p><strong>Activation function:</strong></p>
<ul>
<li>t=e(z[L])</li>
<li>a [L]=e (z [L])Σj=14⁡tj,ai [L]=tiΣj=14⁡tj 归一化，将其转换为概率分布，使得输出值之和始终为 1。</li>
</ul>
<p>例如：</p>
<blockquote>
<p>z[L]=[52−13],t=[e5e2e−1e3]</p>
<p>a[L]=t=[0.8420.0420.0020.114]</p>
</blockquote>
<p>用 e 是因为要保证每一个概率为正数，且 ex  是单调递增函数，很好的区分每个类别的重要性</p>
<p>如下在没有隐藏层的情况下，所做的就是计算 Z [1]=W [1] x+b [1]</p>
<p>输出 y^=a [l]=g (z [1])，g 为 <strong>Softmax</strong> 函数</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240426105814710.png?lastModify=1726287112" alt="image-20240426105814710" /></p>
<h2 id="训练一个-softmax-分类器training-a-softmax-classifier"><a class="anchor" href="#训练一个-softmax-分类器training-a-softmax-classifier">#</a> 训练一个 Softmax 分类器（Training a Softmax classifier）</h2>
<blockquote>
<p>z[L]=[52−13],t=[e5e2e−1e3]</p>
</blockquote>
<p><strong>hardmax</strong> 会把上述 z [L] 变为：[1000]，最大元素位置为 1，其余为 0</p>
<p><strong>Softmax</strong> 所做的从 z 到这些概率的映射更为温和</p>
<p>当 C=2 时候，<strong>Softmax</strong> 会变回 <strong>logistic</strong> 回归</p>
<blockquote>
<p>对于 L 层</p>
<p>a1=ez1ez1+ez2=11+e−(z1−z2)</p>
<p>a2=ez2ez1+ez2=11+e−(z2−z1)</p>
</blockquote>
<p><strong>Softmax</strong> 回归将 <strong>logistic</strong> 回归推广到了两种分类以上。</p>
<p>假设 y (1)=[0100],a [L]=y^=[0.30.20.10.4]</p>
<p>Loss fuction: L(y<sup>,y)=−Σj=1C⁡yjlogy</sup>j=−logy^2</p>
<blockquote>
<p>要使得损失函数小，那么就要使 y^2  大</p>
</blockquote>
<p>cost function: J(W[1],b[1],⋯)=1mΣi=1m⁡L(y^,y)</p>
<p>对于反向传播</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240426131935666.png?lastModify=1726287112" alt="image-20240426131935666" /></p>
<p>dz[L]=−y(1−y^)</p>
<blockquote>
<p>dz[L]=∂L∂a[L]∂a[L]∂z[L]</p>
<p>∂a[L]∂z[L]=y<sup>(1−y</sup>)</p>
<p>∂L∂a[L]=−yy^</p>
</blockquote>
<h2 id="深度学习框架deep-learning-frameworks"><a class="anchor" href="#深度学习框架deep-learning-frameworks">#</a> 深度学习框架（Deep Learning frameworks）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240426134208990.png?lastModify=1726287112" alt="image-20240426134208990" /></p>
<h2 id="tensorflow"><a class="anchor" href="#tensorflow">#</a> TensorFlow</h2>
<p>对于 J (W)=W2−10W+25 <strong>TensorFlow</strong> 如何将其最小化</p>
<h1 id="结构化机器学习项"><a class="anchor" href="#结构化机器学习项">#</a> 结构化机器学习项</h1>
<h2 id="为什么是ml策略"><a class="anchor" href="#为什么是ml策略">#</a> 为什么是 ML 策略？</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427142647752.png?lastModify=1726287112" alt="image-20240427142647752" /></p>
<h2 id="正交化orthogonalization"><a class="anchor" href="#正交化orthogonalization">#</a> 正交化（Orthogonalization）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427143157723.png?lastModify=1726287112" alt="image-20240427143157723" /></p>
<blockquote>
<p>上图有<strong>很多旋钮</strong>可以用来调整图像的各种性质</p>
</blockquote>
<p>若采用一个旋钮来控制，几乎<strong>不</strong>可能把电视调好，让图像调到正<strong>中间</strong></p>
<blockquote>
<p>0.1x：表示图像高度</p>
<p>0.3x：表示图像宽度</p>
<p>−1.7x：表示提醒角度等等</p>
</blockquote>
<p>所以在这种情况下，<strong>正交化</strong>指的是电视设计师设计这样的旋钮，使得每个旋钮都只调整一个性质，这样调整电视图像就容易得多，就可以把图像调到正中。</p>
<blockquote>
<p>一一对应（低耦合）</p>
</blockquote>
<p>通过正交化（相互独立），使得只控制一个性质，其他性质不会受到影响</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427145611713.png?lastModify=1726287112" alt="image-20240427145611713" /></p>
<blockquote>
<p>希望每一个阶段都有良好的表现</p>
</blockquote>
<ul>
<li>
<p>若算法在 cost function 不能很好的拟合 training set，想要一个旋钮来调整算法而不会影响 dev set 等</p>
<blockquote>
<p>bigger network，Adam 优化算法等</p>
</blockquote>
</li>
<li>
<p>若算法在 cost function 不能很好的拟合 dev set，也应该有独立的旋钮来调整。</p>
<blockquote>
<p>例如在 training set 上很好，但在 dev set 上拟合的不好</p>
<ul>
<li>Regularization</li>
<li>Bigger training set</li>
</ul>
</blockquote>
</li>
<li>
<p>若算法在 cost function 不能很好的拟合 test set，同样需要独立的旋钮来调整</p>
<blockquote>
<p>在 dev set 做的很好，但是在 test set 却不行</p>
<ul>
<li>说明在 dev set 上过拟合，需要使用更大 dev set</li>
</ul>
</blockquote>
</li>
<li>
<p>若在 test set 上做得很好，但无法给你的猫图片应用用户提供良好的体验</p>
<blockquote>
<p>需要回去改变 dev set（例如分布不正确） 或 cost function（例如测量指标不对）</p>
</blockquote>
</li>
</ul>
<h2 id="单一数字评估指标single-number-evaluation-metric"><a class="anchor" href="#单一数字评估指标single-number-evaluation-metric">#</a> 单一数字评估指标（Single number evaluation metric）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427151830023.png?lastModify=1726287112" alt="image-20240427151830023" /></p>
<ul>
<li>
<p><strong>Precision</strong>：这图有 95% 的概率是猫</p>
</li>
<li>
<p><strong>Recall</strong>：对于所有猫图 90% 的概率识别出来了是猫</p>
<blockquote>
<p>有多少猫被认为是猫</p>
</blockquote>
</li>
</ul>
<p>使用上述 2 个指标有个问题，例如 A 在 <strong>Recall</strong> 表现的好，而 B 在 <strong>Precision</strong> 表现的好，无法判断哪个分类器好。很难进行判断</p>
<p>使用新的指标来结合 <strong>Precision</strong> 和 <strong>Recall</strong>，那就是 <strong>F1 Score</strong></p>
<p>F1 Score=21P+1R</p>
<blockquote>
<p>查准率 P 和查全率 R 的调和平均数。但非正式来说，你可以将它看成是某种查准率和查全率的<strong>平均值</strong>（<strong>调和平均</strong>）</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427153423707.png?lastModify=1726287112" alt="image-20240427153423707" /></p>
<p>在上述例子中 A 的 <strong>F1 Score</strong> 高，所以选出 A，淘汰 B</p>
<p><strong>Dev set + Single real number evaluation metric</strong></p>
<ul>
<li>一个开发集，加上<strong>单实数评估指标</strong></li>
</ul>
<blockquote>
<p>可以加速改进机器学习算法的迭代过程</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427153830675.png?lastModify=1726287112" alt="image-20240427153830675" /></p>
<h2 id="满足和优化指标satisficing-and-optimizing-metrics"><a class="anchor" href="#满足和优化指标satisficing-and-optimizing-metrics">#</a> 满足和优化指标（Satisficing and optimizing metrics）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427155311428.png?lastModify=1726287112" alt="image-20240427155311428" /></p>
<blockquote>
<p>上述除了 <strong>Accuracy</strong> 之外还需要考虑 <strong>Running time</strong></p>
</blockquote>
<p>可以将二者组合成一个整体评估指标</p>
<p>例如：Cost=accuracy−0.5×running time</p>
<ul>
<li>
<p>对于 <strong>Accuracy</strong>，目的是最优化（<strong>Optimizing</strong>）</p>
<blockquote>
<p>也就是 <strong>maximize accuracy</strong></p>
</blockquote>
</li>
<li>
<p>对于 <strong>runningTime</strong>，目的是满足（<strong>Satisficing</strong>）一定的条件，设置一个阈值即可</p>
<blockquote>
<p>实际情况可能是，只要运行时间少于 100 毫秒，你的用户就不会在乎运行时间是 100 毫秒还是 50 毫秒，甚至更快。</p>
<p>例如：runningTime≤100ms</p>
</blockquote>
</li>
</ul>
<p>一般来说：若考虑 N 个指标，有时候 1 个作为优化指标，其余 N−1 都是满足指标（只需要达到一定的阈值即可）</p>
<p>对于唤醒语音控制设备，可能会在乎触发字检测系统的准确性</p>
<blockquote>
<p>有人说出其中一个触发词，有多大概率可以唤醒你的设备</p>
</blockquote>
<p>也需要考虑假阳性（<strong>false positive</strong>）的数量</p>
<blockquote>
<p>没有人在说这个触发词时候，它被唤醒的概率有多大</p>
</blockquote>
<p>组合考虑就是<strong>最大化精确度</strong>。即：</p>
<ul>
<li>当某人说出唤醒词，设备被唤醒的概率最大化</li>
<li>以及满足 24 小时内最多只能有 1 次 <strong>false positive</strong></li>
</ul>
<p>这些<strong>评估指标</strong>必须是在训练集或开发集或测试集上计算或求出来的。</p>
<p>所以还需要做一件事，就是设立训练集、开发集，还有测试集。</p>
<h2 id="训练开发测试集划分traindevtest-distributions"><a class="anchor" href="#训练开发测试集划分traindevtest-distributions">#</a> 训练 / 开发 / 测试集划分（Train/dev/test distributions）</h2>
<blockquote>
<p>dev set 是用来选择方案的</p>
</blockquote>
<p>开发集 <strong>dev</strong> 也叫做保留交叉验证集（<strong>hold out cross validation set</strong>）</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427165638917.png?lastModify=1726287112" alt="image-20240427165638917" /></p>
<p>开发一个猫分类器，如何在这些国家中设立开发集和测试集呢</p>
<p>一种是：前 4 个作为 <strong>dev set</strong>，后 4 个作为 <strong>test set</strong></p>
<ul>
<li>
<p>缺点就是：开发集和测试集来自不同的分布。</p>
<blockquote>
<p>需要让它们来自相同的分布</p>
</blockquote>
</li>
<li>
<p>有可能最终在测试集上测试系统时，测试集的数据和开发集的数据 <strong>差异很大</strong>，并发现花了那么多个月的时间去针对<strong>开发集</strong>优化，在测试集上的表现却<strong>不佳</strong>。</p>
</li>
<li>
<p>若开发集和测试集来自不同的分布</p>
<blockquote>
<p>就好像原本尝试毕竟靶心，最后却需要移动靶心</p>
</blockquote>
</li>
</ul>
<p>为了避免上述情况，最好随机洗牌，也就是开发集和测试集都有来自八个地区的数据，并且开发集和测试集都来自同一分布，这分布就是你的所有数据<strong>混在一起</strong>。</p>
<p>对于靶心。设立<strong>开发集</strong>加上一个<strong>单实数评估指标</strong>，这就是像是定下目标， 然后告诉你的团队，那就是你要瞄准的<strong>靶心</strong></p>
<blockquote>
<p>因为一旦建立了这样的开发集和指标，团队就可以<strong>快速迭代</strong>，尝试不同的想法，跑实验，可以很快地使用开发集和指标去评估不同分类器，然后尝试选出最好的那个分类器</p>
</blockquote>
<p>而设立训练集的<strong>方式</strong>则会影响你逼近那个目标有多快</p>
<h2 id="开发集和测试集的大小size-of-dev-and-test-sets"><a class="anchor" href="#开发集和测试集的大小size-of-dev-and-test-sets">#</a> 开发集和测试集的大小（Size of dev and test sets）</h2>
<p>[训练，验证，测试集](#Size of dev and test sets)</p>
<h2 id="什么时候该改变开发测试集和指标"><a class="anchor" href="#什么时候该改变开发测试集和指标">#</a> 什么时候该改变开发 / 测试集和指标？</h2>
<p>设置开发集和评估<strong>指标</strong>，就像是把目标定在某个位置，让你的团队瞄准。但有时候在项目进行途中，可能意识到，目标的位置放错了。这种情况下，你应该移动你的目标。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427180950666.png?lastModify=1726287112" alt="image-20240427180950666" /></p>
<blockquote>
<p>对于 <strong>A</strong></p>
<ul>
<li>
<p>虽然有 3% 的错误，但是 <strong>A</strong> 因为某种原因，把很多 pornographic 图片<strong>分类成</strong> cat。</p>
<p>那么在部署 <strong>A</strong> 的时候，用户会看到更多的猫图（因为识别猫的错误率只有 3%）</p>
<p>但同时也会给用户推送一些 pornographic 图像，完全不可接收</p>
</li>
</ul>
<p>对于 <strong>B</strong></p>
<ul>
<li>即使 <strong>B</strong> 有 3% 的错误，但是不会推送 pornographic 图 <a href="test%5Cmodel%5Cmymodel%5Cmeta.py">meta.py</a> 像，所以 <strong>B</strong> 实际上是<strong>更好</strong>的算法</li>
</ul>
<hr />
<p>评估指标 + 开发集：Prefer <strong>A</strong></p>
<ul>
<li><strong>A</strong> 的错误率低</li>
</ul>
<p>用户 / 你：Prefer <strong>B</strong></p>
</blockquote>
<p>在这种情况下，原来的<strong>指标错误</strong>地预测算法 <strong>A</strong> 是更好的算法。</p>
<p>这就发出了信号：</p>
<ul>
<li>应该改变评估指标了</li>
<li>或者要改变开发集或测试集。</li>
</ul>
<p>在这种情况下，你用的<strong>分类错误率指标</strong>可以写成这样：</p>
<p ypred(i)≠y(i)="">Error=1mdevΣi=1mdev⁡I</p>
<ul>
<li>函数 I：一个函数， 统计出里面这个表达式为真的样本数</li>
</ul>
<blockquote>
<p>在整个开发集样本中不等于 cat 的数量 <strong>/</strong> 整个样本的数量</p>
</blockquote>
<p>其中一个<strong>修改</strong>评估指标的方法是：</p>
<p>Error=1Σi=1mdev⁡w(i)Σi=1mdev⁡w(i)I{ypred(i)≠y(i)}w(i)={1,if x(i) is non−porn10,if x(i) is porn</p>
<blockquote>
<p>也就是赋予 pornographic 图像更大的<strong>权重</strong>，识别到该图时候，可以让 <strong>Error</strong> 快速变大</p>
</blockquote>
<p>【注】：不需要太注重新错误率指标是怎么定义的。</p>
<p>关键在于，如果对旧的错误率指标不满意，那就<strong>不要</strong>一直沿用不满意的错误率指标</p>
<ul>
<li>而应该尝试定义一个<strong>新的指标</strong>，能够更加符合偏好，定义出实际更适合的算法。</li>
</ul>
<p><strong>正交化</strong></p>
<p>处理机器学习问题时，应该把它切分成独立的步骤</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427184739178.png?lastModify=1726287112" alt="image-20240427184739178" /></p>
<ol>
<li>
<p>设定目标：定义一个指标去评估分类器（独立的旋钮去调整）</p>
</li>
<li>
<p>分开考虑如何<strong>改善</strong>系统在这个指标上的表现</p>
<blockquote>
<p>例如针对上述成本函数 J 的优化，并像之前一样引入<strong>权重</strong>修改 J，更加符合偏好</p>
</blockquote>
</li>
</ol>
<blockquote>
<p>【注】：如何定义 J 并不重要，关键在于正交化的思路</p>
<ul>
<li>把<strong>设立目标</strong>定为第一步</li>
<li>然后<strong>瞄准和射击目标</strong>是独立的第二步。</li>
</ul>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427191132306.png?lastModify=1726287112" alt="image-20240427191132306" /></p>
<p>上述问题在于：开发集和测试集的图片取景很专业，而用户真正关心的是，他们上传的图片（可能低质量）能不能被<strong>正确识别</strong>。</p>
<blockquote>
<p>若在当前开发集或者开发集和测试集分布中表现很好</p>
<p>但是但你的实际应用程序，你真正关注的地方表现不好（实际中）</p>
</blockquote>
<p>那么就需要<strong>修改指标</strong>或者<strong>开发测试集</strong></p>
<ul>
<li>
<p>也就是开发测试集都是这些高质量图像，但在开发测试集上做的评估<strong>无法预测你的应用实际的表现</strong></p>
<blockquote>
<p>因为你的应用处理的是低质量图像</p>
</blockquote>
<p>那么就应该改变你的开发测试集，让你的数据更能反映你实际需要处理好的数据。</p>
</li>
</ul>
<p><strong>保持同一个分布</strong>：[训练 / 开发 / 测试集划分](#Train/dev/test distributions)</p>
<p><strong>评估指标和开发集</strong>：可以更快做出决策，判断算法 A 还是算法 B 更优，可以<strong>加速</strong>团队迭代的速度</p>
<blockquote>
<p>即使无法定义出一个很完美的评估指标和开发集，你直接<strong>快速设立</strong>出来，然后使用它们来驱动团队的迭代速度。</p>
<p>如果在这之后，发现选的不好，有更好的想法，那么完全可以马上改。</p>
</blockquote>
<p>最好<strong>不要</strong>在没有评估指标和开发集时跑太久，因为那样可能会减慢团队迭代和改善算法的速度。</p>
<blockquote>
<p>例如：算法可能会在不同的数据集上表现不一致，难以衡量其性能。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427153830675.png?lastModify=1726287112" alt="image-20240427153830675" /></p>
</blockquote>
<h2 id="为什么是人的表现"><a class="anchor" href="#为什么是人的表现">#</a> 为什么是人的表现？</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427211358551.png?lastModify=1726287112" alt="image-20240427211358551" /></p>
<blockquote>
<p>当开始往人类水平努力，进展很快，但是达到阈值后，表现比人类更好时候，进展和精确度的提升就变得更慢了</p>
</blockquote>
<p>贝叶斯最优错误率（Bayes optimal error）：。随着时间的推移，当继续训练算法时，可能模型越来越大，数据越来越多，但是性能<strong>无法超过</strong>某个理论上限</p>
<ul>
<li>所以贝叶斯最优错误率一般认为是理<strong>论上可能达到的最优错误率</strong></li>
<li>从 x 映射到 y 的理论最优函数，永远不会被超越</li>
</ul>
<p>机器学习的进展往往相当快，直到超越人类的表现之前一直很快，当超越人类的表现时，有时进展会<strong>变慢</strong>。</p>
<blockquote>
<p>原因一：<strong>离贝叶斯最优错误率</strong>已经不远了。人们非常擅长看图像，分辨里面有没有猫或者听写音频。所以，当超越人类的表现之后也许<strong>没有太多的空间</strong>继续改善了。</p>
<ul>
<li>空间不足</li>
</ul>
<p>原因二：只要表现比人类的表现更差，那么实际上可以使用某些<strong>工具</strong>来提高性能。一旦超越了人类的表现，这些工具就没那么好用了。</p>
<ul>
<li>工具不好用</li>
</ul>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240427212623014.png?lastModify=1726287112" alt="image-20240427212623014" /></p>
<p>只要你的机器学习算法比人类差</p>
<ul>
<li>
<p>从让人帮你标记数据</p>
</li>
<li>
<p>人工错误率分析</p>
<blockquote>
<p>可以让人类看看你算法处理的例子，知道错误出在哪里，并尝试了解<strong>为什么</strong>人能做对，<strong>算法做错</strong></p>
</blockquote>
</li>
<li>
<p>以更好地分析偏差和方差</p>
</li>
</ul>
<p>但只要算法做得比人类好，这三种策略就很难利用了。</p>
<h2 id="可避免偏差avoidable-bias"><a class="anchor" href="#可避免偏差avoidable-bias">#</a> 可避免偏差（Avoidable bias）</h2>
<p>你希望你的学习算法能在训练集上表现良好，但有时你实际上<strong>并不想做得太好</strong>。</p>
<p>你得知道人类水平的表现是怎样的，可以确切告诉你算法在训练集上的表现到底应该有多好，或者有多不好</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429123517344.png?lastModify=1726287112" alt="image-20240429123517344" /></p>
<blockquote>
<p>以人类的错误率<strong>估计或者代替</strong>贝叶斯错误率</p>
<ul>
<li>因为人类实际上是非常擅长计算机视觉任务的</li>
</ul>
</blockquote>
<p>对于左边这种情况：在训练集上的表现和人类水平的表现有<strong>很大差距</strong></p>
<ul>
<li>
<p>说明算法对训练集的拟合不好，所以应该重点减少偏差。</p>
<blockquote>
<p>例如：训练更大的神经网络，多次梯度下降</p>
</blockquote>
</li>
</ul>
<p>对于右边这种情况：应该重点减少方差</p>
<ul>
<li>例如：正则化，更大训练集</li>
</ul>
<blockquote>
<p>对于训练集上的偏差：没有太多改善的空间，应该与贝叶斯错误率只差 0.5%</p>
<ul>
<li>除非过拟合</li>
</ul>
</blockquote>
<p><strong>可避免误差</strong>：<strong>贝叶斯</strong>错误率或者对贝叶斯错误率的估计和<strong>训练</strong>错误率之间的差值</p>
<p>左边的例子中，这 7% 衡量了可避免偏差大小， 而 2% 衡量了方差大小。</p>
<ul>
<li>所以在左边这个例子里，专注减少可避免偏差可能潜力更大。</li>
</ul>
<h2 id="理-解-人-的-表-现-understanding-human-level-performance"><a class="anchor" href="#理-解-人-的-表-现-understanding-human-level-performance">#</a> 理 解 人 的 表 现 （ Understanding human-level  performance）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429125518897.png?lastModify=1726287112" alt="image-20240429125518897" /></p>
<p>思考<strong>人类水平错误率</strong>最有用的方式之一：把它作为<strong>贝叶斯错误率</strong>的替代或估计。</p>
<p>人类水平错误率的定义可以<strong>不一样</strong></p>
<ul>
<li>
<p>为了发表研究论文或者部署系统，可以使用 1%</p>
</li>
<li>
<p>对于有些系统的医生团队可以到达 0.5% 的错误率，那么根据定义，最优错误率必须在 0.5% 以下。</p>
<blockquote>
<p>可以使用 0.5% 来作为贝叶斯错误率</p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429130716243.png?lastModify=1726287112" alt="image-20240429130716243" /></p>
<p>对于最右边的例子：若贝叶斯估计为 0.5%，也就是可避免偏差比方差大，可以尽量的去减少偏差。</p>
<p>若用 0.7% 代替 0.5%，那么测得的可避免偏差基本上是 0%，那就可能忽略可避免偏差了。</p>
<ul>
<li>
<p>可以试一试能不能让训练集做的更好，也就是尽量减少偏差</p>
<blockquote>
<p>因为：<strong>可能无法知道离贝叶斯错误率有多远</strong></p>
</blockquote>
</li>
<li>
<p>当接近人类水平时进展会越来越难</p>
</li>
</ul>
<p>若用 1% 代替 0.5%，那么很难知道是不是应该<strong>继续去拟合</strong>训练集（可能导致高方差）</p>
<ul>
<li>这种问题只会出现在你的算法已经做得很好的时候，只有你已经做到 0.7%,0.8%, 接近人类水平时会出现。</li>
</ul>
<p>对于左边的两个例子：<strong>远离</strong>人类水平时，将优化目标放在偏差或方差上可能更<strong>容易</strong>一点。</p>
<p>对于最右边的例子：当你们<strong>接近</strong>人类水平时，<strong>更难</strong>分辨出问题是偏差还是方差。</p>
<p><strong>所以</strong>：机器学习项目的进展在已经做得很好的时候，很难更进一步。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429131944990.png?lastModify=1726287112" alt="image-20240429131944990" /></p>
<p>在人类可以做得很好的任务中， 可以估计人类水平的错误率，使用人类水平错误率来<strong>估计</strong>贝叶斯错误率。</p>
<ul>
<li>训练错误率到贝叶斯错误率估计值的差距：<strong>可避免偏差</strong>有多大</li>
<li>训练错误率和开发错误率之间的差值：<strong>方差上的问题</strong>有多大</li>
</ul>
<p>对于识别猫：人类表现接近完美，所以贝叶斯错误率页接近完美（几乎为 0%）</p>
<p>对于数据噪点很多时，比如背景声音很嘈杂的语言识别</p>
<ul>
<li>
<p>更好的估计贝叶斯错误率很有必要，可以帮助你<strong>更好地估计</strong>可避免偏差和方差</p>
<p>这样你就能更好的做出<strong>决策</strong></p>
<blockquote>
<p>选择减少偏差的策略</p>
<p>还是减少方差的策略。</p>
</blockquote>
</li>
</ul>
<p>这个决策技巧通常很有效，直到你的系统性能开始超越人类，那么你对贝叶斯错误率的估计就<strong>不再准确</strong>了</p>
<blockquote>
<p>若一再减少偏差可能<strong>过拟合</strong>，导致<strong>高方差</strong>（死循环）</p>
</blockquote>
<h2 id="超过人的表现surpassing-human-level-performance"><a class="anchor" href="#超过人的表现surpassing-human-level-performance">#</a> 超过人的表现（Surpassing human- level performance）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429133124673.png?lastModify=1726287112" alt="image-20240429133124673" /></p>
<p>对于可避免偏差</p>
<ul>
<li>
<p>左边：0.5% 就是对贝叶斯错误率的估计，所以可避免偏差是 0.1%</p>
<blockquote>
<p>减少方差的可能的空间比减少偏差的大</p>
</blockquote>
</li>
<li>
<p>右边：由于训练错误率低于对贝叶斯错误率的估计，很难直到可避免偏差是多少。</p>
<blockquote>
<p>可能过拟合了 0.2%</p>
<p>贝叶斯错误率其实是 0.1%,0.2%,0.3%</p>
</blockquote>
<p>实际上<strong>没有足够的信息</strong>来判断优化你的算法时应该专注减少偏差还是减少方差，这样你取得进展的效率就会降低。</p>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429134415034.png?lastModify=1726287112" alt="image-20240429134415034" /></p>
<p>对于左边四个例子都是从结构化的数据中学习得来的，并不是自然感知问题</p>
<ul>
<li>系统看到的数据量可能比任何人类能看到的都<strong>多</strong>，所以这样就相对容易得到超越人类水平的系统。</li>
</ul>
<p>而对于自然感知任务中，人类往往表现非常好，所以有可能对计算机来说在自然感知任务的表现要超越人类要<strong>更难</strong>一些。</p>
<h2 id="改善你的模型的表现improving-your-model-performance"><a class="anchor" href="#改善你的模型的表现improving-your-model-performance">#</a> 改善你的模型的表现（Improving your model  performance）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429135208689.png?lastModify=1726287112" alt="image-20240429135208689" /></p>
<p>想要让一个监督学习算法达到实用，基本上希望或者假设你可以完成两件事情</p>
<ul>
<li>算法对训练集的拟合很好（可以看出做到可避免偏差很低）</li>
<li>在训练集中做得很好，然后推广到开发集和测试集也很好（方差不是太大）。</li>
</ul>
<blockquote>
<p>在正交化下：上述共有两组旋钮</p>
<ul>
<li>修正可避免偏差问题</li>
<li>处理方差问题：正则化，收集更多训练数据</li>
</ul>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429135955789.png?lastModify=1726287112" alt="image-20240429135955789" /></p>
<p><strong>可避免偏差</strong>：</p>
<ul>
<li>
<p>规模更大模型</p>
</li>
<li>
<p>更好的优化算法：<strong>Momentum</strong>，<strong>RMSprop</strong>，<strong>Adam</strong> 等等。</p>
</li>
<li>
<p>更好的神经网络架构或更好的超参数</p>
<blockquote>
<p>改变激活函数，改变层数或者隐藏单位数（可能会让模型规模变大）</p>
<p>或者试用其他模型，其他架构，如循环神经网络和卷积神经网络。</p>
<ul>
<li>很难预先判断是否能更好地拟合训练集，但有时换架构可能会得到好得多的结果。</li>
</ul>
</blockquote>
</li>
</ul>
<p><strong>方差</strong>：</p>
<ul>
<li>
<p>收集更多数据</p>
<blockquote>
<p>收集更多数据去训练可以帮你更好地<strong>推广</strong>到系统<strong>看不到的开发集数据</strong></p>
</blockquote>
</li>
<li>
<p>正则化：<strong>L2</strong>，<strong>dropout</strong>，<strong>data augumentation</strong></p>
</li>
</ul>
<h2 id="进行错误分析carrying-out-error-analysis"><a class="anchor" href="#进行错误分析carrying-out-error-analysis">#</a> 进行错误分析（Carrying out error analysis）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429145659475.png?lastModify=1726287112" alt="image-20240429145659475" /></p>
<blockquote>
<p>在开发集上：90% 的准确率，10% 的错误率</p>
<p>注意到：算法将狗分类为猫。</p>
<p>假设专门针对狗设计项目，花很长时间却发现一点用都没有（值不值得）</p>
</blockquote>
<p>假设 100 个错误标记样本中只有 5% 是狗，就是说在 100 个<strong>错误</strong>标记的开发集样本中， 有 5 个是狗。</p>
<ul>
<li>
<p><strong>也就是</strong>：在典型的 100 个出错样本中，即使你完全解决了狗的问题，你也只能修正这 100 个错误中的 5 个。</p>
</li>
<li>
<p>那么最多只能希望错误率从 10%→9.5%</p>
<blockquote>
<p>也就是错误率只下将了 0.5%</p>
</blockquote>
</li>
</ul>
<p>但至少这个分析给出了一个<strong>上限</strong>。</p>
<ul>
<li>
<p>如果你继续处理狗的问题，能够改善算法性能的上限。</p>
<blockquote>
<p>就意味着，<strong>最好能到哪里</strong>，完全解决狗的问题可以对你有多少帮助。</p>
</blockquote>
</li>
</ul>
<p>假设 100 个错误标记样本中只有 50% 是狗，就是说在 100 个<strong>错误</strong>标记的开发集样本中， 有 50 个是狗。</p>
<ul>
<li>这 100 个错误标记的开发集样本， 你发现实际有 50 张图都是狗，所以有 50% 都是狗的照片，现在花时间去解决狗的问题可能<strong>效果就很好</strong>。</li>
<li>那么希望错误率从 10%→5%</li>
</ul>
<p>简单的人工统计步骤，错误分析，可以节省大量时间，可以迅速决定什么是最重要的，或者最有希望的方向。</p>
<blockquote>
<p>如上述若只需要 5 到 10 分钟的时间，就能给你估计这个方向有多少价值。</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429151109523.png?lastModify=1726287112" alt="image-20240429151109523" /></p>
<blockquote>
<p>干扰分类器的不同类型（<strong>分类器结果 y^ 为 1 类型</strong>）</p>
</blockquote>
<p>快速统计的步骤，你可以经常做，最多需要几小时， 就可以真正帮你选出<strong>高优先级任务</strong>，并了解每种手段对性能有多大<strong>提升空间</strong>。</p>
<h2 id="清除标注错误的数据cleaning-up-incorrectly-labeled-data"><a class="anchor" href="#清除标注错误的数据cleaning-up-incorrectly-labeled-data">#</a> 清除标注错误的数据（Cleaning up Incorrectly labeled  data）</h2>
<h3 id="训练集"><a class="anchor" href="#训练集">#</a> 训练集</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429152257082.png?lastModify=1726287112" alt="image-20240429152257082" /></p>
<p>深度学习算法对于训练集中的<strong>随机错误</strong>是相当健壮的（robust）。</p>
<ul>
<li>只要你的标记出错的样本离随机错误<strong>不太远</strong></li>
</ul>
<p>若错误足够<strong>随机</strong>，那么放着这些错误 不管可能也没问题，而不要花太多时间修复它们。</p>
<p>只要总数据集总足够大，实际错误率可能不会太高。</p>
<p>而对于<strong>系统性错误</strong>：做标记的人一直把<strong>白色的狗标记成猫</strong></p>
<ul>
<li>分类器学习之后，会把所有白色的狗都分类为猫。</li>
</ul>
<h3 id="开发集和测试集"><a class="anchor" href="#开发集和测试集">#</a> 开发集和测试集</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429155354726.png?lastModify=1726287112" alt="image-20240429155354726" /></p>
<blockquote>
<p>若担心开发集或测试集上标记出错的样本带来的影响，一般建议你在<strong>错误分析</strong>时，添加一个额外的列，这样你也可以统计<strong>标签</strong>  y=1 错误的样本数。</p>
<p>其中分类器的输出 y^ 与开发集的标签 y 不一致</p>
<ul>
<li><strong>因为是标签出错了，而不是分类器出错</strong></li>
</ul>
</blockquote>
<p>是否值得修正 6% 的标记出错的样本</p>
<ul>
<li>若这些标记错误<strong>严重影响</strong>了你在开发集上评估算法的能力，那么就应该去花时间修正错误的标签。</li>
<li>若它们<strong>没有严重</strong>影响到你用开发集评估成本偏差的能力，那么可能就不应该花宝贵的时 间去处理。</li>
</ul>
<p>一般看 3 个数字来确定是否值得去<strong>人工修正</strong>标记出错的数据</p>
<ul>
<li>整体开发集错误率（Overall dev set error）</li>
<li><strong>错误标记</strong>引起的错误的数量或者百分（Errors due incorrect labels）</li>
<li>其他原因导致的错误（Errors due to other causes）</li>
</ul>
<p>例如左边：开发集上有 10% 错误，其中 0.6% 是因为标记出错，剩下的占 9.4%，是其他原因导致的（比如把狗误认为猫， 大猫图片）。</p>
<ul>
<li>说明 9.4% 错误率（其他原因）需要<strong>集中精力修正</strong></li>
</ul>
<p>对于右边：开发集上有 2% 错误，其中 0.6% 是因为标记出错，剩下的占 1.4%，是其他原因导致的（比如把狗误认为猫， 大猫图片）。</p>
<ul>
<li>说明标记出错实际上占比 30%。测得的那么大一部分的错误都是开发集<strong>标记出错</strong>导致的，那似乎修正开发集里的错误标签似乎更有价值。</li>
</ul>
<p>开发集的主要<strong>目的</strong>：希望用它来从两个分类器 A 和 B 中选择一个。</p>
<p>所以若在开发集分别 2.1%,1.9% 的错误率，不能确定前者比后者优。</p>
<ul>
<li>
<p>也就是不能信任开发集了，无法告诉你这个分类器是否比这个好</p>
<blockquote>
<p>因为 0.6% 的错误率是<strong>标记出错</strong>导致的。</p>
</blockquote>
</li>
</ul>
<p>现在就有很好的理由去修正开发集里的<strong>错误标签</strong></p>
<ul>
<li>右边：标记出错对算法错误的整体评估标准有<strong>严重</strong>的影响。</li>
<li>左边：标记出错对你算法影响的百分比还是<strong>相对较小</strong>的</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429181902928.png?lastModify=1726287112" alt="image-20240429181902928" /></p>
<p>对于第二点：考虑同时检验算法判断正确和判断错误的样本</p>
<blockquote>
<p>算法有可能因为<strong>运气好</strong>把某个东西判断对了</p>
</blockquote>
<ul>
<li>
<p>第二点不容易做，且通常不会做。</p>
<blockquote>
<p>因为：如果你的分类器很准确，那么判断错的次数比判断正确的次数要少得多。</p>
<p>那么就有 2% 出错，98% 都是对的</p>
</blockquote>
<p>所以更容易检查 2% 数据上的标签，然而检查 98% 数据上的标签要花的<strong>时间长得多</strong>，所以通常不这么做，但也是要考虑到的</p>
</li>
</ul>
<h2 id="快速搭建你的第一个系统并进行迭代"><a class="anchor" href="#快速搭建你的第一个系统并进行迭代">#</a> 快速搭建你的第一个系统，并进行迭代</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240429215931160.png?lastModify=1726287112" alt="image-20240429215931160" /></p>
<p>若所在领域有很多可以借鉴的学术文献，处理的问题和要解决的问题几乎完全相同</p>
<ul>
<li>
<p>那么可以从现有大量学术文献为基础出发，一开始就搭建比较复杂的系统。</p>
<blockquote>
<p>人脸识别就有很多学术文献，尝试搭建一个人脸识别设备</p>
</blockquote>
</li>
</ul>
<p>若第一次处理某个新问题</p>
<ul>
<li>建议构建一些快速而粗糙的实现，然后用来帮你找到改善系统要<strong>优先处理的方向</strong>。</li>
</ul>
<h2 id="在不同的划分上进行训练并测试"><a class="anchor" href="#在不同的划分上进行训练并测试">#</a> 在不同的划分上进行训练并测试</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240430093510189.png?lastModify=1726287112" alt="image-20240430093510189" /></p>
<blockquote>
<p>左图：200,000 个样本（高清）</p>
<p>右图：10,000 个样本（模糊）</p>
<p>但是真正<strong>关心</strong>的算法表现是最终的分类器要在 <strong>mobile app</strong> 上传的图片表现<strong>良好</strong></p>
</blockquote>
<p><strong>Option 1</strong>：两组数据集<strong>合并</strong>在一起（洗牌），这样共有了 210,000 个样本</p>
<ul>
<li>
<p>假设 <strong>training set</strong>：205,000 个样本；<strong>dev、test set</strong>：各 2500 个样本</p>
<blockquote>
<p>好处：方便统一管理（都来自同一分布）</p>
<p>坏处：在开发集中很多照片来自 <strong>web</strong>，并不是真正关心的数据分布</p>
<ul>
<li>例如：期望值 2500×200k210k=2381 个样本是 <strong>web</strong>，只有 <strong>119</strong> 个样本是 <strong>mobile app</strong></li>
</ul>
</blockquote>
</li>
<li>
<p><strong>数据不平衡</strong>：大部分精力都在优化 <strong>web</strong> 上面的图片（靶心有问题）</p>
</li>
</ul>
<p><strong>Option 2</strong>：若需要再加上 5000 个 <strong>mobile app</strong> 样本</p>
<ul>
<li>
<p>假设 <strong>training set</strong>：205,000 个样本；<strong>dev、test set</strong>：各 2500 个样本</p>
<blockquote>
<p><strong>training set</strong>：其中 5000 个样本来自 <strong>mobile app</strong></p>
<p><strong>dev、test set</strong>：都来自 <strong>mobile app</strong></p>
<p>缺点：<strong>training set</strong> 和 <strong>dev、test set</strong> 分布不一样</p>
</blockquote>
</li>
<li>
<p>事实证明，这样把数据分成 训练、开发和测试集，在长期能给你带来更好的系统性能。</p>
</li>
<li>
<p>靶心就是需要处理的目标</p>
</li>
<li>
<p><strong>dev、test set</strong> 来自你需要处理的目标的数据</p>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240430095235870.png?lastModify=1726287112" alt="image-20240430095235870" /></p>
<h2 id="数据分布不匹配时的偏差与方差的分析bias-and-variance-with-mismatched-data-distributions"><a class="anchor" href="#数据分布不匹配时的偏差与方差的分析bias-and-variance-with-mismatched-data-distributions">#</a> 数据分布不匹配时的偏差与方差的分析（Bias and  Variance with mismatched data distributions）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240430101832566.png?lastModify=1726287112" alt="image-20240430101832566" /></p>
<blockquote>
<p>若开发集来自同一分布，那么存在很大的方差问题</p>
</blockquote>
<p>若开发集来不不同分布，那么就不能确定是方差问题了</p>
<blockquote>
<p>因为可能训练集容易识别，然而开发集却很难识别</p>
</blockquote>
<p>两件事变化：</p>
<ol>
<li>算法只见过训练集，没有见过开发集</li>
<li>开发集来自不同分布</li>
</ol>
<blockquote>
<p>很难确认增加 9% 误差率是因为方差问题，还是不同分布问题（没看到开发集的数据）</p>
</blockquote>
<p>为了弄清楚哪个因素影响大，可以定义一组新的数据：<strong>Training-dev set</strong></p>
<ul>
<li>
<p>与 <strong>training set</strong> 相同分布，但是不作为训练集</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240430102857089.png?lastModify=1726287112" alt="image-20240430102857089" /></p>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240430103212284.png?lastModify=1726287112" alt="image-20240430103212284" /></p>
<blockquote>
<p>左边：方差（过拟合）</p>
<p>右边：数据不匹配</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240430103429000.png?lastModify=1726287112" alt="image-20240430103429000" /></p>
<blockquote>
<p>左边：可避免偏差</p>
<p>右边：可避免偏差 + 数据不匹配</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240430104031726.png?lastModify=1726287112" alt="image-20240430104031726" /></p>
<blockquote>
<p>右边：若 <strong>Training set error、Training-dev set error</strong> 比 <strong>Dev error、Test error</strong> 高很多，意外</p>
<ul>
<li>说明训练数据其实比你的开发集和测试集<strong>难识别得</strong>多（例如处理语音识别任务时）</li>
</ul>
</blockquote>
<ul>
<li>若 <strong>Dev error</strong> 比 <strong>Test error</strong> 低很多，说明对 <strong>dev set</strong> 过拟合</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501185818343.png?lastModify=1726287112" alt="image-20240501185818343" /></p>
<h2 id="处理数据不匹配问题addressing-data-mismatch"><a class="anchor" href="#处理数据不匹配问题addressing-data-mismatch">#</a> 处理数据不匹配问题（Addressing data mismatch）</h2>
<p>训练集来自和开发测试集不同的分布，可能会导致数据不匹配的问题</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501190630402.png?lastModify=1726287112" alt="image-20240501190630402" /></p>
<p>对于第一点，若是语音的话，可能需要人工听一下来自开发集的样本，并尝试弄清楚<strong>开发集和训练集</strong>到底有什么<strong>不同</strong>。</p>
<blockquote>
<p>例如：可能会发现很多开发集样本噪音很多，有很多汽车噪音，这是开发集和训练集差异之一</p>
<p>在车子里的语言激活后视镜，发现它可能经常识别<strong>错误街道号码</strong></p>
</blockquote>
<ul>
<li>当了解开发集误差的性质时，就知道开发集有可能跟训练集<strong>不同或者更难识别</strong>，那么可以尝试把训练数据变得更像开发集一点（第二点）</li>
</ul>
<p>对于第二点：可以收集更多<strong>类似</strong>开发集和测试集的数据</p>
<blockquote>
<p>例如：发 现车辆背景噪音是主要的错误来源，那么可以<strong>模拟车辆噪声数据</strong></p>
</blockquote>
<p>若目标是让训练数据<strong>更接近</strong>你的开发集</p>
<p>可以通过<strong>人工数据合成</strong>，可以快速制造更多的训练数据，就像真的在车里录的那样， 那就不需要花时间实际出去收集数据</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501193532248.png?lastModify=1726287112" alt="image-20240501193532248" /></p>
<blockquote>
<p><strong>The quick brown fox jumps over the lazy dog</strong>：清晰的语音：10000 小时</p>
<p><strong>Car noise</strong>：嘈杂的语音：1 小时</p>
</blockquote>
<p>对于将 1 小时的语言混入到 10000 小时的语音中，可能出现潜在的问题：</p>
<ul>
<li>对 1 小时的语音<strong>过拟合</strong></li>
</ul>
<blockquote>
<p>如果只录了一小时汽车噪音，那可能只模拟了全部数据空间的一小部分，可能只从汽车噪音的<strong>很小的子集</strong>来合成数据。</p>
</blockquote>
<p>若收集 10,000 小时的汽车噪音是否可行，这样就不用一遍又一遍地回放那 1 小时汽车噪音，-</p>
<ul>
<li>就有 10,000 个小时永不重复的汽车噪音来叠加到 10,000 小时安静背景下录得的永不重复 的语音录音。</li>
<li>这是可以做的，但不保证能做。但是使用 10,000 小时永不重复的汽车噪音， 而不是 1 小时重复学习，算法有可能取得<strong>更好的性能。</strong></li>
</ul>
<blockquote>
<p>但是人耳是无法分辨这 10,000 个小时听起来和那 1 小时<strong>没什么区别</strong>，所以最后可能会制 造出这个原始数据很少的，在一个<strong>小得多的空间子集</strong>合成的训练数据，但你自己没意识到。</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501194827676.png?lastModify=1726287112" alt="image-20240501194827676" /></p>
<blockquote>
<p>人工合成数据的另一个例子，假设你在研发无人驾驶汽车，你可能希望检测出这样的车，然后用这样的框包住它。</p>
</blockquote>
<p>一个思路是：用计算机合成图像来<strong>模拟</strong>成千上万的车辆</p>
<blockquote>
<p>例如右边两张图片就是合成的</p>
</blockquote>
<ul>
<li>但是还是也有可能出现这种情况：学习算法可能会对<strong>合成的这一个小子集过拟合</strong>。</li>
</ul>
<p>特别是：在电脑游戏中（例如地平线）进行截图车辆</p>
<blockquote>
<p>假设这个游戏只有 20 辆独立的车</p>
</blockquote>
<ul>
<li>
<p>如果你用着 20 量独特的车合成的照片去训练系统，那么你的神经网络很可能对这 20 辆车<strong>过拟合</strong>，但人类很难分辨出来。</p>
<blockquote>
<p>即使这些图像看起来很逼真，你可能真的只用了所有可能出现的车辆的<strong>很小的子集</strong>。</p>
</blockquote>
</li>
</ul>
<h2 id="迁移学习transfer-learning"><a class="anchor" href="#迁移学习transfer-learning">#</a> 迁移学习（Transfer learning）</h2>
<p>有的时候神经网络可以从一个任务中习得知识， 并将这些知识应用到另一个独立的任务中</p>
<p>例如：你已经训练好一个神经网络， 能够识别像<strong>猫</strong>这样的对象，然后使用那些知识，或者部分习得的知识去帮助您更好地阅读 <strong>x</strong> 射线扫描图，这就是所谓的迁移学习。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501200856217.png?lastModify=1726287112" alt="image-20240501200856217" /></p>
<blockquote>
<p>在第一阶段的训练过程中：当你进行图像识别任务训练时，你可以训练神经网络的所有常用参数，所有的权重，所有的层，然后你就得到了一个能够做<strong>图像识别预测的网络</strong>。</p>
</blockquote>
<p>要实现迁移学习：删除最后的输出层以及权重，把数据集换成新的 (x,y) 一对</p>
<ul>
<li>现在这些变成放射科图像</li>
</ul>
<p>然后初始化最后一层权重：W [L],b [L] 随机初始化</p>
<p>最后在这个新数据集上重新训练网络，也就是在新的放射科数据集上训练网络。</p>
<ul>
<li>
<p>若数据集<strong>较小</strong>：就只训练输出层前的最后一层，或者也许是最后一两层</p>
<blockquote>
<p>训练整个网络可能导致对该数据集过拟合</p>
</blockquote>
</li>
<li>
<p>若数据集<strong>多</strong>：可以重新训练网络中的<strong>所有参数</strong>。</p>
</li>
</ul>
<p>若重新训练神经网络中的<strong>所有参数</strong>，那么这个在图像识别数据的<strong>初期训练阶段</strong>，有时称为<strong>预训练（pre-training）</strong></p>
<blockquote>
<p>因为你在用图像识别数据去<strong>预先初始化</strong>，或者预训练神经网络的权重。</p>
</blockquote>
<p><strong>微调（fine tuning）</strong>：然后更新相应权重，在放射科数据上<strong>训练</strong></p>
<p>为什么这样做有效果呢？</p>
<ul>
<li>
<p>有很多<strong>低层次特征</strong>：往往是泛化的，易于表达的（如纹理，颜色、边缘、棱角等）</p>
<blockquote>
<p>例如：边缘检测、曲线检测、阳性对象检测</p>
<p><strong>高层次特征</strong>：往往复杂的，难以说明的</p>
<ul>
<li>比如金色的头发、瓢虫的翅膀、缤纷的花儿等</li>
</ul>
</blockquote>
</li>
<li>
<p>算法学到了很多结构信息，其中一些知识可能会很有用</p>
<blockquote>
<p>图像形状的信息，可以了解不同图像的组成部分是怎样的，学到线条、点、曲线这些知识，也许对象的一小部分</p>
<p>这些知识有可能帮助放射科诊断网络<strong>学习更快一些</strong>，或者需要<strong>更少的学习数据</strong>。</p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501201908824.png?lastModify=1726287112" alt="image-20240501201908824" /></p>
<p>迁移学习什么时候是有意义的呢？</p>
<p><strong>迁移学习定义</strong>：将某个领域或任务上学习到的知识或模式应用到不同但相关的领域或问题中</p>
<ul>
<li><strong>宏观上有较多相似处</strong>才能用迁移学习</li>
</ul>
<p>对于图像识别应用在放射科诊断：1,000,000 个图像识别的样本；100 个放射科样本</p>
<ul>
<li>那么前者可以学习<strong>低层次的特征</strong>，可以在神经网络的前面几层学到如何识别很多有用的特征</li>
<li>虽然只有 100 次 <strong>x</strong> 射线扫描，但可以从图像识别训练中学到的<strong>很多知识迁移</strong>，并且真正帮你加强放射科识别任务的性能，即使你的放射科数据很少。</li>
</ul>
<p>对于语音识别应用在唤醒字检测：10,000h  的语音识别与 1h 的唤醒字样本</p>
<ul>
<li>同理</li>
</ul>
<p>反过来</p>
<ul>
<li>若 100 个图像识别的样本；1,000 个放射科样本。说明后者的样本比前者<strong>更有价值</strong></li>
<li>若 10h  的语音识别与 50h 的唤醒字样本，相似度很大。迁移学习可能会有帮助，虽然 10 小时的语音识别不会有太大坏处，但是增益也不大</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501204340409.png?lastModify=1726287112" alt="image-20240501204340409" /></p>
<p>总结：若训练数据很少，所以需要找一个数据很多的相关问题来预先学习，并将知识<strong>迁移</strong>到这个新问题上</p>
<h2 id="多任务学习multi-task-learning"><a class="anchor" href="#多任务学习multi-task-learning">#</a> 多任务学习（Multi-task learning）</h2>
<p>在迁移学习中，你的步骤是<strong>串行</strong>的，你从任务 A 里学习只是然后迁移到任务 B。</p>
<p>在多任务学习中，你是<strong>同时</strong>开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每 个任务都能帮到其他所有任务。</p>
<p>例如需要检测图片中的行人、车辆、停车标志，还有交通灯各种其他东西</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501210348381.png?lastModify=1726287112" alt="image-20240501210348381" /></p>
<p>若这是输出图像 x (i)，那么不再是一个标签 y (i)，而是有四个标签：y (i):4×1</p>
<p>对于 m 个样本：Y=[|||⋯|y (1) y (2) y (3)⋯y (m)|||⋯|]</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501210807505.png?lastModify=1726287112" alt="image-20240501210807505" /></p>
<blockquote>
<p>第一个输出节点：是否有行人</p>
<p>第二个输出节点：是否有车</p>
<p>第三个输出节点：是否有停车标志</p>
<p>第四个输出节点：是否有红绿灯</p>
</blockquote>
<p>整个训练集的平均损失 <strong>Loss</strong>：</p>
<p>1mΣi=1m⁡Σj=14⁡L(y^j(i),yj(i))</p>
<ul>
<li>Σj=14⁡L (y^j (i),yj (i)) 是单个预测的损失：四个分量的<strong>求和</strong></li>
<li>L(y<sup>j(i),yj(i))=−yj(i)log⁡y</sup>j(i)−(1−yj(i))log⁡(1−y^j(i))</li>
</ul>
<p>与 <strong>Softmax</strong> 的主要区别在于</p>
<ul>
<li><strong>Softmax</strong> 的 y^ 只有一个，不可能同时是猫又同时是狗：单个标签分给单个样本</li>
<li>多任务学习：多个任务可以同时存在，例如一张图可能：行人，车都为 1</li>
</ul>
<p>对于神经网络</p>
<ul>
<li>
<p>可以建立<strong>单个</strong>神经网络，解决 4 个问题，系统试图告诉你每张图里面有没有这四个物体</p>
<blockquote>
<p>神经网络一些<strong>早期特征</strong>，在识别不同物体时<strong>都会用到</strong></p>
<ul>
<li>训练一个神经网络做四件事情会<strong>比</strong>训练四个完全独立的神经网络分别做四件事性能要<strong>更好</strong></li>
</ul>
</blockquote>
</li>
<li>
<p>训练四个不同的网络</p>
</li>
</ul>
<p>另外，多任务学习也可以处理图像只有<strong>部分物体被标记</strong>的情况</p>
<p>Y=[11⋯0⋯?⋯01⋯1⋯1⋯??⋯1⋯?⋯??⋯0⋯?⋯]</p>
<p>即使有些没有被标签</p>
<blockquote>
<p>例如：一个标签样本只对行人，车标签为 1,0，而停车牌，红绿灯没有被标签</p>
</blockquote>
<p>也可以在上面训练算法，同时做四个任务，即使一些图像只有一小部分标签，其他是问号或者不管是什么。</p>
<ul>
<li>因为：损失函数支队带 0 和 1 标签的 j 值求和</li>
</ul>
<p>那么多任务学习什么时候有意义呢？</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501212952253.png?lastModify=1726287112" alt="image-20240501212952253" /></p>
<p>对于第二点：</p>
<ul>
<li>
<p>在迁移学习中：从这 1 百万个样本学到的知识，真的可以帮你增强对更小数据集任务 <strong>B</strong> 的训练</p>
</li>
<li>
<p>在多任务学习中：若专注于 A100 的学习，可用通过在其他 99 项任务的训练（99000）个样本，这可能大幅提升算法性能，可以提供很多知识来增强这个任务的性能。</p>
<blockquote>
<p>反之：只有 1000 个 样本的训练集，效果可能会很差。</p>
</blockquote>
<p>通常会看的是如果专注于<strong>单项任务</strong>，如果想要从多任务学习得到很大性能提升，那 么其他任务加起来必须要有比单个任务<strong>大得多的数据量</strong></p>
<blockquote>
<p>类似于迁移学习</p>
</blockquote>
</li>
</ul>
<p>对于第三点：</p>
<ul>
<li>
<p>多任务学习的替代方法是为每个任务训练一个单独的神经网络。</p>
<blockquote>
<p>因为：同时训练所有的任务可能会导致<strong>模型过大</strong></p>
</blockquote>
</li>
<li>
<p>若可以训练一个足够<strong>大的神经网络</strong>，那么多任务学习肯定不会或者很少会降低性能</p>
</li>
</ul>
<h2 id="什么是端到端的深度学习what-is-end-to-end-deep-learning"><a class="anchor" href="#什么是端到端的深度学习what-is-end-to-end-deep-learning">#</a> 什么是端到端的深度学习？（What is end-to-end deep  learning?）</h2>
<p>简而言之，以前有一些数据处理系统或者学习系统，它们需要<strong>多个阶段</strong>的处理。</p>
<p>那么端到端深度学习就是<strong>忽略</strong>所有这些不同的阶段，用单个神经网络代替它。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501220552796.png?lastModify=1726287112" alt="image-20240501220552796" /></p>
<p>端到端深度学习就只需要把训练集拿过来，直接学到了 x 和 y 之间的函数映射，直接绕过了其中很多步骤。</p>
<p>挑战之一：需要大量的数据才能让系统表现<strong>良好</strong></p>
<ul>
<li>数据量小的时候，传统的流水线方法效果可能更好</li>
<li>数据量中等的时候，可以采用中间件的方法（绿色），绕过特征提取...</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501224729945.png?lastModify=1726287112" alt="image-20240501224729945" /></p>
<p>对于一步到位：直接将原始照 片喂到一个神经网络，试图找出一个人的身份。</p>
<ul>
<li>其中一个问题是，人可以从<strong>很多不同的角度</strong>接近门禁，他们可能 在绿色位置，可能在蓝色位置。</li>
</ul>
<p>迄今最好的方法似乎是一个多步方法</p>
<blockquote>
<ol>
<li>检测人脸</li>
<li>放大裁剪图像使得居中，然后再喂到神经网络中</li>
</ol>
</blockquote>
<ul>
<li>
<p>第一步：弄清楚人脸在哪里；第二步：弄清楚这是谁</p>
<blockquote>
<p>对于第二步：训练网络的方式就是<strong>输入两张图片</strong>，然后你的网络做的就是将输入的两张图<strong>比较</strong>一下，判断是否是同一个人</p>
<ul>
<li>例如一一比对</li>
</ul>
</blockquote>
</li>
<li>
<p><strong>两个学习算法</strong>分别解决两个更简单的任务，并在整体上得到更好的表现。</p>
</li>
</ul>
<p>为什么两步法更好呢？</p>
<ul>
<li>
<p>你解决的两个问题，每个问题实际上要简单得多</p>
</li>
<li>
<p>训练的<strong>子数据多</strong></p>
<blockquote>
<p>对于第一步：是观察一张图，找出人脸所在的位置，把 人脸图像框出来，所以有很多标签数据 (x,y)</p>
<ul>
<li>x 是图片，y 是人脸的位置</li>
</ul>
<p>对于第二步：人脸的图片更不用说，大头照海量</p>
</blockquote>
</li>
</ul>
<p>相比之下，想要一步到位：这样的 (x,y) 的数据就少得多</p>
<ul>
<li>x ：拍摄的图像，y：该人的身份</li>
<li>因为你<strong>没有足够多的数据</strong>去解决这个端到端学习问 题，但你却有足够多的数据来解决子问题 1 和子问题 2 。</li>
</ul>
<p>实际上，把这个分成两个子问题，比纯粹的端到端深度学习方法，达到<strong>更好的表现</strong>。</p>
<p>不过如果你有<strong>足够多的数据</strong>来做端到端学习，也许端到端方法效果<strong>更好</strong>。</p>
<ul>
<li>但在今天的实践中，并不是最好的方法。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501230011364.png?lastModify=1726287112" alt="image-20240501230011364" /></p>
<p>对于机器翻译系统，也有复杂的流水线</p>
<blockquote>
<p>例如：英语 -&gt; 文本分析 -&gt; ...... -&gt; 法语</p>
</blockquote>
<p>但是对于机器翻译的确有很多数据对：(x,y)</p>
<ul>
<li>x：英文句子，y：对应的法语翻译</li>
</ul>
<p>所以端到端深度学习在机器翻译领域非常好用</p>
<ul>
<li>因为在今天可以收集 (x,y) 对的大数据集</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240501230235200.png?lastModify=1726287112" alt="image-20240501230235200" /></p>
<p>根据 x 射线图判断年龄</p>
<p>采用<strong>非端到端的方法</strong></p>
<ul>
<li>第一步：分辨出每一段的骨头应该在哪里</li>
<li>第二步：根据骨骼的长度判断年龄</li>
</ul>
<p>相比之下，如果你<strong>直接</strong>从图像去判断孩子的年龄，那么你需要大量的数据去直接训练。</p>
<ul>
<li>但没有足够的数据来用端到端的方式来训练这个 任务。</li>
</ul>
<h2 id="是否要使用端到端的深度学习whether-to-use-end-to-end-learning"><a class="anchor" href="#是否要使用端到端的深度学习whether-to-use-end-to-end-learning">#</a> 是否要使用端到端的深度学习？（Whether to use end to-end learning?）</h2>
<p>可以根据一些准则，判断你的应用程序是否有希望使用端到端方法。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240502212146666.png?lastModify=1726287112" alt="image-20240502212146666" /></p>
<p><strong>Pros</strong>：</p>
<ul>
<li>
<p>让数据说话</p>
<blockquote>
<p>如果有足够多的 (x,y) 数据，那么不管从 x 到 y 最适合的函数映射是什么</p>
<p>如果训练一个足够大的神经网络，希望这个神经网络能自己搞清楚。</p>
<p>直接从 x 到 y 输入去训练的神经网络，可能更能够捕获数据中的任何统计信息，而不是被迫引入人类的成见。</p>
<ul>
<li>
<p>例如：不要强迫你的学习算法以音位（<strong>phonemes</strong>）为单位思考</p>
<p>要让算法学习它想学习的<strong>任意</strong>表示方式。</p>
<p>整体表现可能会更好。</p>
</li>
</ul>
</blockquote>
</li>
<li>
<p>所需手工设计的组件更少</p>
<blockquote>
<p>这也许能够简化设计工作流程，不需要花太多时间去手工设计功能，</p>
</blockquote>
</li>
</ul>
<p><strong>Cons</strong>：</p>
<ul>
<li>
<p>需要大量的数据：要直接学到这个 x 到 y 的映射，你可能需要大量 (x,y) 数据。</p>
<blockquote>
<p>直接学习从系统的一端到系统的另一端。</p>
</blockquote>
</li>
<li>
<p>排除了可能有用的手工设计组件</p>
<blockquote>
<p>若你没有很多数据，学习算法就没办法从很小的训练集数据中获得<strong>洞察力</strong></p>
<ul>
<li>所以手工设计组件在这种情况，可能是把人类知识直接注入算法的途径</li>
</ul>
</blockquote>
<p>人工组件也有可能真的<strong>伤害</strong>到你的算法表现</p>
<blockquote>
<p>强制算法以<strong>音位为单位</strong>思考。</p>
</blockquote>
<p>所以是一把双刃剑</p>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240502214807924.png?lastModify=1726287112" alt="image-20240502214807924" /></p>
<p>若想要从 x 到 y 的数据学习一个函数</p>
<ul>
<li>
<p>例如从图像中识别出所有骨头的位置（相对简单）</p>
<blockquote>
<p>那么系统不需要那么多数据来学会处理这个任务</p>
</blockquote>
</li>
<li>
<p>例如从图像中识别出人脸，也可以不需要太多数据或有足够多的数据去解决</p>
</li>
<li>
<p>相对来说，若从 x 射线图直接映射到 <strong>age</strong>，那么更为复杂</p>
<blockquote>
<p>若用纯端到端方法，需要很多数据去学习。</p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240502215226878.png?lastModify=1726287112" alt="image-20240502215226878" /></p>
<blockquote>
<p>直接输入图像，直接得出方向盘转角，就目前能收集到的数据，还有现在训练神经网络的能力是有局限的。</p>
<p>一般用多个步骤进行</p>
<ul>
<li>例如深度学习检测车辆，行人等，然后...</li>
</ul>
</blockquote>
<h1 id="卷积神经网络convolutional-neural-networks"><a class="anchor" href="#卷积神经网络convolutional-neural-networks">#</a> 卷积神经网络（Convolutional  Neural Networks）</h1>
<h2 id="计算机视觉computer-vision"><a class="anchor" href="#计算机视觉computer-vision">#</a> 计算机视觉（Computer vision）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240502232028555.png?lastModify=1726287112" alt="image-20240502232028555" /></p>
<blockquote>
<ol>
<li>让计算机识别出这只猫</li>
<li>目标检测：首先需要计算出图中有<strong>哪些物体</strong>，比如汽车，还 有图片中的其他东西，再将它们模拟成<strong>一个个盒子</strong>，或用一些其他的技术识别出它们在图片中的<strong>位置</strong>。</li>
<li>创造出了新的艺术风格</li>
</ol>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240502232510817.png?lastModify=1726287112" alt="image-20240502232510817" /></p>
<p>相对于 64×64×3=12288，1000×1000 图片的特征向量 x 的维度达到 3 million</p>
<p>也就意味着：，W [1]:(1000,3m)，x∈R3m，W [1] 会有 30 亿个参数</p>
<ul>
<li>
<p>在参数如此大量的情况下，难以获得足够的数据来防止神经网络发生<strong>过拟合和竞争需求</strong>，要处理包含 30 亿参数的神经网络，巨大的内存需求让人不太能接受。</p>
<blockquote>
<p>过多的参数可能会导致模型变得过于灵活。这意味着模型可以以极高的精度来记忆训练数据中的每一个<strong>细节和特例</strong>，而不是真正理解数据中的潜在模式和规律。</p>
<p>模型可能会记住训练集中的噪声、异常值或数据集中的<strong>不重要特征</strong>（过拟合），而不是学习到数据背后的通用模式（<strong>低层次特征</strong>）。</p>
<p>模型的泛化能力可能会受到影响</p>
<ul>
<li>
<p>过度依赖于训练数据中的特定细节而不是真正理解数据的本质规律</p>
<blockquote>
<p>只记住了具体的例子，一旦遇到变化就<strong>束手无策</strong>了。</p>
</blockquote>
</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="边缘检测示例edge-detection-example"><a class="anchor" href="#边缘检测示例edge-detection-example">#</a> 边缘检测示例（Edge detection example）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503123338343.png?lastModify=1726287112" alt="image-20240503123338343" /></p>
<blockquote>
<p>神经网络的前几层是如何检测<strong>边缘</strong>的，然后后面的层有可能检测到物体的<strong>部分</strong>区域，更靠后的一些层可能检测到<strong>完整</strong>的物体</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503123431196.png?lastModify=1726287112" alt="image-20240503123431196" /></p>
<p><strong>垂直边缘检测</strong></p>
<ul>
<li>行人的轮廓线某种程度上也是垂线</li>
<li>栏杆就对应垂直线</li>
</ul>
<p><strong>水平边缘检测</strong></p>
<ul>
<li>栏杆就是很明显的水平线</li>
</ul>
<p>那如何在图像中检测这些边缘呢？</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503124129850.png?lastModify=1726287112" alt="image-20240503124129850" /></p>
<blockquote>
<p>6×6×1 的矩阵是灰度图像，没有 RGB 三通道。</p>
<p>过滤器（核）：为了检测图像中的垂直边缘，3×3</p>
<p>∗：卷积运算</p>
<p>结果为 4×4 的矩阵</p>
</blockquote>
<p>将 3×3 的矩阵作为滑动窗口</p>
<ul>
<li>第一行第一列的计算如上所示</li>
</ul>
<blockquote>
<p>第一行的计算：从左向右滑动</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503125828437.png?lastModify=1726287112" alt="image-20240503125828437" /></p>
</blockquote>
<p>第二行：首先向下滑动一格，然后同第一行的计算</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503125929480.png?lastModify=1726287112" alt="image-20240503125929480" /></p>
</blockquote>
</blockquote>
<ul>
<li>实现卷积运算：不同的编程语言有不同的函数</li>
</ul>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/Convolution_schematic.gif?lastModify=1726287112" alt="Convolution_schematic" /></p>
</blockquote>
<p>为什么这个可以做垂直边缘检测呢？</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503130615065.png?lastModify=1726287112" alt="image-20240503130615065" /></p>
<blockquote>
<p>对于最左边的 6×6 的图像：可以想象成一个图片</p>
<ul>
<li>左边白色，右边黑色（数字的差值大）</li>
</ul>
</blockquote>
<p>可以通过卷积运算得到 4×4 的矩阵。</p>
<p>可以发现检测到的边缘太粗了</p>
<ul>
<li>因为此处的图像太小了，若使用 1000×1000 的图像，就会会很好地检测出图像中的<strong>垂直边缘</strong></li>
<li>在此例子中在输出图像中间的亮处，表 示在图像中间有一个特别<strong>明显的垂直边缘</strong></li>
</ul>
<p><strong>启发</strong></p>
<ol>
<li>因为使用的是 3×3 的过滤器，所以垂直边缘是一个 3×3 的区域。</li>
<li>左边明亮的像素，中间不需要考虑，右边深色的像素</li>
<li>在 6×6 的图像中间部分，明亮的像素在左边，深色的像素在右边，就被视为一个垂直边缘</li>
<li>卷积运算提供了一个方便的方法来发现图像中的垂直边缘。</li>
</ol>
<h2 id="更多边缘检测内容more-edge-detection"><a class="anchor" href="#更多边缘检测内容more-edge-detection">#</a> 更多边缘检测内容（More edge detection）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503131456203.png?lastModify=1726287112" alt="image-20240503131456203" /></p>
<p>如下图所示，它的颜色被<strong>翻转</strong>了：左边暗，右边亮</p>
<p>若使用相同的滤波器进行卷积，那么中间则为 −30</p>
<ul>
<li>如果将矩阵转换为图片，那么中间的<strong>过渡部分被翻转</strong>了，也就是<strong>由暗向亮</strong>过渡。</li>
</ul>
<p>可以取出矩阵的绝对值，这个特定的过滤器确实可以<strong>区分</strong>这两种明暗变化的区别。</p>
<p>对于<strong>水平边缘检测</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503132235976.png?lastModify=1726287112" alt="image-20240503132235976" /></p>
<blockquote>
<p>对于绿色所在部分的 30 区域</p>
<ul>
<li>上边比较亮，下边比较暗，所以为正值</li>
</ul>
<p>对于紫色所在部分的 −30 区域</p>
<ul>
<li>底部比较亮，上部比较暗</li>
</ul>
</blockquote>
<p>对于 10 的区域的过渡带，为什么会出现呢？</p>
<blockquote>
<p>区域左边<strong>两列</strong>是正边，右边<strong>一列</strong>是负边，正边和负边的值加在一起得到了一个<strong>中间值</strong> 10</p>
</blockquote>
<ul>
<li>
<p>因为图片比较小，仅有 6×6。</p>
<p>若图片尺寸很大，这些中间值就会变得非常小。</p>
<blockquote>
<p>像素点变多时，图像的分辨率增加了，因此图像中的细节更加丰富，过渡也更为平滑</p>
</blockquote>
</li>
</ul>
<p>总而言之，通过使用不同的过滤器，可以找出<strong>垂直的或是水平的边缘。</strong></p>
<p>也可以使用 <strong>Sobel filter、Scharr filter</strong></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503133608660.png?lastModify=1726287112" alt="image-20240503133608660" /></p>
</blockquote>
<p>你真正想去检测出复杂图像的边缘也不一定要使用上述数字</p>
<ul>
<li>把这矩阵中的 9 个数字当成 9 个<strong>参数</strong>，并且在之后可以学习使用反向传播算法，其目标就是去<strong>理解</strong>这 9 个参数。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503133812836.png?lastModify=1726287112" alt="image-20240503133812836" /></p>
<p>通过<strong>反向传播</strong>算法能够让神经网络学习任何它所需要的 3×3 的过滤器，并在整幅图片上去应用它</p>
<blockquote>
<p>例如：垂直的边缘，水平的边缘，还有其他角度的边缘</p>
</blockquote>
<h2 id="padding"><a class="anchor" href="#padding">#</a> Padding</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503134411371.png?lastModify=1726287112" alt="image-20240503134411371" /></p>
<blockquote>
<p>每一次的卷积操作，输出的维度是 (n−f+1)×(n−f+1)</p>
</blockquote>
<p>两个缺点</p>
<ol>
<li>
<p>每次做卷积操作，你的图像就会缩小</p>
<p>做了几次卷积操作，图像可能会缩小到 1×1 的大小。</p>
<p>可不想让的图像在每次<strong>识别边缘或其他特征</strong>时都<strong>缩小</strong>，丢失了图像中的一些细节信息</p>
</li>
<li>
<p>角落边缘的像素，例如绿色阴影标记只被一个<strong>输出所触碰或者使用</strong></p>
<p>而中间的像素点（例如<strong>红色</strong>），就会有许多 3×3 的区域与之<strong>重叠</strong></p>
<p>所以那些在角落或者边缘区域的像素点在输出中<strong>采用较少</strong>，意味着丢掉了图像边缘位置的<strong>许多信息</strong>。</p>
</li>
</ol>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503135825105.png?lastModify=1726287112" alt="image-20240503135825105" /></p>
<p>可以通过填充来解决这 2 缺点：6×6→8×8 的图像</p>
<ul>
<li>输出后的图像大小和原始大小一致</li>
<li>而且丢失信息或者更准确来说角落或图像边缘的信息发挥的作用较小的这一缺点就被<strong>削弱</strong>了。</li>
<li>(n+2p−f+1)×(n+2p−f+1)</li>
</ul>
<p>也可以周围填充更多像素。</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/PAD.png?lastModify=1726287112" alt="PAD" /></p>
</blockquote>
<p><strong>Valid and Same convolutions</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503140520712.png?lastModify=1726287112" alt="image-20240503140520712" /></p>
<p>滤波器一般选用<strong>奇数</strong></p>
<ul>
<li>
<p>原因一：为了使得输出和输入大小相等，p=f−12</p>
<p>只有 f 是奇数的情况下，<strong>Same</strong> 卷积才会有<strong>自然的填充</strong>（否则会不对称的填充）</p>
</li>
<li>
<p>原因而：奇数维过滤器会有一个中心点，便于指出过滤器的位置。</p>
</li>
</ul>
<h2 id="卷积步长strided-convolutions"><a class="anchor" href="#卷积步长strided-convolutions">#</a> 卷积步长（Strided convolutions）</h2>
<p>如下位卷积步长为 2</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503141355196.png?lastModify=1726287112" alt="image-20240503141355196" /></p>
<p>若 <strong>padding</strong> 为 p，<strong>stride</strong> 为 s</p>
<ul>
<li>
<p>那么输出的矩阵为：⌊n+2p−fs+1⌋×⌊n+2p−fs+1⌋</p>
<blockquote>
<p>向下取整防止出现小数</p>
<p>只在蓝框<strong>完全包括</strong>在图像或填充完的图像内部时， 才对它进行运算。如果有任意一个蓝框移动到了外面，那你就不要进行相乘操作，这是一个惯例。</p>
<ul>
<li><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503142013099.png?lastModify=1726287112" alt="image-20240503142013099" /></li>
</ul>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503142650487.png?lastModify=1726287112" alt="image-20240503142650487" /></p>
<p>之前的没进行镜像翻转过的过滤器称作：互相关操作</p>
<p>而经过翻转过的过滤器称作：卷积操作</p>
<blockquote>
<p>先顺时针 90°，然后水平翻转得到结果</p>
</blockquote>
<p>但是在深度学习中，惯例将不进行翻转操作叫做卷积操作。</p>
<ul>
<li>
<p>因为卷积核（格子都是<strong>参数</strong>）通常是<strong>随机初始化</strong>的，而且在训练过程中会自动<strong>学习到合适的参数</strong>。</p>
<blockquote>
<p>因此，通常情况下，不会对卷积核进行翻转操作。</p>
</blockquote>
</li>
</ul>
<h2 id="三维卷积convolutions-over-volumes"><a class="anchor" href="#三维卷积convolutions-over-volumes">#</a> 三维卷积（Convolutions over volumes）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503150241743.png?lastModify=1726287112" alt="image-20240503150241743" /></p>
<blockquote>
<p>图像的通道数必须和过滤 器的通道数匹配：紫色部分</p>
</blockquote>
<p>3×3×3 的滤波器有 27 个数，每个数字一一与图像对应相乘，最后相加得到输出</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503150427282.png?lastModify=1726287112" alt="image-20240503150427282" /></p>
</blockquote>
<p>若想检测图像<strong>红色通道的边缘</strong></p>
<p>可以将三层滤波器分别设置为：</p>
<p>R=[10−110−110−1],G=[000000000],B=[000000000]</p>
<ul>
<li>那么这就是一个检测<strong>垂直</strong>边界的过滤器，但只对<strong>红色</strong>通道有用。</li>
</ul>
<p>或者不关心垂直边界在哪个颜色通道里，那么你可以用一个这样的过滤器：</p>
<p>R=[10−110−110−1],G=[10−110−110−1],B=[10−110−110−1]</p>
<p><strong>惯例</strong>：当你的输入有特定的高宽和通道数时，过滤器可以有<strong>不同</strong>的高，<strong>不同</strong>的宽，但是必须<strong>一样的通道数。</strong></p>
<blockquote>
<p>file:///D:/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deeplearning.ai-andrew-ng/COURSE%204%20Convolutional%20Neural%20Networks/Week%2001/images/conv_kiank.mp4</p>
</blockquote>
<p>若同时检测垂直边缘和水平边缘，还有 45° 倾斜的边缘，还有 70° 倾斜的边缘怎么做？</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503151754910.png?lastModify=1726287112" alt="image-20240503151754910" /></p>
<blockquote>
<p>第一个可能是垂直边缘检测器</p>
<p>第二个可能是水平边缘检测器</p>
</blockquote>
<p><strong>卷积核通道数等于输入通道数，卷积核个数等于输出通道数</strong></p>
<p><strong>维度：</strong></p>
<p>n×n×nc ∗ f×f×nc → (n−f+1)×(n−f+1)×nc′</p>
<p>现在可以用它的一小部分直接在三个通道的 <strong>RGB</strong> 图像上进行操作。</p>
<p><strong>更重要的是</strong>：可以检测两个特征（比如垂直和水平边缘）或者 10 个 或者 128 个或者几百个<strong>不同的特征</strong>，并且输出的通道数会等于你要检测的特征数。</p>
<h2 id="单层卷积网络one-layer-of-a-convolutional-network"><a class="anchor" href="#单层卷积网络one-layer-of-a-convolutional-network">#</a> 单层卷积网络（One layer of a convolutional network）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503155257222.png?lastModify=1726287112" alt="image-20240503155257222" /></p>
<blockquote>
<p>卷积操作（<strong>线性</strong>）的输出矩阵，加上偏差 b，然后再应用非线性激活函数，例如（<strong>Relu</strong>），最终得到另一个 4×4 的矩阵</p>
</blockquote>
<p>上述图像操纵，它是卷积神经网络的<strong>一层</strong>。</p>
<p>对于 z [1]=W [1] a [0]+b [1],a [1]=g (z [1])</p>
<ul>
<li>a[0]：<img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503160419201.png?lastModify=1726287112" alt="image-20240503160419201" /> W[1]：<img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503160439809.png?lastModify=1726287112" alt="image-20240503160439809" /></li>
<li>W[1]a[0]：<img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503160727924.png?lastModify=1726287112" alt="image-20240503160727924" /> z[1]：<img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503160653550.png?lastModify=1726287112" alt="image-20240503160653550" /></li>
<li>a[1]：<img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503160802020.png?lastModify=1726287112" alt="image-20240503160802020" /></li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503160904589.png?lastModify=1726287112" alt="image-20240503160904589" /></p>
<p>若使用了 10 个滤波器，也就是有 10 个特征，最终会得到 4×4×10 的输出图像</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503161113215.png?lastModify=1726287112" alt="image-20240503161113215" /></p>
<p>上述 10 个滤波器，每个滤波器 28 个参数。一共只有 280 个参数</p>
<p>不论图片有多大，<strong>参数始终是 280 个</strong></p>
<p>用这 10 个过滤器来<strong>提取特征</strong>，如垂直边缘，水平边缘和其它特征。</p>
<p>即使这些图片很大，参数却很少，这就是卷积神经网络的一个特征，叫作 “<strong>避免过拟合</strong>”。</p>
<p>关于符号表示</p>
<table>
<thead>
<tr>
<th>名词解释</th>
<th>名词解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>f[l]：<strong>filter size</strong></td>
<td><strong>Input</strong>：nH[l−1]×nW[l−1]×nc[l−1]</td>
</tr>
<tr>
<td>pl：<strong>padding</strong></td>
<td><strong>Output</strong>：nH[l]×nW[l]×nc[l]</td>
</tr>
<tr>
<td>s[l]：<strong>stride</strong></td>
<td>nH[l]=⌊nH[l−1]+2p[l]−f[l]s[l]+1⌋</td>
</tr>
<tr>
<td>nc[l]：<strong>number of filters</strong></td>
<td>nW[l]=⌊nW[l−1]+2p[l]−f[l]s[l]+1⌋</td>
</tr>
<tr>
<td><strong>Each filter</strong>：f[l]×f[l]×nc[l−1]</td>
<td></td>
</tr>
<tr>
<td><strong>Activations</strong>：a[l]→nH[l]×nW[l]×nc[l]</td>
<td>A [l]→m×nH [l]×nW [l]×nc [l]⏟nc×nH×nW&lt;br /&gt; 有时候顺序也会变化</td>
</tr>
<tr>
<td><strong>Weights</strong>：f[l]×f[l]×nc[l−1]×nc[l]</td>
<td></td>
</tr>
<tr>
<td><strong>bias</strong>：nc[l]:(1,1,1,nc[l])</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="简单卷积网络示例a-simple-convolution-network-example"><a class="anchor" href="#简单卷积网络示例a-simple-convolution-network-example">#</a> 简单卷积网络示例（A simple convolution network  example）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503170545396.png?lastModify=1726287112" alt="image-20240503170545396" /></p>
<p>达到最后一个隐藏层的输出：7×7×40，也就是提取了 1960 个特征。</p>
<blockquote>
<p>假设使用了 40 个滤波器</p>
</blockquote>
<p>然后对该卷积进行处理，可以将其<strong>平滑或展开</strong>成 1960 个单元。</p>
<p>平滑处理后可以输出一个向量，填充内容可以是 <strong>logistic</strong> 还是 <strong>softmax</strong> 回归单元</p>
<blockquote>
<p>完全取决于想识图片上有没有猫，还是想识别 K 种不同对象中的一种</p>
</blockquote>
<p>随着神经网络计算深度不断加深，通常开始时的图像也要更大一些</p>
<ul>
<li><strong>高度和宽度</strong>会在一段时间内保持一致，然后随着网络深度的加深而<strong>逐渐减小</strong></li>
<li><strong>通道数量在增加</strong></li>
</ul>
<p>一个典型的卷积神经网络通常有<strong>三层</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503171234821.png?lastModify=1726287112" alt="image-20240503171234821" /></p>
<h2 id="池化层pooling-layers"><a class="anchor" href="#池化层pooling-layers">#</a> 池化层（Pooling layers）</h2>
<p>卷积网络也经常使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性</p>
<ul>
<li>
<p><strong>保留主要特征</strong>：通常选择<strong>最大值</strong>或平均值作为代表。这样可以保留最显著的特征，有助于提取更加抽象和高级的特征。</p>
</li>
<li>
<p><strong>降低过拟合风险</strong>：有助于减少特征图的维度，从而减少了模型的过拟合风险。</p>
<p>这种降低维度的同时也可以视作一种正则化操作，有助于提高模型的泛化能力。</p>
<blockquote>
<p>类似于 <strong>dropout</strong>，将权重的维度有所降低（规模更小）</p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503172918894.png?lastModify=1726287112" alt="image-20240503172918894" /></p>
<p>而对于上述 4×4 的矩阵，那么用到<strong>最大池化</strong></p>
<ul>
<li>f=2：过滤器的规模为 2</li>
<li>s=2：步长为 2</li>
</ul>
<blockquote>
<p>所以是 2×2</p>
</blockquote>
<p>对最大池化功能的直观理解，可以把这个 4×4 区域看作是<strong>某些特征的集合</strong>，也就是神经网络中某一层的<strong>非激活值（z）集合</strong>。</p>
<p>数字大：可能探测到了某些特定的特征</p>
<blockquote>
<p>例如：左上象限具有的特征可能是一个垂直边缘，一只眼睛等</p>
</blockquote>
<p>所以最大化运算的实际作用就是，如果在过滤器中提取到某个特征，那么<strong>保留其最大值</strong>。</p>
<ul>
<li>提取主要特征。</li>
</ul>
<blockquote>
<p>若右上象限中不存在这个特征，那么其中的最大值也还是很<strong>小</strong></p>
</blockquote>
<p>它有一组超参数，但<strong>并没有参数需要学习</strong>。</p>
<ul>
<li>一旦确定了 f 和 s，它就是一个固定运算，梯度下降无需改变任何 值。</li>
</ul>
<p>卷积层输出大小的公式同样适用于最大池化：⌊n+2p−fs+1⌋</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503174534015.png?lastModify=1726287112" alt="image-20240503174534015" /></p>
<p>对于 nc 个通道，每个通道都单独执行最大池化计算</p>
<ul>
<li>5×5×nc→3×3×nc</li>
</ul>
<p>对于平均池</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503181401557.png?lastModify=1726287112" alt="image-20240503181401557" /></p>
</blockquote>
<p>目前来说，最大池化比平均池化更常用。</p>
<ul>
<li>但也有例外，就是深度很深的神经网络，可以用<strong>平均池化</strong>来分解规模为 7×7×1000 的网络的表示层，在整个空间内求平均值，得到 1×1×1000</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503181946047.png?lastModify=1726287112" alt="image-20240503181946047" /></p>
<p>池化的超级参数包括过滤器大小 f 和步长 s</p>
<ul>
<li>
<p>常用参数值为 f=2,s=2，<strong>应用频率非常高</strong></p>
<p>其效果相当于高度和宽度<strong>缩减一半</strong></p>
</li>
<li>
<p>也有使用 f=2,s=2 的情况。</p>
<p>至于其它超级参数就要看用的是最大池化还是平均池化了</p>
</li>
</ul>
<p>对于 <strong>padding</strong>，很少用（例外）</p>
<h2 id="卷积神经网络示例convolutional-neural-network-example"><a class="anchor" href="#卷积神经网络示例convolutional-neural-network-example">#</a> 卷积神经网络示例（Convolutional neural network  example）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240503223250001.png?lastModify=1726287112" alt="image-20240503223250001" /></p>
<p>通常在计算神经网络有多少层时，通常<strong>只统计</strong>具有权重和参数的层。</p>
<ul>
<li>因为池化层 <strong>Pooling</strong> 没有权重和参数，只有一些超参数。</li>
</ul>
<p>对于  <code>Pool 2</code>  ：5×5×16，包含 400 个元素</p>
<p>将  <code>Pool 2</code>  平整化为一个大小为 400 的<strong>一维向量</strong>。</p>
<ul>
<li>作为下一层输入的特征</li>
</ul>
<p>下一层含有 120 个单元，这就是<strong>第一个全连接层</strong>，标记为  <code>FC3</code> 。</p>
<ul>
<li>这 400 个单元与 120 个单元紧密相连</li>
<li>其权重矩阵为 W [3]:(120,400)</li>
</ul>
<p>这就是所谓的 “<strong>全连接</strong>”</p>
<ul>
<li>因为这 400 个单元与这 120 个单元的每一项<strong>连接</strong></li>
</ul>
<blockquote>
<p>【注意】：看上去它有很多超参数。常规做法是，尽量不要自己设置超参数，而是查看<strong>文献</strong>中别人采用了哪些超参数，选一个在别人任务中效果<strong>很好的架构</strong>，那么它也有可能适用于你自己的应用程序</p>
</blockquote>
<p>随着神经网络深度的加深，高度 nH 和宽度 nW 通常都会减少</p>
<ul>
<li>而通道数量会增加</li>
</ul>
<p><strong>常见模式</strong>就是：一个或多个卷积 → 一个池化层 → 一个或多个卷积 → 一个池化层 ⋯ → 几个全连接层 → <strong>softmax</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"><strong>Activation shape</strong></th>
<th style="text-align:center"><strong>Activation Size</strong></th>
<th style="text-align:center"><strong># parameters</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Input:</strong></td>
<td style="text-align:center">(32,32,3)</td>
<td style="text-align:center">3072</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV1</strong>(f=5,s=1)</td>
<td style="text-align:center">(28,28,8)</td>
<td style="text-align:center">6272</td>
<td style="text-align:center">608</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL1</strong></td>
<td style="text-align:center">(14,14,8)</td>
<td style="text-align:center">1568</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center"><strong>CONV2</strong>(f=5,s=1)</td>
<td style="text-align:center">(10,10,16)</td>
<td style="text-align:center">1600</td>
<td style="text-align:center">3216</td>
</tr>
<tr>
<td style="text-align:center"><strong>POOL2</strong></td>
<td style="text-align:center">(5,5,16)</td>
<td style="text-align:center">400</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC3</strong></td>
<td style="text-align:center">(120,1)</td>
<td style="text-align:center">120</td>
<td style="text-align:center">48,120</td>
</tr>
<tr>
<td style="text-align:center"><strong>FC4</strong></td>
<td style="text-align:center">(84,1)</td>
<td style="text-align:center">84</td>
<td style="text-align:center">10,164</td>
</tr>
<tr>
<td style="text-align:center"><strong>Softmax</strong></td>
<td style="text-align:center">(10,1)</td>
<td style="text-align:center">10</td>
<td style="text-align:center">841</td>
</tr>
</tbody>
</table>
<p>许多计算机视觉研究正在探索如何把这些基本模块整合起来，构建高效的神经网络，整合这些基本模块确实需要<strong>深入的理解</strong></p>
<ul>
<li>找到整合基本构造模块最好方法就是<strong>大量阅读别人的案例</strong></li>
</ul>
<h2 id="为什么使用卷积why-convolutions"><a class="anchor" href="#为什么使用卷积why-convolutions">#</a> 为什么使用卷积？（Why convolutions?）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509130219409.png?lastModify=1726287112" alt="image-20240509130219409" /></p>
<blockquote>
<p>假设有一张 32×32×3 的图片，使用 <strong>Filter</strong> 大小为 5 的 6 个过滤器，结果为 28×28×6</p>
</blockquote>
<ul>
<li>其中一层有 3072 个单元，下一层有 4074 个单元</li>
</ul>
<p>然后进行全连接，<strong>权重矩阵</strong>为：4074×3072≈14W</p>
<ul>
<li>所以要训练的参数很多</li>
<li>若是 1000×1000 的图片，那么权重矩阵会变得非常大</li>
</ul>
<p>上述<strong>卷积层</strong>的<strong>参数数量</strong>：(5×5×3+1)∗6=456</p>
<p>卷积网络映射这么少参数有两个原因：</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509131951554.png?lastModify=1726287112" alt="image-20240509131951554" /></p>
<p><strong>参数共享</strong>（使用相同的<strong>卷积核</strong>）</p>
<ul>
<li>
<p>输入的每一个维度类型都是一样的（例如都是垂直边缘检测）</p>
<blockquote>
<p><strong>不同维度具有相同的特征表示</strong></p>
</blockquote>
<ul>
<li>
<p>相反，若一个维度是身高，另一个维度是体重，那么就不能共用参数了</p>
<blockquote>
<p>要两个不同的过滤器</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>卷积操作在输入数据的<strong>不同位置上共享</strong>相同的权重参数，以便提取垂直边缘或其它特征</p>
</li>
</ul>
<p><strong>稀疏连接</strong>（区别于全连接）</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509133210290.png?lastModify=1726287112" alt="image-20240509133210290" /></p>
</blockquote>
<ul>
<li>
<p>每个输出仅仅依赖于 9 个特征，与其他特征无关</p>
<blockquote>
<p>其他像素对输出没有任何影响</p>
</blockquote>
</li>
</ul>
<p>神经网络可以通过这两种机制减少参数，以便我们用<strong>更小的</strong>训练集来训练它，从而预防过度拟合。</p>
<p>卷积网络善于捕捉<strong>平移不变性</strong></p>
<ul>
<li>
<p>意味着无论对象或特征在图像中的位置如何改变，模型都能够识别它们（更好的提取相似特征）</p>
<blockquote>
<p>假设你在看一张图片，图片上有一只猫，它在图像的左上角。如果你用眼睛移动一下，猫的位置会改变，可能会出现在图像的右下角。然而，对于人类来说，猫仍然是猫，不管它在图像中的哪个位置。这种对对象的位置变化不敏感的能力就是平移不变性的一个示例。</p>
</blockquote>
</li>
<li>
<p>平移不变性是通过<strong>卷积操作和参数共享</strong>来实现的。</p>
<blockquote>
<p>卷积操作允许卷积核在图像上滑动并提取特征</p>
<p>参数共享使得相同的卷积核在不同位置上使用相同的参数。</p>
</blockquote>
<p>这意味着无论对象在图像中的位置如何改变，通过<strong>共享参数</strong>，卷积核都能够识别相同的特征（计算的结果相等）。</p>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509134154197.png?lastModify=1726287112" alt="image-20240509134154197" /></p>
<h2 id="为什么要进行实例探究"><a class="anchor" href="#为什么要进行实例探究">#</a> 为什么要进行实例探究？</h2>
<p>事实上，过去几年计算机视觉研究中的大量研究都集中在如何把这些基本构件组合起来，形成有效的卷积神经网络。</p>
<ul>
<li>
<p>最直观的方式之一就是去看一些案例</p>
<blockquote>
<p>就像很多人通过看别人的代码来学习编程一 样，通过研究别人构建有效组件的案例是个不错的办法。</p>
</blockquote>
</li>
</ul>
<p>实际上在计算机视觉任务中表现 良好的神经网络框架往往也适用于其它任务</p>
<ul>
<li>例如识别猫、狗、人等的神经网络，而你的计算机视觉识别任务是构建一个自动驾驶汽车，完全可以<strong>借鉴</strong>别人的神经网络框架来解决自己 的问题。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509135052840.png?lastModify=1726287112" alt="image-20240509135052840" /></p>
<h2 id="经典网络classic-networks"><a class="anchor" href="#经典网络classic-networks">#</a> 经典网络（Classic networks）</h2>
<h3 id="lenet-5"><a class="anchor" href="#lenet-5">#</a> LeNet-5</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509171627540.png?lastModify=1726287112" alt="image-20240509171627540" /></p>
<p>现在版本使用 <strong>softmax</strong> 函数输出十种分类结果</p>
<ul>
<li>而 <strong>LeNet-5</strong> 网络在输出层使用了另外一种，现在已经很少用到的分类器。</li>
</ul>
<p>过去，人们使用 <strong>sigmoid</strong> 函数和 <strong>tanh</strong> 函数，而不是 <strong>ReLu</strong> 函数，这篇论文中使用的正是 <strong>sigmoid</strong> 函数和 <strong>tanh</strong> 函数。</p>
<p>这种网络结构的特别之处还在于，各网络层之间是<strong>有关联的</strong></p>
<blockquote>
<p>比如：nH×nW×nc 的网络，使用尺寸为 f×f×nc 的过滤器</p>
<p>每个过滤器的通道数和它上一层的通道数相同。</p>
<ul>
<li>这是由于在当时，计算机的运行速度非常慢，为了<strong>减少</strong>计算量和参数，经典的 <strong>LeNet-5</strong> 网络使用了非常复杂的计算方式，每个过滤器都采用和输入模块一样的通道数量。</li>
</ul>
</blockquote>
<h3 id="alexnet"><a class="anchor" href="#alexnet">#</a> AlexNet</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509174715467.png?lastModify=1726287112" alt="image-20240509174715467" /></p>
<p>这种神经网络与 <strong>LeNet</strong> 有很多相似之处，不过 <strong>AlexNet</strong> 要大得多。正如前面的 <strong>LeNet</strong> 或 <strong>LeNet-5</strong> 大约有 6 万个参数，而 <strong>AlexNet</strong> 包含约 6000 万个参数。</p>
<p>当用于训练图像和数据集时，<strong>AlexNet</strong> 能够处理非常<strong>相似的基本构造模块</strong></p>
<ul>
<li>这些模块往往包含着大量的隐藏单元或数据，这一点 <strong>AlexNet</strong> 表现出色。</li>
</ul>
<p><strong>AlexNet</strong> 比 <strong>LeNet</strong> 表现更为出色的另一个 原因是它使用了 <strong>ReLu</strong> 激活函数。</p>
<p>在写这篇论文的时候，<strong>GPU</strong> 的处理速度还比较慢，所以 <strong>AlexNet</strong> 采用了非常复杂的方法在两个 <strong>GPU</strong> 上进行训练。</p>
<blockquote>
<p>大致原理是，这些层分别拆分到两个不同的 <strong>GPU</strong> 上，同 时还专门有一个方法用于两个 <strong>GPU</strong> 进行交流。</p>
</blockquote>
<p>论文还提到，经典的 <strong>AlexNet</strong> 结构还有另一种类型的层，叫作 “局部响应归一化层” （Local Response Normalization），即 <strong>LRN</strong> 层</p>
<blockquote>
<p>基本思路是假设网络的一块是 13×13×256</p>
<p><strong>LRN</strong> 要做的就是选取一个位置，从这个位置<strong>穿过</strong>整个通道，能得到 256 个数字， 并进行归一化。</p>
</blockquote>
<ul>
<li>对于这张 13×13 的图像中的每个位置来说， 我们可能<strong>并不需要</strong>太多的高激活神经元。</li>
</ul>
<blockquote>
<p>但是后来，很多研究者发现 <strong>LRN</strong> 起不到太大作用</p>
</blockquote>
<h3 id="vgg-16"><a class="anchor" href="#vgg-16">#</a> VGG-16</h3>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509182144548.png?lastModify=1726287112" alt="image-20240509182144548" /></p>
<blockquote>
<p><strong>same</strong> 为 <strong>same padding</strong></p>
<p>过滤器是二维的</p>
</blockquote>
<p><strong>VGG-16</strong>：网络中包含 13 个卷积层和 3 个全连接层</p>
<p>此网络结构的简单原则</p>
<ul>
<li>过滤器的数量翻倍（卷积层的过滤器数量变化<strong>存在一定的规律</strong>）</li>
<li>池化层缩小一半图像的高度和宽度</li>
</ul>
<p>建议从介绍 <strong>AlexNet</strong> 的 论文开始，然后就是 <strong>VGG</strong> 的论文，最后是 <strong>LeNet</strong> 的论文。</p>
<h2 id="残差网络"><a class="anchor" href="#残差网络">#</a> 残差网络</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509184358968.png?lastModify=1726287112" alt="image-20240509184358968" /></p>
<p>a [l] 拷贝到神经网络的深层，在 <strong>relu</strong> 非线性激活函数前加上 a [l]</p>
<ul>
<li>是一条<strong>捷径</strong>（<strong>跳跃连接</strong>）</li>
</ul>
<p>也就是 a [l+2]=g (z [l+2])→a [l+2]=g (z [l+2]+a [l])</p>
<ul>
<li>加上的 a [l] 产生了一个残差块</li>
</ul>
<p>a [l] 插入的试机是在线性激活之后，<strong>relu</strong> 激活之前</p>
<p>所以构建一个 <strong>ResNet</strong> 网络就是通过将很多这样的残差块<strong>堆积</strong>在一起，形成一个很深神经网络</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509185111308.png?lastModify=1726287112" alt="image-20240509185111308" /></p>
<p>把普通网络（Plain network）变为残差网络：</p>
<p>加上所有跳跃连接，<strong>每两层增加一个捷径，构成一个残差块</strong>。</p>
<ul>
<li>上述 5 个残差块连接在一起构成一个残差网络。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240509201735447.png?lastModify=1726287112" alt="image-20240509201735447" /></p>
<ul>
<li>实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练，训练错误会越来越多</li>
</ul>
<h2 id="残差网络为什么有用why-resnets-work"><a class="anchor" href="#残差网络为什么有用why-resnets-work">#</a> 残差网络为什么有用？（Why ResNets work?）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240510124758529.png?lastModify=1726287112" alt="image-20240510124758529" /></p>
<p>a[l+2]=g(z[l+2]+a[l])=g(W[l+2]a[l+1]+b[l+2]+a[l])</p>
<p>若 W [l+2]=0,b [l+2]=0</p>
<ul>
<li>
<p>则 a [l+2]=g (W [l+2] a [l+1]+b [l+2]+a [l])=g (a [l])=a [l]</p>
<blockquote>
<p>使用 <strong>relu</strong></p>
</blockquote>
</li>
<li>
<p>残差块学习这个恒等式函数并不难，跳跃连接使我们很容易得出 a [l+2]=a [l]</p>
</li>
</ul>
<blockquote>
<p>在传统的深层神经网络中如果 W [l+2]=0，那么在进行反向传播时，梯度会在该层被乘以零，从而导致梯度的消失。</p>
<p>但在残差网络中，即使 W [l+2]=0，由于跳跃连接的存在，输入 a [l] 会直接传递到输出 a [l+2] 中（a [l+2]≠0），因此梯度仍然可以通过跳跃连接直接传播到较浅层的网络层，避免了梯度消失的问题。</p>
<p>∂f(g(x))∂x=∂f(g(x))∂g(x)⋅∂g(x)∂x</p>
<p>∂[f(g(x))+g(x)]∂x=∂f(g(x))∂g(x)⋅∂g(x)∂x+∂g(x)∂x</p>
</blockquote>
<p>可以发现 z [l+2] 与 a [l] 具有相等的维度，因为 <strong>ResNets</strong> 使用了许多 <strong>same</strong> 卷积。</p>
<blockquote>
<p><strong>same</strong> 卷积：卷积操作中输入和输出的维度大小相同的情况</p>
</blockquote>
<p>若  z [l+2] 与 a [l] 具有<strong>不同的维度</strong>：</p>
<p>a[l+2]=g(z[l+2]+Wsa[l])</p>
<ul>
<li>不需要对 Ws 做任何操作。它是网络通过学习得到的矩阵或参 数，它是一个固定矩阵。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240511181105494.png?lastModify=1726287112" alt="image-20240511181105494" /></p>
<h2 id="网络中的网络以及-11-卷积network-in-network-and-11-convolutions"><a class="anchor" href="#网络中的网络以及-11-卷积network-in-network-and-11-convolutions">#</a> 网络中的网络以及 1×1 卷积（Network in Network and  1×1 convolutions）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240511182943154.png?lastModify=1726287112" alt="image-20240511182943154" /></p>
<p>1×1 卷积可以从根本上理解为对这 32 个不同的位置都应用一个全连接层（每一个通道都应用一个全连接层）</p>
<p>在输入层实施一个<strong>非平凡计算</strong></p>
<blockquote>
<p>非平凡计算是指能够从输入数据中提取出有用信息、进行深层次的特征提取和转换的计算过程，而不仅仅是一些简单的线性操作或基本的数据处理</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240511183421023.png?lastModify=1726287112" alt="image-20240511183421023" /></p>
<p>对于通道数量非常大，可以使用 1×1 卷积进行压缩通道</p>
<blockquote>
<p>上述使用 32 个 1×1×192 的过滤器来压缩通道</p>
</blockquote>
<p>它给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变</p>
<h2 id="谷歌-inception-网络简介inception-network-motivation"><a class="anchor" href="#谷歌-inception-网络简介inception-network-motivation">#</a> 谷歌 Inception 网络简介（Inception network motivation）</h2>
<p>构建卷积层时，你要决定过滤器的大小究竟是 1×1，3×3 还是 5×5，或者要不要添加池化层。而 <strong>Inception</strong> 网络的作用就是<strong>代替</strong>你来决定，虽然网络架构因此变得更加复杂，但网络表现却非常好</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240511220417381.png?lastModify=1726287112" alt="image-20240511220417381" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240511221552463.png?lastModify=1726287112" alt="image-20240511221552463" /></p>
<p>一共约 1.2 亿 次乘法运算：</p>
<ul>
<li>
<p>一个过滤器为 5×5×192 要对应的通道格子相乘相乘，一共 32 个过滤器也就是</p>
<p>5×5×192×28×28×32</p>
</li>
</ul>
<p>即使在现在，用计算机执行 1.2 亿次乘法运算，成本也是相当高的。</p>
<p>为了降低计算成本，用计算成本除以因子 10，结果它从 1.2 亿减小到原来的十分之一。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240511222457261.png?lastModify=1726287112" alt="image-20240511222457261" /></p>
<blockquote>
<p>类似于瓶子：线索小再放大</p>
<p>其中中间的层次为<strong>瓶颈层</strong></p>
</blockquote>
<p>16 个过滤器的存在：为了在压缩的同时尽可能<strong>提取原始信息</strong></p>
<p>事实证明，只要<strong>合理</strong>构建瓶颈层，既可以显著缩小表示层规模，又不会降低网络性能，从而节省了计算。</p>
<h2 id="inception-网络inception-network"><a class="anchor" href="#inception-网络inception-network">#</a> Inception 网络（Inception network）</h2>
<p><strong>Inception</strong> 模块会将之前层的激活或者输出作为它的输入</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512133358681.png?lastModify=1726287112" alt="image-20240512133358681" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512133935197.png?lastModify=1726287112" alt="image-20240512133935197" /></p>
<blockquote>
<p>由每一个 <strong>Inception</strong> 模块组成的</p>
</blockquote>
<p>对于上述的分支，应该把它看做 <strong>Inception</strong> 网络的一个细节，它确保了即便是隐藏单元和中间层也参与了特征计算，它们也能预测图片的分类。</p>
<ul>
<li>它在 <strong>Inception</strong> 网络中，起到一种调整的效果，并且能防止网络发生过拟合。</li>
</ul>
<h2 id="迁移学习transfer-learning-2"><a class="anchor" href="#迁移学习transfer-learning-2">#</a> 迁移学习（Transfer Learning）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512151522060.png?lastModify=1726287112" alt="image-20240512151522060" /></p>
<blockquote>
<p>若训练集<strong>很小</strong></p>
</blockquote>
<ul>
<li>从网上下载一些神经网络开源的实现，不仅把代码下载下来，也把<strong>权重</strong>下载下来。</li>
<li>去掉原本的 <strong>Softmax</strong> 层，创建你自己的 <strong>Softmax</strong> 单元，用来输出 Tigger、Misty 和 neither 三个类别。</li>
</ul>
<p>通过使用其他人预训练的权重，很可能得到很好的性能。</p>
<p>在上述中，只需要训练 <strong>softmax</strong> 层的权重</p>
<ul>
<li>把前面的权重都<strong>冻结</strong></li>
</ul>
<p>由于前面的层都冻结了，相当于一个固定的函数， 不需要改变（不需要改变它，也不训练它）</p>
<p><strong>特征缓存</strong></p>
<ol>
<li>使用原始样本数据输入深度神经网络（上述紫色部分）</li>
<li>将输出的特征向量（激活值）存储在硬盘上面</li>
<li>使用特征向量来进行训练一个很浅的 <strong>softmax</strong> 模型（后部分）</li>
</ol>
<p>优点：不需要每次遍历训练集再重新计算这个激活值了。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512152440842.png?lastModify=1726287112" alt="image-20240512152440842" /></p>
<blockquote>
<p>若含有大量的数据</p>
</blockquote>
<p>应该冻结<strong>更少</strong>的层，然后训练后面的层。</p>
<p>如果输出层的类别不同，那么需要<strong>构建</strong>自己的输出单元</p>
<ul>
<li>可以取后面几层的权重，用作初始化，然后从这里开始梯度下降。</li>
<li>或者可以直接去掉这几层，换成自己的隐藏单元和自己的 <strong>softmax</strong> 输出层</li>
</ul>
<p><strong>如果有越来越多的数据，需要冻结的层数越少， 能够训练的层数就越多</strong>。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512152855947.png?lastModify=1726287112" alt="image-20240512152855947" /></p>
<p>如果有大量数据，应该做的就是用开源的网络和它的权重，把所有的<strong>权重当作初始化</strong>，然后训练整个网络。</p>
<h2 id="数据增强data-augmentation"><a class="anchor" href="#数据增强data-augmentation">#</a> 数据增强（Data augmentation）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512154113743.png?lastModify=1726287112" alt="image-20240512154113743" /></p>
<p>给 <strong>R</strong>、<strong>G</strong> 和 <strong>B</strong> 三个通道上加上不同的失真值。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512154339910.png?lastModify=1726287112" alt="image-20240512154339910" /></p>
<p>对 R、 G 和 B 有不同的采样方式，其中一种影响颜色失真的算法是 <strong>PCA</strong>，即主成分分析</p>
<p><strong>PCA</strong> 颜色增强的大概含义：比如说， 如果图片呈现紫色，即主要含有<strong>红色和蓝色</strong>，绿色很<strong>少</strong>，然后 <strong>PCA</strong> 颜色增强算法就会对红色和蓝色<strong>增减</strong>很多，绿色<strong>变化相对少</strong>一点，所以使总体的颜色保持一致。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512155156271.png?lastModify=1726287112" alt="image-20240512155156271" /></p>
<p>常用的实现数据增强的方法：使用<strong>一个线程或者是多线程</strong>，这些可以用来加载数据， 实现变形失真，然后传给其他的线程或者其他进程</p>
<blockquote>
<p>可以并行实现</p>
</blockquote>
<p>一个好的开始可能是使用别人的开源实现，了解他们如何实现数据增强</p>
<h2 id="计算机视觉现状the-state-of-computer-vision"><a class="anchor" href="#计算机视觉现状the-state-of-computer-vision">#</a> 计算机视觉现状（The state of computer vision）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512160723362.png?lastModify=1726287112" alt="image-20240512160723362" /></p>
<blockquote>
<p>可以认为大部分机器学习问题是介于少量数据和大量数据范围之间的</p>
<p>对象检测的数据更少</p>
<ul>
<li>获取<strong>边框的成本</strong>比标记对象的成本更高</li>
</ul>
</blockquote>
<p>有很多数据时：倾向于使 用更简单的算法和更少的手工工程</p>
<blockquote>
<p>当有大量的数据时，只要有一个大型的神经网络，甚至一个更简单的架构，可以是一个神经网络， 就可以去学习它想学习的东西。</p>
</blockquote>
<p>没有那么多的数据：更多的是手工工程（有很多<strong>小技巧</strong>可用）</p>
<ul>
<li>网络更加的复杂（非常复杂的超参数选择）</li>
<li>迁移学习</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512161859294.png?lastModify=1726287112" alt="image-20240512161859294" /></p>
<p><strong>Benchmark</strong> 基准测试：是一个评价方式</p>
<blockquote>
<p>As computer architecture advanced, it became more  difficult to compare the performance of various computer systems simply by looking at their  specifications. Therefore, tests were developed that allowed <strong>comparison of different  architectures.</strong></p>
</blockquote>
<p>对于上述第一点<strong>集成</strong></p>
<ul>
<li>
<p>意味着要对每张图片进行测试，可能需要在从 3 到 15 个不同的网络中运行一个图像，这是很典型的，因为这 3 到 15 个网络可能会让运行时间变慢，甚至更多时间</p>
<blockquote>
<p>所以技巧之一的集成是人们在基准测试中表现出色和赢得比赛的利器，但这<strong>几乎不用</strong>于生产服务于客户的</p>
<ul>
<li>除非有一个巨大的计算预算而且不介意在每个用户图像数据上花费大量的计算。</li>
</ul>
</blockquote>
</li>
<li>
<p>占用了更多的计算机存。</p>
</li>
</ul>
<p>对 <strong>benchmarks</strong>  测试有帮助的另一个技巧就是 <strong>Multi-crop at test  time</strong></p>
<ul>
<li>裁剪后，通过分类器来运行这十张图片，然后对结果进行<strong>平均</strong>。</li>
<li>不会占用太多的内存，但它仍然会让运行时间变慢。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240512162429075.png?lastModify=1726287112" alt="image-20240512162429075" /></p>
<h2 id="目标定位object-localization"><a class="anchor" href="#目标定位object-localization">#</a> 目标定位（Object localization）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513152054579.png?lastModify=1726287112" alt="image-20240513152054579" /></p>
<blockquote>
<p>图片分类的思路可以帮助学习分类定位，而对象定位的 思路又有助于学习对象检测</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513152448954.png?lastModify=1726287112" alt="image-20240513152448954" /></p>
<p>定位图片中汽车的位置：可以让神经网络多输出几个单元，输出一个边界框</p>
<blockquote>
<p>bx,by,bh,bw</p>
</blockquote>
<p>符号约定：</p>
<ul>
<li>左上角：(0,0)</li>
<li>右下角：(1,1)</li>
<li>中心点：(bx,by)</li>
<li>边界宽，高：bh,bw</li>
</ul>
<p>因此训练集不仅包含神经网络要预测的对象分类标签，还要<strong>包含</strong>表示边界框的这四个数字，接着采用监督学习算法，输出一个分类标签，还有四个参数值，从而给出检测对象的边框位置。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513154022011.png?lastModify=1726287112" alt="image-20240513154022011" /></p>
<p>y=[pcbxbybhbwc1c2c3],y=[1bxbybhbw010],y=[0???????]</p>
<p>L(y<sup>,y)={(y1</sup>−y1)2+(y2<sup>−y2)2+⋯+(y8</sup>−y8)2 if y1=1(y1^−y1)2 if y1=0</p>
<blockquote>
<p>上述为简化的损失函数</p>
</blockquote>
<p>可以对 c1,c2,c3 使用 <strong>softmax</strong> 损失，pc 使用 <strong>sigmoid</strong> 损失，b? 使用 <strong>平方差</strong> 损失。</p>
<h2 id="特征点检测landmark-detection"><a class="anchor" href="#特征点检测landmark-detection">#</a> 特征点检测（Landmark detection）</h2>
<p>神经网络可以通过输出图片上特征点的 (𝑥,𝑦) 坐标来实现对目标特征的识别</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513155825994.png?lastModify=1726287112" alt="image-20240513155825994" /></p>
<blockquote>
<p>眼角坐标为 (l1x,l1y)。</p>
</blockquote>
<p>假设脸部由 64 个特征点，有些点甚至可以帮助你定义脸部轮廓或下颌轮廓。</p>
<ul>
<li>选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部 关键特征点的位置。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513160659384.png?lastModify=1726287112" alt="image-20240513160659384" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513160807579.png?lastModify=1726287112" alt="image-20240513160807579" /></p>
<p>一旦了解如何用二维坐标系定义人物姿态，操作起来就相当简单了，批量添加输出单元，用以输出要识别的各个特征点的 (𝑥,𝑦) 坐标值。</p>
<ul>
<li>特征点的特性在所有图片中必须保持一致</li>
</ul>
<h2 id="目标检测object-detection"><a class="anchor" href="#目标检测object-detection">#</a> 目标检测（Object detection）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513161138560.png?lastModify=1726287112" alt="image-20240513161138560" /></p>
<blockquote>
<p>剪掉汽车以外的部分，使汽车居于中间位置，并基本占据整张图片。（反之端到端的深度学习）</p>
<ol>
<li>更加<strong>集中</strong>地关注目标物体特征，而不是被图像的背景或边缘干扰。</li>
<li>模型可以更容易地学习到目标的特征，而不需要额外的处理来适应目标在图像中的<strong>位置变化</strong>。（图像随便移动旋转，相对位置不变）</li>
</ol>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513162549386.png?lastModify=1726287112" alt="image-20240513162549386" /></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513162615441.png?lastModify=1726287112" alt="image-20240513162615441" /></p>
</blockquote>
<p><strong>思路</strong>：以固定步幅移动窗口，遍历图像的每个区域，把这些剪切后的小图像输入卷积网络，对每个位置按 0 或 1 进行分类</p>
<ul>
<li>每次移动都要重新输入到卷积网络中进行分类</li>
</ul>
<p><strong>缺点</strong>：计算成本</p>
<ul>
<li>
<p>在图片中剪切出太多小方块，卷积网络要一个个地处理。</p>
<p>如果选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响<strong>性能</strong>。</p>
<p>反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着<strong>超高的计算成本。</strong></p>
</li>
</ul>
<p>卷积网络运行单个分类<strong>人物</strong>的<strong>成本却高</strong>得多，像这样滑动窗口太慢。除非采用超细粒度或极小步幅，否则无法准确定位图片中的对象。</p>
<h2 id="滑动窗口的卷积实现convolutional-implementation-of-sliding-windows"><a class="anchor" href="#滑动窗口的卷积实现convolutional-implementation-of-sliding-windows">#</a> 滑动窗口的卷积实现（Convolutional implementation of  sliding windows）</h2>
<p>为了构建滑动窗口的卷积应用，首先要知道如何把神经网络的全连接层转化成卷积层。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513171807156.png?lastModify=1726287112" alt="image-20240513171807156" /></p>
<p>通过卷积实现滑动窗口对象检测算法</p>
<blockquote>
<p>如下使用 14×14×3 的滑动窗口，然后再使用 10×10×16 的滑动窗口 ⋯</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513174531599.png?lastModify=1726287112" alt="image-20240513174531599" /></p>
<p>对于 16×16×3 的测试（参数相等）图片，最开始使用 14×14×3 的滑动窗口进行滑动，经过网络后输出为 2×2×4 的分类结果。</p>
<ul>
<li>也就是前面所述的滑动操作，每个点（1×1×4）就是一个区域的输出结果</li>
</ul>
<p><strong>优点</strong></p>
<blockquote>
<p>假设使用 4 次滑动窗口，也就是 4 个区域需要判断</p>
</blockquote>
<ul>
<li>
<p>在<strong>全连接</strong>层中，每个区域（还需要提取出来）需要<strong>重新经过全部网络</strong>的计算（计算 4 次）</p>
<blockquote>
<p>因为从卷积过渡到 <strong>FC</strong> 的时候，每个像素值都与网络中的每个神经元相连</p>
<ul>
<li>输入<strong>展平</strong>为一个向量</li>
</ul>
<p>重复的计算</p>
</blockquote>
</li>
<li>
<p>转换为<strong>卷积</strong>层，对于每个区域只需要对自己的区域特征进行操作，不需要重新计算整个网络的参数（计算 1 次）</p>
<blockquote>
<p><strong>参数共享</strong></p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513180621292.png?lastModify=1726287112" alt="image-20240513180621292" /></p>
<p>只需要经过一次卷积网络即可</p>
<h2 id="bounding-box-预测bounding-box-predictions"><a class="anchor" href="#bounding-box-预测bounding-box-predictions">#</a> Bounding Box 预测（Bounding box predictions）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513183502545.png?lastModify=1726287112" alt="image-20240513183502545" /></p>
<p>在滑动窗口法中，你取这些离散的位置集合，然后在它们上运行分类器，在这种情况下，这些边界框<strong>没有</strong>一个能完美匹配汽车位置</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513190556397.png?lastModify=1726287112" alt="image-20240513190556397" /></p>
<blockquote>
<p><strong>YOLO</strong> 取对象的中点，将带有中心的对象分配给包含中点的格子</p>
<ul>
<li>上述简单点取 3×3 的宫格</li>
</ul>
</blockquote>
<p>对于每一个格子都有对应的标签 y，总的目标输出维度 3×3×8</p>
<p>训练一个输入为 100×100×3 的神经网络，最后就映射到一个 3×3×8 的输出尺寸</p>
<blockquote>
<p>用反向传播训练神经网络时，将任意输入 x 映射到这类输出向量 y。</p>
<ul>
<li>通过反向传播，神经网络能够<strong>学习</strong>（不断更新参数）到输入与输出之间的映射关系，并且调整参数以使得预测输出<strong>尽可能</strong>地接近目标标签。</li>
</ul>
</blockquote>
<p>即使对象可以横跨多个格子，也只会被分配到 9 个格子其中 之一</p>
<ul>
<li>在 19×19 网格中， 两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会<strong>更低</strong>。</li>
</ul>
<p><strong>YOLO</strong> 直接让网络来<strong>输出</strong>窗口的位置和大小，可以具有任意宽高比，并且能输出更精确的坐标，不会受到滑动窗口分类器的步长大小限制。</p>
<p>其次，这是一个卷积实现，其中参数都是共享的，在 19×19 的网格上同之前不需要跑 361 次，只需要跑一次即可。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240513192255854.png?lastModify=1726287112" alt="image-20240513192255854" /></p>
<p>指定边界框的方式有很多，但这种约定是比较合理的</p>
<blockquote>
<p>不过还有其他更复杂的参数化方式，涉及到 <strong>sigmoid</strong> 函数</p>
<ul>
<li>确保这个值（𝑏x 和 𝑏y）介于 0 和 1 之间，</li>
</ul>
<p>然后使用指数参数化来确保这些（𝑏ℎ和𝑏𝑤）都是非负数，因为 0.9 和 0.5，这个必须大于等于 0。</p>
</blockquote>
<h2 id="交并比intersection-over-union"><a class="anchor" href="#交并比intersection-over-union">#</a> 交并比（Intersection over union）</h2>
<p>交并比函数，可以用来评价对象检测算法。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514130836636.png?lastModify=1726287112" alt="image-20240514130836636" /></p>
<p>IOU=A∩BA∪B</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514130955615.png?lastModify=1726287112" alt="image-20240514130955615" /></p>
</blockquote>
<p>一般约定，在计算机检测任务中，如果 IOU≥0.5，就说检测正确，如果预测器和实际边界框完美重叠，<strong>IOU</strong> 就是 1，因为交集就等于并集。</p>
<h2 id="非极大值抑制non-max-suppression"><a class="anchor" href="#非极大值抑制non-max-suppression">#</a> 非极大值抑制（Non-max suppression）</h2>
<p>非极大值抑制这个方法可以确保你的算法对每个对象只检测一次</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514132118689.png?lastModify=1726287112" alt="image-20240514132118689" /></p>
<p>找到最大概率的那个，然后抑制和这个<strong>最大概率的边框</strong>有很高<strong>交并比</strong>，高度重叠的其他边界框（大于阈值舍去）</p>
<blockquote>
<p>蓝色，暗蓝色</p>
</blockquote>
<ul>
<li>
<p>如果你直接抛弃变暗的矩形，那就剩下高亮显示的那些</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514132320526.png?lastModify=1726287112" alt="image-20240514132320526" /></p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514132838792.png?lastModify=1726287112" alt="image-20240514132838792" /></p>
<p>如果尝试同时检测三个对 象，比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。</p>
<ul>
<li>事实证明，正确的做法是<strong>独立进行三次</strong>非极大值抑制，对每个输出类别都做一次</li>
</ul>
<h2 id="anchor-boxes"><a class="anchor" href="#anchor-boxes">#</a> Anchor Boxes</h2>
<p>如果想让一个格子检测出多个对象，可以使用 <strong>anchor box</strong> 这个概念</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514134547294.png?lastModify=1726287112" alt="image-20240514134547294" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514135538858.png?lastModify=1726287112" alt="image-20240514135538858" /></p>
<p>不只分配一个格子，还会分配到一个跟你检测对象 <strong>IoU</strong> 最大的一个 <strong>anchor box</strong></p>
<blockquote>
<p>输出的标签 y （一个<strong>中心格子</strong>）从 8 维（<strong>anchor box 1</strong>）变为 16 维（<strong>anchor box 1 and 2</strong>）</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514140356986.png?lastModify=1726287112" alt="image-20240514140356986" /></p>
<blockquote>
<p>一个格子输出多个结果</p>
</blockquote>
<p>实践中这种情况很少发生，特别是如果你用的是 19×19 网格而不是 3×3 的网格，两个对象中点处于 361 个格子中同一个格子的概率很低，确实会出现，但出现频率不高。</p>
<p>人们一般手工指定 <strong>anchor box</strong> 形状，可以选择 5 到 10 个 <strong>anchor box</strong> 形状，覆盖到多种不同的形状，可以涵盖你想要检测的对象的各种形状。</p>
<h2 id="yolo-算法putting-it-together-yolo-algorithm"><a class="anchor" href="#yolo-算法putting-it-together-yolo-algorithm">#</a> YOLO 算法（Putting it together: YOLO algorithm）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514142914909.png?lastModify=1726287112" alt="image-20240514142914909" /></p>
<blockquote>
<p>构造训练集</p>
</blockquote>
<p>红框和 <strong>anchor box 2</strong> 的交并比更高，那么<strong>车子就和向量的下半部分</strong>相关。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514143852158.png?lastModify=1726287112" alt="image-20240514143852158" /></p>
<blockquote>
<p>输入图像，神经网络会输出尺寸为 3×3×2×8 的值</p>
</blockquote>
<p>最后要运行一下非极大值抑制</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514145712474.png?lastModify=1726287112" alt="image-20240514145712474" /></p>
<ol>
<li>
<p>若使用两个 <strong>anchor box</strong>，那么对于于 9 个格子中任何一个都会有两个预测的边界框，其中一个的概率 pc 很低。</p>
<p>但 9 个格子中，每个都有两个预测的边界框</p>
</li>
<li>
<p>接下来抛弃概率很低的预测</p>
</li>
<li>
<p>然后用非极大值抑制处理同一物体的不同框</p>
<blockquote>
<p>选概率最大的框，然后拿最大概率的框和其他同类别框 <strong>IoU</strong>，大于阈值就舍弃</p>
<ul>
<li>所以车的竖直框就被舍弃了</li>
</ul>
</blockquote>
<p>最后，若有三个对象检测类别，希望检测行人，汽车和摩托车</p>
<ul>
<li>
<p>对于<strong>每个类别单独</strong>运行非极大值抑制，处理预测结果所属类别的边界框</p>
<p>用非极大值抑制来处理行人类别</p>
<p>用非极大值抑制处理车子类别</p>
<p>然后对摩托车类别进行非极大值抑制</p>
</li>
</ul>
</li>
</ol>
<h2 id="候选区域region-proposals"><a class="anchor" href="#候选区域region-proposals">#</a> 候选区域（Region proposals）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514152923858.png?lastModify=1726287112" alt="image-20240514152923858" /></p>
<p>在滑动窗口中，使用训练过的分类器，在这些窗口中全部运行一遍，然后运行一个检测器，看看里面是否有车辆，行人和摩托车。</p>
<ul>
<li>现在也可以运行一下卷积算法</li>
</ul>
<p><strong>缺点</strong>就是：在显然没有任何对象的区域浪费时间</p>
<p>在候选区域运行，而选出候选区域的方法是运行<strong>图像分割算法</strong>（为了找出可能存在对象的区域）。</p>
<blockquote>
<p>先找出可能 2000 多个色块，然后在这 2000 个 色块上放置边界框，然后在这 2000 个色块上运行分类器，这样需要处理的位置可能要少的多，可以减少卷积网络分类器运行时间，比在图像所有位置运行一遍分类器要快</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240514153842423.png?lastModify=1726287112" alt="image-20240514153842423" /></p>
<p><strong>Fast R-CNN</strong>：使用卷积实现了滑动窗法。</p>
<ul>
<li>最初的算法是逐一对区域分类的（每一个区域都要跑一次）</li>
</ul>
<p><strong>Faster R-CNN</strong>：使用卷积神经网络，而不是更传统的分割算法来获得候选区域色块</p>
<h2 id="什么是人脸识别"><a class="anchor" href="#什么是人脸识别">#</a> 什么是人脸识别？</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515094249503.png?lastModify=1726287112" alt="image-20240515094249503" /></p>
<blockquote>
<p>一个是图像匹配系统，一个是系统中的数据去匹配图像</p>
</blockquote>
<p>所以验证系统，准确率 99% 是可以的</p>
<p>但是在人脸识别，准确率要远远高于 99%</p>
<ul>
<li>其准确率为 99.9% 或者更高</li>
</ul>
<h2 id="one-shot-学习one-shot-learning"><a class="anchor" href="#one-shot-学习one-shot-learning">#</a> One-Shot 学习（One-shot learning）</h2>
<p>人脸识别所面临的一个挑战就是你需要解决一次学习问题</p>
<ul>
<li>这意味着在大多数人脸识 别应用中，你需要通过单单一张图片或者单单一个人脸样例就能去识别这个人。</li>
<li>而历史上， 当深度学习只有一个<strong>训练样例</strong>时，它的表现并不好</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515094934603.png?lastModify=1726287112" alt="image-20240515094934603" /></p>
<blockquote>
<p>系统需要做的就是，仅仅通过一张已有的照片，来识别前面这个人确实是她。</p>
<p>相反，如果机器看到一个不在数据库里的人，机器应该能分辨出她不是数据库中四个人之一。</p>
</blockquote>
<p>所以在一次学习问题中，只能通过<strong>一个样本</strong>进行学习，以能够认出同一个人。</p>
<p>一种方法是：将人的照片放进卷积神经网络中，使用 <strong>softmax</strong> 单元来输出 4 种或 5 种，分别对应这 4 个人，或者 4 个都不是，所以 <strong>softmax</strong> 里我们会有 5 种输出。</p>
<ul>
<li>
<p>但实际上这样效果并不好，因为如此小的训练集不足以去训练一个稳健的神经网络。</p>
<blockquote>
<p>特征太少</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>假如有<strong>新人</strong>加入你的团队，你现在将会有 5 个组员需要识别，所以输出就变成 了 6 种，这时你要重新训练你的神经网络吗？</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515095431234.png?lastModify=1726287112" alt="image-20240515095431234" /></p>
<blockquote>
<p>学习 <strong>Similarity</strong> 函数</p>
</blockquote>
<p>也就是 d (img1,img2)=degree of difference between images</p>
<ul>
<li>
<p>它以两张图片作为输入，然后输出这两张图片的差异值。</p>
<p>若 d≤τ （超参数），那么这时就能预测这两张图片是同一个人</p>
</li>
</ul>
<h2 id="siamese-网络siamese-network"><a class="anchor" href="#siamese-网络siamese-network">#</a> Siamese 网络（Siamese network）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515102154631.png?lastModify=1726287112" alt="image-20240515102154631" /></p>
<blockquote>
<p>把第二张图片喂给有<strong>同样参数的同样的神经网络</strong></p>
<p>f (x (1)) 可看作是输入图像 x (1) 的编码</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515102601662.png?lastModify=1726287112" alt="image-20240515102601662" /></p>
<p>这两个网络有相同的参数，所以你实际 要做的就是训练一个网络，它计算得到的编码可以用于函数 d，它可以告诉你两张图片是否是同一个人。</p>
<p>如果你改变这个网络所有层的参数，你会得到不同的<strong>编码结果</strong>，你要做的就是用<strong>反向传播</strong>来改变这些所有的参数，以<strong>确保满足</strong>这些条件。</p>
<blockquote>
<p>这里的损失函数：使得差距最小</p>
</blockquote>
<h2 id="triplet-损失triplet-损失"><a class="anchor" href="#triplet-损失triplet-损失">#</a> Triplet 损失（Triplet 损失）</h2>
<p>要想通过学习神经网络的参数来得到优质的人脸图片编码</p>
<ul>
<li>方法之一就是定义<strong>三元组损失函数</strong>然后应用梯度下降。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515103815182.png?lastModify=1726287112" alt="image-20240515103815182" /></p>
<blockquote>
<p>三元组损失，它代表通常会<strong>同时看三张图片</strong></p>
</blockquote>
<p>若采用公式来说明，也就想要 d=||f (A)−f (P)||2 很小，即</p>
<p>||f(A)−f(P)||2≤||f(A)−f(N)||2→||f(A)−f(P)||2−||f(A)−f(N)||2≤0</p>
<ul>
<li>若 f=0 ，那么始终满足上述方程。</li>
<li>若每个图片的编码和其他图片一样，那么也满足上述方程（0−0）</li>
</ul>
<p>所以需要修改目标：不能刚好小于等于 0，要小于一个 0−α 值</p>
<p>||f(A)−f(P)||2−||f(A)−f(N)||2+α≤0</p>
<blockquote>
<p>例如：α=0.2，d (A,P)=0.5,d (A,N)=0.51</p>
<ul>
<li>所以增加了这个间隔 α ，使得二者至少相差 0.2。（分类要清楚，起码差距 <strong>margin</strong> 要有 0.2 的显著差别）</li>
</ul>
</blockquote>
<p><strong>Loss function</strong>：</p>
<p>L(A,P,N)=max(||f(A)−f(P)||2−||f(A)−f(N)||2+α,0)J=∑i=1mL(A(i),P(i),N(i))</p>
<p>假如你有一个 10000 个图片的训练集，里面是 1000 个不同的人的照片</p>
<ul>
<li>要做的就是取这 10000 个图片，然后生成这样的三元组，然后训练你的学习算法，对这种<strong>代价函数用梯度下降</strong>，这个代价函数就是定义在你数据集里的这样的三元组图片上。</li>
</ul>
<blockquote>
<p>训练完这个系统之后，你可以应用到你的一次学习问题上，对于你的人脸识别系统， 可能你只有想要识别的某个人的一张照片。</p>
<ul>
<li>但对于训练集，你需要确保有同一个人的多个图片</li>
</ul>
</blockquote>
<p>若<strong>随机</strong>地选择三元组，那么 ||f (A)−f (P)||2−||f (A)−f (N)||2≤0 就很容易被满足</p>
<ul>
<li>
<p>说明 𝐴 和 𝑁 比 𝐴 和 P 差别很大的概率很大</p>
<p>网络并不能从中学习到什么，因为损失很小（上述式子损失为 0，想到与不做梯度下降）</p>
</li>
</ul>
<p>所以为了构建一个数据集，你要做的就是尽可能选择难训练的三元组 A,P,N</p>
<ul>
<li>
<p>可以使得 d (A,P)≈d (A,N)</p>
<p>这样你的学习算法会竭尽全力使右边这个式子变大，或者使左边这个式子变小，使得满足至少有一个 α 的间隔</p>
</li>
<li>
<p>梯度算法才会有效果</p>
</li>
</ul>
<blockquote>
<p>Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1503.03832.pdf">FaceNet: A Unified Embedding  for Face Recognition and Clustering </a></p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515110759037.png?lastModify=1726287112" alt="image-20240515110759037" /></p>
<p>定义了这些包括 𝐴、𝑃 和 𝑁 图片的数据集之后，你还需要做的就是用梯度下降最小化之前定义的代价函数 𝐽</p>
<ul>
<li>
<p>这样做的效果就是反向传播到网络中的所有参数来学习到一种编码</p>
<p>使得如果两个图片是同一个人，那么它们的 𝑑 就会很小</p>
<p>如果两个图片不是同一个人， 它们的 𝑑 就会很大。</p>
</li>
</ul>
<p>相比于从头训练这些网络， 在这一领域，由于这些数据集太大，这一领域的一个<strong>实用操作</strong>就是下载别人的预训练模型， 而不是一切都要从头开始。</p>
<ul>
<li>但是即使你下载了别人的预训练模型，了解怎么训练这些算法也是有用的，以防针对一些应用你需要从头实现这些想法。</li>
</ul>
<h2 id="人脸验证与二分类face-verification-and-binary-classification"><a class="anchor" href="#人脸验证与二分类face-verification-and-binary-classification">#</a> 人脸验证与二分类（Face verification and binary  classification）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515114030808.png?lastModify=1726287112" alt="image-20240515114030808" /></p>
<p>例如：y^=σ(∑k=1128wk|f (x (i)) k−f (x (j)) k|+b)</p>
<blockquote>
<p>将在这 128 个单元上训练合适的<strong>权重</strong>，用来预测两张图片是否是一个人</p>
</blockquote>
<ul>
<li>
<p>也可以使用 y^=σ(∑k=1128wk (f (x (i)) k−f (x (j)) k) 2f (x (i)) k+f (x (j)) k+b)</p>
<blockquote>
<p>为 χ2 分布</p>
</blockquote>
</li>
</ul>
<p>不需要每次都计算上述这些特征，可以提前计算好（<strong>precompute</strong>）</p>
<blockquote>
<p>先根据数据库中的图片计算好编码，然后存储起来，来<strong>人</strong>后只需要让计算这个人，然后去与数据库中的编码进行比较即可</p>
</blockquote>
<h2 id="什么是神经风格迁移what-is-neural-style-transfer"><a class="anchor" href="#什么是神经风格迁移what-is-neural-style-transfer">#</a> 什么是神经风格迁移？（What is neural style transfer?）</h2>
<p>卷积神经网络最有趣的应用是神经风格迁移</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515172609222.png?lastModify=1726287112" alt="image-20240515172609222" /></p>
<h2 id="深度卷积网络学习什么what-are-deep-convnets-learning"><a class="anchor" href="#深度卷积网络学习什么what-are-deep-convnets-learning">#</a> 深度卷积网络学习什么？（What are deep ConvNets  learning?）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515173559159.png?lastModify=1726287112" alt="image-20240515173559159" /></p>
<p>从第一层的隐藏单元开始，假设你遍历了训练集，然后找到那些使得<strong>单元激活最大化</strong>的一些图片，或者是<strong>图片块</strong>。</p>
<ul>
<li>换句话说，将你的训练集经过神经网络，然后弄明白哪一张图片最大限度地激活特定的单元。</li>
</ul>
<blockquote>
<p>例如：选择一个隐藏单元，发现有 9 个图片最大化了单元激活</p>
<ul>
<li>
<p>似乎是图片浅层区域显示了隐藏单元所看到的，找到了像左边的边缘或者线，</p>
<p>这就是那 9 个最大化地激活了隐藏单元激活项的图片块。</p>
</li>
</ul>
</blockquote>
<ul>
<li>
<p>最大化激活目的：最大程度激活<strong>某个隐藏单元</strong>的输入<strong>图像特征</strong></p>
<blockquote>
<p>也就是把某个特征的相似图片块找出来（这特征是提取出来的）</p>
</blockquote>
<p><strong>例如</strong>：第一个隐藏单元到寻找坐上到右下的斜线，选出了最符合的 9 个图片</p>
</li>
</ul>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515175414681.png?lastModify=1726287112" alt="image-20240515175414681" /></p>
<p>这是 9 个不同的代表性<strong>神经元</strong>，每一个不同的图片块都<strong>最大化地激活</strong>了。</p>
<p>可以这样理解，第一层的隐藏单元通常会找一些简单的特征，比如说边缘或者颜色阴影。</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515213452509.png?lastModify=1726287112" alt="image-20240515213452509" /></p>
<p>从检测简单的事物， 比如说，第一层的边缘，第二层的质地，到深层的复杂物体。</p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515215258079.png?lastModify=1726287112" alt="image-20240515215258079" /></p>
</blockquote>
<h2 id="代价函数cost-function"><a class="anchor" href="#代价函数cost-function">#</a> 代价函数（Cost function）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515214416164.png?lastModify=1726287112" alt="image-20240515214416164" /></p>
<p>Jcontent (C,G)：度量生成图片 G 的内容与内容图片 C 的内容有多相似。</p>
<ul>
<li>α,β 为超参数</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515214730562.png?lastModify=1726287112" alt="image-20240515214730562" /></p>
<ol>
<li>随机初始化 G</li>
<li>使用梯度下降逐步得到风格图片</li>
</ol>
<blockquote>
<p>经过两个损失函数 Jc,JS ，使得 G 逐步提取的特征是 C,S 的组合</p>
<ul>
<li>提取了 C 的边缘特征和 S 的颜色特征，然后组成 G</li>
</ul>
</blockquote>
<h2 id="内容代价函数content-cost-function"><a class="anchor" href="#内容代价函数content-cost-function">#</a> 内容代价函数（Content cost function）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240515220121552.png?lastModify=1726287112" alt="image-20240515220121552" /></p>
<p>风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。</p>
<p>J(G)=αJcontent(C,G)+βJstyle(S,G)</p>
<ul>
<li>
<p>使用隐藏层 l 来计算代价</p>
<blockquote>
<p>若 l 很小，比如隐藏层 1，这个代 价函数就会使你的生成图片像素上非常<strong>接近</strong>你的内容图片（浅层特征都是你的图像上面的）</p>
<p>若 l 很大，深层，那么就会问，内容图片里是否有狗，然后它就会确保生成图片里有一个<strong>狗</strong>（组合后的特征）</p>
<p>l 会选择在网络的中间层</p>
</blockquote>
</li>
<li>
<p>使用预训练的卷积模型，例如 <strong>VGG</strong> 网络等</p>
</li>
<li>
<p>衡量一个内容图片和一个生成图片他们在内容上的相似度</p>
<p>Jcontent(C,G)=12||a<a href="C">l</a>−a<a href="G">l</a>||2</p>
<blockquote>
<p>展成向量形式</p>
</blockquote>
<ul>
<li>这就是两个图片之间 l 层激活值差值的平方和，若损失越小，那么 G 与 C 就越相似</li>
</ul>
</li>
</ul>
<h2 id="风格代价函数style-cost-function"><a class="anchor" href="#风格代价函数style-cost-function">#</a> 风格代价函数（Style cost function）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516141642193.png?lastModify=1726287112" alt="image-20240516141642193" /></p>
<p>选择某一隐藏层 l，为图片的风格定义一个深度测量</p>
<p>现在要做的就是将图片的风格定义为 l 层中各个通道之间激活项的<strong>相关系数</strong>。</p>
<ul>
<li>
<p>例如：计算红色、黄色前两个通道之间激活项的相关系数</p>
<blockquote>
<p>左下角在第一个通道中含有某个激活项，第二个通道也含有某个激活项，于是它们组成了<strong>一对数字</strong></p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516142616776.png?lastModify=1726287112" alt="image-20240516142616776" /></p>
<blockquote>
<p>若红色、黄色两个通道的神经元分别对应图片特定是否有<strong>垂直纹理、橙色区域</strong></p>
</blockquote>
<ul>
<li>若两个通道有高度相关性：那么出现垂直纹理，就有很大概率是橙色</li>
<li>若两个通道不相关：那么有垂直纹理的地方很大概率不是橙色的</li>
</ul>
<blockquote>
<p>相关系数描述的是：当图片某处出现这种垂直纹理时，该处又同时是橙色的可能性。</p>
<p>相关系数这个概念提供了一种去测量这些<strong>不同的特征</strong>的方法</p>
</blockquote>
<p>如果在通道之间使用相关系数来描述通道的风格，能做的就是测量<strong>生成</strong>图像中第一个通道是否与第二个通道相关，</p>
<ul>
<li>
<p>通过测量，能得知在生成的图像中垂直纹理和橙色同时出现或者不同时出现的<strong>频率</strong></p>
<p>这样你将能够测量<strong>生成</strong>的图像的风格与<strong>输入</strong>的风格图像的<strong>相似程度</strong>。</p>
</li>
</ul>
<p><strong>Style matrix（Gram matrix）</strong></p>
<p>ai,j,k[l]=activation at (i,j,k). G<a href="S">l</a>:(nc[l],nc[l])</p>
<p>Gkk′<a href="S">l</a>=∑i=1nH[l]∑j=1nW[l]aijk<a href="S">l</a>aijk′<a href="S">l</a>Gkk′<a href="G">l</a>=∑i=1nH[l]∑j=1nW[l]aijk<a href="G">l</a>aijk′<a href="G">l</a></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516145309432.png?lastModify=1726287112" alt="image-20240516145309432" /></p>
</blockquote>
<p>Gkk′[l]：用来测量 k 通道与 k′ 通道中的激活项之间的相关系数</p>
<blockquote>
<p>在某一层，若一个 <strong>filter</strong> 提取的是某种线条，另一个 <strong>filter</strong> 提取的是某种颜色</p>
<ul>
<li>那么若把两张风格相似的图片放进去计算这一层的值 a 应该是呈现正相关的。</li>
</ul>
<p><strong>Gram</strong> 矩阵的元素表示了特征图中不同通道之间的相关性程度。较大的元素值表示对应通道之间的相关性较强，而较小的元素值表示相关性较弱。</p>
</blockquote>
<ul>
<li>
<p>风格就是不同特征之间的联系程度，G 来衡量</p>
<blockquote>
<p>关于特征关系可以表示风格，例如深色 + 密集的线很压抑，这就是一种风格。</p>
</blockquote>
</li>
<li>
<p>衡量的是不同特征间关系，有 nc 个通道就有 nc 个特征</p>
</li>
</ul>
<p>Jstyle<a href="S,G">l</a>=1(2nH[l]nW[l]nC[l])2||G<a href="S">l</a>−G<a href="G">l</a>||F2=1(2nH[l]nW[l]nC[l])2∑k∑k(Gkk′<a href="S">l</a>−Gkk′<a href="G">l</a>)</p>
<blockquote>
<p>一般我们只要将它乘以一个超参数 β 就行。</p>
</blockquote>
<p>实际上，如果你对各层都使用风格代价函数，会让结果变得更好。</p>
<p>Jstyle(S,G)=∑lλ[l]Jstyle<a href="S,G">l</a></p>
<p>λ：每个层的权重</p>
<ul>
<li>越往深层，风格就越像 S 图像</li>
</ul>
<blockquote>
<p>这样将使你能够在神经网络中使用不同的层，包括之前的一些可以测量类似边缘这样的低级特征的层，以及之后的一些能测量高级特征的层</p>
</blockquote>
<p>J(G)=αJcontent(C,G)+βJstyle(S,G)</p>
<h2 id="一维到三维推广1d-and-3d-generalizations-of-models"><a class="anchor" href="#一维到三维推广1d-and-3d-generalizations-of-models">#</a> 一维到三维推广（1D and 3D generalizations of models）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516152316176.png?lastModify=1726287112" alt="image-20240516152316176" /></p>
<p>这些方法也可以应用于 1 维数据，你可以在不同的位置使用相同的特征检测器</p>
<blockquote>
<p>比如说，为了区分 EKG 信号中的心跳的差异，可以在不同的时间轴位置使用<strong>同样</strong>的特征来检测心跳。</p>
</blockquote>
<p>CT 扫描实现的是它可以获取你身体不同片段。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516193505708.png?lastModify=1726287112" alt="image-20240516193505708" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516194034882.png?lastModify=1726287112" alt="image-20240516194034882" /></p>
<p>某种程度上 3D 数据也可以使用 3D 卷积网络学习，这些过滤器实现的功能正是通过你 的 3D 数据进行特征检测。</p>
<p>CT 医疗扫描是 3D 数据的一个实例</p>
<ul>
<li>另一个数据处理的例子是你可以将电影中随时间变化的不同视频切片看作是 3D 数据，你可以将这个技术用于检测动作及人物行为。</li>
</ul>
<h1 id="序列模型sequence-models"><a class="anchor" href="#序列模型sequence-models">#</a> 序列模型 (Sequence Models)</h1>
<h2 id="为什么选择序列模型why-sequence-models"><a class="anchor" href="#为什么选择序列模型why-sequence-models">#</a> 为什么选择序列模型？（Why Sequence Models?）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516195234602.png?lastModify=1726287112" alt="image-20240516195234602" /></p>
<p>所以这些问题都可以被称作使用标签数据 (𝑥,𝑦) 作为训练集的监督学习。</p>
<p>你可以看出序列问题有很多不同类型。有些问题里，输入数据  𝑥 和输出数据 𝑦 都是序列</p>
<p>在另一些问题里，只有 𝑥 或者只有 𝑦 是序列。</p>
<h2 id="数学符号notation"><a class="anchor" href="#数学符号notation">#</a> 数学符号（Notation）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516201241082.png?lastModify=1726287112" alt="image-20240516201241082" /></p>
<p>若输入数据是 9 个<strong>单词</strong>组成的序列，所以最终我们会有 9 个<strong>特征集和</strong>来表示这 9 个单词</p>
<ul>
<li>
<p>x&lt;1&gt;⋯x&lt;9&gt;：索引不同的位置</p>
<p>x&lt;t&gt;：索引的中间位置。</p>
</li>
</ul>
<p>输出数据：y&lt;1&gt;⋯y&lt;9&gt;</p>
<p>Tx：输入序列的长度，Ty：输出序列的长度</p>
<p>x (i)&lt;t&gt;：第 i 个训练样本的第 t 个元素</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516202218070.png?lastModify=1726287112" alt="image-20240516202218070" /></p>
<blockquote>
<p>构建这个词典的一个方法是遍历你的训练集，并且找到前 10,000 个常用词，你也可以去浏览一些网络词典，它能告诉你英语里最常用的 10,000 个单词，接下来你可以用 <strong>one-hot</strong> 表示法来表示词典里的每个单词。</p>
</blockquote>
<p>用序列模型在 𝑋 和目标输出 𝑌 之间学习建立一个映射</p>
<p>若遇到了一个不在词表中的单词</p>
<ul>
<li>创建一个新的标记 <strong>UNK</strong>（Unknow Word） 的伪单词。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516203820128.png?lastModify=1726287112" alt="image-20240516203820128" /></p>
<p>在标准的网络中，9 个输入单词（可能是 <strong>one-hot</strong> 向量），输出也是 9 个（表示是不是人名的一部分。）</p>
<ol>
<li>
<p>输入和输出数据在不同例子中可以有不同的长度</p>
</li>
<li>
<p>一个像这样单纯的神经网络结构，它并不共享从文本的不同位置上学到的特征</p>
<p>若神经网络已经学习到了在位置 1 出现的 <strong>Harry</strong> 可能是人名的一部分</p>
<ul>
<li>那么如果 <strong>Harry</strong> 出现在其他位置，比如 x&lt;t&gt; 时，希望它也能够<strong>自动识别</strong>其为人名的一部分</li>
</ul>
</li>
<li>
<p>上述每个 x&lt;t&gt; 都是 10,000 维的 <strong>one-hot</strong> 向量，那么这会是十分庞大的输入层，第一层的权重矩阵就会有着巨量的参数。</p>
</li>
</ol>
<h2 id="循环神经网络"><a class="anchor" href="#循环神经网络">#</a> 循环神经网络</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240601150441616.png?lastModify=1726287112" alt="image-20240601150441616" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516211411606.png?lastModify=1726287112" alt="image-20240516211411606" /></p>
<blockquote>
<p>每一个 x&lt;t&gt; 到 y&lt;t&gt; 都是一个<strong>神经网络</strong></p>
</blockquote>
<p>每个神经网络输出的 a&lt;1&gt; 作为下一个神经网络的输入。</p>
<ul>
<li>在每一个时间步中，循环神经网络传递一个激活值到下一 个时间步中用于计算。</li>
</ul>
<p>对于 a&lt;0&gt; 通常是 0 向量。</p>
<p>Wax 等都是在每个时间步是共享的。</p>
<p>在上述的循环神经网络中，在预测 y^&lt;3&gt; 时，不仅要使用 x&lt;3&gt; 的信息，还使用了 x&lt;1&gt;,x&lt;2&gt; 的信息。</p>
<p>但是也有<strong>缺点</strong>：在预测 y^&lt;3&gt; 的时候，没有用到 x&lt;4&gt;,x&lt;5&gt; 等的信息</p>
<ul>
<li>
<p>例如：<strong>He said, “Teddy Roosevelt was a great President.”</strong></p>
<p>为了判断 <strong>Teddy</strong> 是不是人名，仅靠前两个单词是完全不够的，还需要指导句中后半部分的信息。</p>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516214459242.png?lastModify=1726287112" alt="image-20240516214459242" /></p>
<blockquote>
<p>Wax：</p>
<ul>
<li>第一个下标表示用来计算某个 a 类型的变量</li>
<li>第二个下标表示 Wax 要乘以某个 x 类型的变量</li>
</ul>
</blockquote>
<p>a&lt;t&gt;=g(Waaa+Waxx&lt;t&gt;+ba)y^&lt;t&gt;=g(Wyaa&lt;t&gt;+by)</p>
<p>对于 a&lt;t&gt;：通常使用 <strong>tanh</strong></p>
<blockquote>
<p>有其他办法避免梯度消失</p>
</blockquote>
<p>对于 y^&lt;t&gt;：二分类的话可以使用 <strong>sigmoid</strong></p>
<p>为了帮我们建立更复杂的神经网络，实际要将上述公式<strong>简化</strong>一下</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240516215530040.png?lastModify=1726287112" alt="image-20240516215530040" /></p>
<blockquote>
<p>矩阵分块</p>
</blockquote>
<h2 id="通过时间的反向传播backpropagation-through-time"><a class="anchor" href="#通过时间的反向传播backpropagation-through-time">#</a> 通过时间的反向传播（Backpropagation through time）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517133511657.png?lastModify=1726287112" alt="image-20240517133511657" /></p>
<p>L&lt;t&gt;(y<sup>&lt;t&gt;,y&lt;t&gt;)=−y&lt;t&gt;log⁡y</sup>&lt;t&gt;−(1−y&lt;t&gt;)log⁡(1−y^&lt;t&gt;)</p>
<ul>
<li>标准逻辑回归损失函数， 也叫<strong>交叉熵损失函数（Cross Entropy Loss）</strong></li>
</ul>
<p>L(y<sup>,y)=∑t=1TyL&lt;t&gt;(y</sup>&lt;t&gt;,y&lt;t&gt;)</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517134439999.png?lastModify=1726287112" alt="image-20240517134439999" /></p>
<h2 id="不同类型的循环神经网络different-types-of-rnns"><a class="anchor" href="#不同类型的循环神经网络different-types-of-rnns">#</a> 不同类型的循环神经网络（Different types of RNNs）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517134626619.png?lastModify=1726287112" alt="image-20240517134626619" /></p>
<ol>
<li>
<p><strong>many-to-many</strong></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517140142446.png?lastModify=1726287112" alt="image-20240517140142446" /></p>
</blockquote>
</li>
<li>
<p><strong>many-to-one</strong></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517140210995.png?lastModify=1726287112" alt="image-20240517140210995" /></p>
<p>例如情感分析，输入文本：“These is nothing to like in this movie“，输出评价（1-5 星）</p>
</blockquote>
</li>
<li>
<p><strong>one-to-one</strong></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517140015673.png?lastModify=1726287112" alt="image-20240517140015673" /></p>
<p>一对一就是个普通的神经网络，不需要 a&lt;0&gt;</p>
</blockquote>
</li>
<li>
<p><strong>one-to-many</strong></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517140245263.png?lastModify=1726287112" alt="image-20240517140245263" /></p>
<p>例如：输入 x 为一个整数，输出为一些音符</p>
</blockquote>
</li>
<li>
<p><strong>many-to-many</strong></p>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517140344303.png?lastModify=1726287112" alt="image-20240517140344303" /></p>
<p>例如：机器翻译，输入（encoder），输出（decoder）</p>
<p>Tx,Ty 可以不同</p>
</blockquote>
</li>
</ol>
<h2 id="语言模型和序列生成language-model-and-sequence-generation"><a class="anchor" href="#语言模型和序列生成language-model-and-sequence-generation">#</a> 语言模型和序列生成（Language model and sequence  generation）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517141009746.png?lastModify=1726287112" alt="image-20240517141009746" /></p>
<blockquote>
<p>第二句话的概率大于第一句话</p>
<p>对于语言模型来说，用 𝑦 来表示这些序列比用 𝑥 来表示要更好</p>
<ul>
<li>关注的是预测目标序列的概率分布</li>
</ul>
</blockquote>
<p>语言模型所做的就是，它会告诉你某个特定的句子它出现的<strong>概率</strong>是多少</p>
<ul>
<li>全世界所有句子之一中被听到的概率</li>
</ul>
<blockquote>
<p>通过统计<strong>语料库</strong>中相应句子的出现频率，语言模型可以计算出这个句子的概率。</p>
<p>换句话说，语言模型可以告诉你某个特定句子在整个语言空间中被听到的概率有多大。</p>
<ul>
<li>由于语言空间是无限的，全世界所有句子之一中被听到的概率理论上是非常小的。</li>
</ul>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240517143150536.png?lastModify=1726287112" alt="image-20240517143150536" /></p>
<p><strong>&lt; EOS&gt;</strong>：句子的结尾（end of sentence）</p>
<p>若训练集中有些单词不在字典中，例如 <strong>Mau</strong> 不在字典中，那么就替换为 <strong>UNK</strong></p>
<ul>
<li>只针对 <strong>UNK</strong> 建立概率模型， 而不是针对这个具体的词 <strong>Mau</strong>。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519125135774.png?lastModify=1726287112" alt="image-20240519125135774" /></p>
<p>a&lt;1&gt;：通过 <strong>softmax</strong> 进行一些预测来计算出第一个词可能会是什么</p>
<ul>
<li>结果就是 y&lt;1&gt;</li>
</ul>
<blockquote>
<p><strong>a</strong> 的概率，<strong>cats</strong> 的概率...</p>
</blockquote>
<ul>
<li>
<p>它只是预测第一个词的概率，而<strong>不去管结果是什么</strong></p>
<blockquote>
<p>通过 <strong>softmax</strong> 层计算第一个单词的概率分布，然后选择<strong>概率最大</strong>的单词作为预测结果。而这个预测结果并不管它是什么，只是单纯地作为下一步的输入条件，继续预测下一个单词</p>
</blockquote>
</li>
</ul>
<p>x&lt;2&gt;=y&lt;1&gt;：传递一个正确的单词：<strong>cats</strong></p>
<ul>
<li>
<p>输出结果同样经过 <strong>softmax</strong> 层进行预测出 y&lt;2&gt;</p>
<p>以 <strong>cats</strong> 为条件</p>
</li>
</ul>
<p>同理：下一步以 <strong>cats average</strong> 为条件</p>
<p>L&lt;t&gt;(y<sup>&lt;t&gt;,y&lt;t&gt;)=−∑iyi&lt;t&gt;log⁡y</sup>i&lt;t&gt;</p>
<p>L=∑tL&lt;t&gt;(y^,y)</p>
<p>计算出整个句子中各个单词的概率：</p>
<p>P(y&lt;1&gt;,y&lt;2&gt;,y&lt;3&gt;)=P(y&lt;1&gt;)P(y&lt;2&gt;|y&lt;1&gt;)P(y&lt;3&gt;|y&lt;1&gt;,y&lt;2&gt;)</p>
<h2 id="对新序列采样sampling-novel-sequences"><a class="anchor" href="#对新序列采样sampling-novel-sequences">#</a> 对新序列采样（Sampling novel sequences）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519140716293.png?lastModify=1726287112" alt="image-20240519140716293" /></p>
<blockquote>
<p>使用训练过的模型进行采样</p>
<ul>
<li>从你的 <strong>RNN</strong> 语言模型中生成一个随机选择的句子</li>
</ul>
</blockquote>
<ul>
<li>输出不断地作为输入</li>
<li>对输出的结果进行采样（随机选择：<strong>np.random.choice</strong>），然后使用 <strong>one-hot</strong> 编码传递下去</li>
</ul>
<p>如何用这个模型输出一个句子</p>
<ul>
<li>基于莎士比亚的诗歌集，构建和训练了一个 RNN 模型。用基于训练好的该模型，即可输出句子，这些句子应该具备莎士比亚风格</li>
</ul>
<p>如何知道一个句子是否结束</p>
<ol>
<li>
<p>若句子在字典中，可以一直进行采样直到得到 <strong>EOS</strong> 标识</p>
</li>
<li>
<p>若字典没有 <strong>EOS</strong>，可以决定从 20 个或 100 个或其他个单词进行采样，然后一直将采样进行下去直到达到<strong>所设定的时间步</strong>。</p>
<p>不过这种过程有时候会产生一些未知标识（可以重新采样或者不管）</p>
</li>
</ol>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519141510623.png?lastModify=1726287112" alt="image-20240519141510623" /></p>
<p>若建立基于字符的语言模型，序列 y^&lt;1&gt;⋯ 将会是单独的字符</p>
<p><strong>优点</strong>：不必担心会出现未知的标识</p>
<p><strong>缺点</strong>：最后会得到太多太长的序列</p>
<blockquote>
<p>大多数英语句子只有 10 到 20 个的 单词，但却可能包含很多很多字符</p>
</blockquote>
<ul>
<li>
<p>计算成本比较高昂</p>
</li>
<li>
<p>基于字符的语言模型在捕捉句子中的长范围依赖关系方面可能不如基于词汇的语言模型那样有效</p>
<blockquote>
<p>基于字符的语言模型仅以<strong>字符</strong>为单位进行建模，而不考虑词汇的语义信息和组合关系。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519142243474.png?lastModify=1726287112" alt="image-20240519142243474" /></p>
</blockquote>
<h2 id="循环神经网络的梯度消失vanishing-gradients-with-rnns"><a class="anchor" href="#循环神经网络的梯度消失vanishing-gradients-with-rnns">#</a> 循环神经网络的梯度消失（Vanishing gradients with  RNNs）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519175125307.png?lastModify=1726287112" alt="image-20240519175125307" /></p>
<p><strong>The cat, which already ate ……, was full.</strong> 与 <strong>The cats, which already ate ……, were full.</strong></p>
<ul>
<li>
<p>这句子有长期的依赖，最前面的单词 <strong>cat</strong> 或者 <strong>cats</strong> 对后面的单词有影响。</p>
<p>目前见到的基本的 <strong>RNN</strong> 模型，不擅长捕获这种长期依赖效应。</p>
</li>
</ul>
<p>对于很深的普通神经网络，从输出 y^ 得到的梯度很难传播回去，很难影响靠前层的权重</p>
<p>对于 <strong>RNN</strong>，同样的梯度消失的问题，后面层的输出误差<strong>很难</strong>影响前面层的计算。</p>
<ul>
<li>这就意味着，实际上很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词</li>
</ul>
<p>基本的 <strong>RNN</strong> 会有很多局部的影响，输出的 y^&lt;t&gt; 主要受到<strong>临近的影响</strong></p>
<blockquote>
<p>RNN 的梯度被近距离梯度主导，被远距离梯度忽略</p>
</blockquote>
<p>对于梯度爆炸很容易发现：参数会大到崩溃，你会看到很多 <strong>NaN</strong>，或者不是数字的情况</p>
<ul>
<li>一个解决方法就是用梯度修剪：观察你的梯度向量，如果它大于某个阈值，缩放梯度向量，保证它不会太大，这就是通过一些最大值来修剪的方 法。</li>
</ul>
<h2 id="gru-单元gated-recurrent-unitgru"><a class="anchor" href="#gru-单元gated-recurrent-unitgru">#</a> GRU 单元（Gated Recurrent Unit（GRU）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519224023543.png?lastModify=1726287112" alt="image-20240519224023543" /></p>
<blockquote>
<p>这就是 <strong>RNN</strong> 隐藏层的单元的可视化呈现</p>
</blockquote>
<ol>
<li>
<p>c=memory cell</p>
</li>
<li>
<p>c&lt;t&gt;=a&lt;t&gt;</p>
<blockquote>
<p>记忆细胞的值</p>
</blockquote>
</li>
<li>
<p>c~&lt;t&gt;=tanh(wc[c&lt;t−1&gt;,x&lt;t&gt;]+bc)</p>
<blockquote>
<p>候选值，进行重写记忆细胞 c</p>
</blockquote>
</li>
<li>
<p>Γu=σ(wu[c&lt;t−1&gt;,xt]+bu)</p>
<blockquote>
<p>更新门，就是个 <strong>flag</strong></p>
<p>由于是 <strong>sigmoid</strong> 函数，所以总是在 0，1 之间</p>
<p>对于大多数可能的输入，<strong>sigmoid</strong> 函数的输出总是非常接近 0 或者非常接近 1</p>
</blockquote>
</li>
<li>
<p>c&lt;t&gt;=Γu∗c~&lt;t&gt;+(1−Γu)∗c&lt;t−1&gt;</p>
<blockquote>
<p>若 Γu=1，那么就代表更新细胞单元</p>
<ul>
<li>c&lt;t&gt;=c~&lt;t&gt;</li>
</ul>
<p>若 Γ=0，那么就代表使用<strong>旧的值</strong></p>
<ul>
<li>c&lt;t&gt;=c&lt;t−1&gt;</li>
</ul>
</blockquote>
</li>
<li>
<p>a&lt;t&gt;=c&lt;t&gt;</p>
</li>
</ol>
<p>对于 <strong>The cat, which already ate..., was full.</strong></p>
<ul>
<li>
<p><strong>cat</strong> →Γu=1,c&lt;t&gt;=1（假设）</p>
<p>从左到右扫描句子，对于每个单词 Γu=0,c&lt;t&gt;=c&lt;t−1&gt; （使用旧值，不更新）</p>
<ul>
<li>仍然记得猫是单数的。</li>
</ul>
<p>知道遇到 <strong>was</strong> →Γu=1,c&lt;t&gt;=1（更新记忆单元）</p>
</li>
<li>
<p>也就是激活值会随着时间推移，由于后层的各种输入和计算之间对后层失去影响（梯度消失），所以想用一个记忆单元保存当前时刻的<strong>激活值</strong>，在后层计算的时候施加影响</p>
</li>
</ul>
<p>也就是通过门决定，当从左到右扫描一个句子的时候，这个时机是要更新某个记忆细胞，还是不更新，不更新直到真的需要使用记忆细胞的时候），这可能在句子之前就决定了。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519225757169.png?lastModify=1726287112" alt="image-20240519225757169" /></p>
<p>由于门 Γu 使用的是 <strong>sigmoid</strong> 函数，所以很容易取到 0，那么 c&lt;t&gt;=c&lt;&gt;t−1，有利于维持细胞的值。</p>
<ul>
<li>这样就不会有梯度消失的问题了。</li>
</ul>
<p>对于 c&lt;t&gt; 可以是一个向量</p>
<blockquote>
<p>例如：100 维的激活值 a&lt;t&gt;，那么 c&lt;t&gt; 也是 100 维的。</p>
</blockquote>
<ul>
<li>c&lt;t&gt;=Γu∗c~&lt;t&gt;+(1−Γu)∗c&lt;t−1&gt; 中的 ∗ 是元素对应的乘积</li>
</ul>
<blockquote>
<p>例如：在处理 &quot;cat eats&quot; 这个句子时，我们可以将主语 &quot;cat&quot; 的单复数信息存储在对应的维度上，同时将谈论的物品 &quot;eat&quot; 的单复数信息存储在另一个对应的维度上。</p>
<ul>
<li>如果后面出现了 &quot;dogs run&quot; 这样的短语，还可以将它们的单复数信息存储在相应的维度上。</li>
</ul>
<p>记忆细胞中有<strong>两个维度</strong>存放 cat 与 dogs 是否使用单复数</p>
</blockquote>
<ul>
<li>想要在后面使用到某一个激活值 a&lt;t&gt;，那么就用 c&lt;t&gt; 对应的维存储即可，在后续时间步中提取并使用该值。</li>
</ul>
<p>对于完整的 <strong>GRU</strong> 单元，改变就是</p>
<p>c<sub>&lt;t&gt;=tanh(wc[Γr∗ct−1,x]+bc)Γu=σ(wu[c&lt;t−1&gt;,xt]+bu)Γr=σ(wr[c&lt;t−1&gt;,xt]+br)c&lt;t&gt;=Γu∗c</sub>&lt;t&gt;+(1−Γu)∗c&lt;t−1&gt;</p>
<p>Γr 门：计算出的下一个 𝑐&lt;t&gt; 的候选值 𝑐̃&lt;𝑡&gt; 跟 𝑐&lt;𝑡−1 &gt; 有多大的<strong>相关性</strong>。</p>
<h2 id="长短期记忆lstmlong-short-term-memoryunit"><a class="anchor" href="#长短期记忆lstmlong-short-term-memoryunit">#</a> 长短期记忆（LSTM（long short term memory）unit）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240519234720084.png?lastModify=1726287112" alt="image-20240519234720084" /></p>
<p>最常用的版本可能是门值不仅取决于 𝑎&lt;𝑡−1&gt; 和 𝑥&lt;𝑡&gt;，有时候也可以偷窥一下 𝑐&lt;𝑡−1&gt; 的值</p>
<ul>
<li>Γu=σ(wu[c&lt;t−1&gt;,xt,ct−1]+bu)</li>
</ul>
<blockquote>
<p>“<strong>窥视孔连接</strong>”（peephole connection）</p>
</blockquote>
<p>比如有一 个 100 维的向量，你有一个 100 维的隐藏的记忆细胞单元，然后比如第 50 个 𝑐&lt;𝑡−1&gt; 的元素只会影响第 50 个元素对应的那个门，所以关系是<strong>一对一</strong>的</p>
<ul>
<li>相反的，第一个 𝑐&lt;𝑡−1&gt; 的元素只能影响门的第一个元素， 第二个元素影响对应的第二个元素</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240601150601132.png?lastModify=1726287112" alt="image-20240601150601132" /></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240601151127941.png?lastModify=1726287112" alt="image-20240601151127941" /></p>
<h2 id="双向循环神经网络bidirectional-rnn"><a class="anchor" href="#双向循环神经网络bidirectional-rnn">#</a> 双向循环神经网络（Bidirectional RNN）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521115613507.png?lastModify=1726287112" alt="image-20240521115613507" /></p>
<p>根据前 3 个单词是无法判断是 <strong>Teddy bears</strong> 还是 <strong>Teddy Roosevelt</strong></p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521120500761.png?lastModify=1726287112" alt="image-20240521120500761" /></p>
<blockquote>
<p>两个双向传播是独立的网络</p>
</blockquote>
<p>很多的 <strong>NLP</strong> 问题，对于大量有自然语言处理问题的文本， 有 <strong>LSTM</strong> 单元的双向 <strong>RNN</strong> 模型是用的最多的。</p>
<p>这个双向 <strong>RNN</strong> 网络模型的缺点就是你需要<strong>完整</strong>的数据的序列，你才能预测任意位置。</p>
<blockquote>
<p>比如说你要构建一个语音识别系统，那么双向 <strong>RNN</strong> 模型需要你考虑整个语音表达，但是如果直接用这个去实现的话，你需要等待这个人<strong>说完</strong>，然后获取整个语音表达才能处理这段语音，并进一 步做语音识别。</p>
</blockquote>
<h2 id="深层循环神经网络deep-rnns"><a class="anchor" href="#深层循环神经网络deep-rnns">#</a> 深层循环神经网络（Deep RNNs）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521121331661.png?lastModify=1726287112" alt="image-20240521121331661" /></p>
<h2 id="词汇表征word-representation"><a class="anchor" href="#词汇表征word-representation">#</a> 词汇表征（Word Representation）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521130433678.png?lastModify=1726287112" alt="image-20240521130433678" /></p>
<blockquote>
<p>O：表示 <strong>one-hot</strong></p>
</blockquote>
<p><strong>缺点</strong>：它把每个词孤立起来，这样使得算法对相关词的泛化能力不强。</p>
<ul>
<li>
<p>例如：上述的 <strong>apple juice</strong> 和 <strong>orange juice</strong>。</p>
<p>算法不知道 <strong>apple</strong> 和 <strong>orange</strong> 的关系很接近，就像 <strong>Man</strong> 与 <strong>Woman</strong></p>
<blockquote>
<p>因为它们之间的内积为 0，也就是线性无关。</p>
<p>很难区分它们之间的差别，因为这些向量内积都是一样的，所以无法知道 <strong>apple</strong> 和 <strong>orange</strong> 要比 <strong>king</strong> 和 <strong>orange</strong>，或者 <strong>queen</strong> 和 <strong>orange</strong> 相似地多。</p>
</blockquote>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521131332496.png?lastModify=1726287112" alt="image-20240521131332496" /></p>
<blockquote>
<p>假设有 300 个不同的特征</p>
</blockquote>
<ul>
<li>e9853： 300 维的向量用来表示 <strong>woman</strong> 这个词</li>
</ul>
<p>可以发现 <strong>apple</strong> 与 <strong>orange</strong> 这种表示非常<strong>相似</strong>，大部分特征都一样。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521132004660.png?lastModify=1726287112" alt="image-20240521132004660" /></p>
<p>如果能够学习到一个 300 维的特征向量，或者说 300 维的词嵌入，通常我们可以做一件事，把这 300 维的数据嵌入到一个二维空间里，这样就可以可视化了。</p>
<ul>
<li>常用的可视 化算法是 <strong>t-SNE</strong> 算法</li>
</ul>
<p>** 嵌入（embeddings）：**300 维空间里的特征表示</p>
<ul>
<li>在 300 维的空间中，<strong>apple</strong> 对应一个 300 维度 的特征向量（被嵌在该空间的一个点了。）</li>
</ul>
<p>为了可视化，<strong>t-SNE</strong> 算法把这个空间映射到低维空间</p>
<h2 id="使用词嵌入using-word-embeddings"><a class="anchor" href="#使用词嵌入using-word-embeddings">#</a> 使用词嵌入（Using Word Embeddings）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521133655921.png?lastModify=1726287112" alt="image-20240521133655921" /></p>
<p>用词嵌入作为输入<strong>训练好</strong>的模型，若有新的输入 <strong>Robert Lin is an apple farmer</strong></p>
<ul>
<li>因为知道 <strong>orange</strong> 与 <strong>apple</strong> 很接近，那么<strong>你的</strong>算法很容易就知道 <strong>Robert Lin</strong> 也是一个人，也是一个人的名字。</li>
</ul>
<p>如果不太常见怎么办？例如：<strong>“Robert Lin is a durian cultivator.</strong></p>
<p>只有一个很小的标记的训练集，你的训练集里甚至可能没有 <strong>durian</strong>（榴莲）或者 <strong>cultivator</strong>（培育家）这两个词。</p>
<p>但是如果你有一个<strong>已经学好</strong>的词嵌入， 它会告诉你 <strong>durian</strong>（榴莲）是水果，就像 <strong>orange</strong>（橙子）一样...</p>
<p>词嵌入能够达到这种效果，其中一个原因就是学习词嵌入的算法会考察<strong>非常大的文本集</strong>，也许是从网上找到的</p>
<ul>
<li>
<p>这样你可以考察很大的数据集可以是 1 亿个单词，甚至达到 100 亿也都是合理的，大量的无标签的文本的训练集。</p>
<p>通过考察大量的无标签文本，很多 都是可以免费下载的，你可以发现 <strong>orange</strong>（橙子）和 <strong>durian</strong>（榴莲）相近，<strong>farmer</strong>（农民） 和 <strong>cultivator</strong>（培育家）相近。</p>
</li>
</ul>
<p>尽管你只有一个很小的训练集，这就使得你可以使用<strong>迁移学习</strong>，把你从互联网上免费获得的大量的无标签文本中学习到的知识迁移到一个任务中，比如你只有少量标记的训练数据集的命名实体识别任务中。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521135139676.png?lastModify=1726287112" alt="image-20240521135139676" /></p>
<blockquote>
<p>当任务的训练集相对较小时，词嵌入的作用最明显</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521135948886.png?lastModify=1726287112" alt="image-20240521135948886" /></p>
<blockquote>
<p>词嵌入和人脸编码之间有奇妙的关系</p>
<ul>
<li>输入到输出为编码 / 嵌入</li>
</ul>
</blockquote>
<p>区别：<strong>人脸识别</strong>中的算法未来可能涉及到海量的人脸照片，而<strong>自然语言处理</strong>有一个固定的词汇表， 而像一些没有出现过的单词我们就记为未知单词。</p>
<h2 id="词嵌入的特性properties-of-word-embeddings"><a class="anchor" href="#词嵌入的特性properties-of-word-embeddings">#</a> 词嵌入的特性（Properties of Word Embeddings）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521141259331.png?lastModify=1726287112" alt="image-20240521141259331" /></p>
<p>假设用 eman 表示 <strong>man</strong></p>
<p>eman−ewoman≈[−2000]eking−equeen≈[−2000]</p>
<ul>
<li>
<p>结果表示，<strong>man</strong> 与 <strong>woman</strong> 主要是 <strong>gender</strong> 上的差异，<strong>king</strong> 和 <strong>queen</strong> 同理</p>
<p>这也就是为什么二者计算的结果是相等的。</p>
</li>
</ul>
<p>算法所做的就是计算 eman−ewoman，然后找出一个向量（一个词），使得 eman−ewoman≈eking−e?</p>
<ul>
<li>当新词是 <strong>queen</strong>，左边会近似地等于右边</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521142303542.png?lastModify=1726287112" alt="image-20240521142303542" /></p>
<blockquote>
<p><strong>sim</strong>：<strong>similarity</strong> 函数</p>
<ul>
<li>需要找到单词 <strong>w</strong> 最大化相似度</li>
</ul>
<p>通过这种方法来做类比推理准确率大概只有 30%~75%，只要算法猜中了单词，就把该次计算视为正确，从而计算出准确率</p>
</blockquote>
<p>对于单词可视化，之前采用的是 <strong>t-SNE</strong> 算法</p>
<ul>
<li>
<p>把这些 300 维的数据用一种<strong>非线性的方式</strong>映射到 2 维平面上，可以得知 <strong>t-SNE</strong> 中这种映射很复杂而且很非线性。</p>
<p>在进行 <strong>t-SNE</strong> 映射之后，你<strong>不能</strong>总是期望使等式成立的关系，会像左边那样成一个平行四边形</p>
</li>
</ul>
<p>在大多数情况下，由于 <strong>t-SNE</strong> 的非线性映射，你就没法再指望这种平行四边形了，很多这种平行四边形的类比关系在 <strong>t-SNE</strong> 映射中都会<strong>失去原貌</strong>。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521143244748.png?lastModify=1726287112" alt="image-20240521143244748" /></p>
<p>如果向量 𝑢 和 𝑣 非常相似，它 们的余弦相似性将接近 1;</p>
<p>如果它们不相似，则余弦相似性将取较小的值。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240521143255806.png?lastModify=1726287112" alt="image-20240521143255806" /></p>
<h2 id="嵌入矩阵embedding-matrix"><a class="anchor" href="#嵌入矩阵embedding-matrix">#</a> 嵌入矩阵（Embedding Matrix）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240522185524581.png?lastModify=1726287112" alt="image-20240522185524581" /></p>
<p><strong>E</strong>：嵌入矩阵，O6257：<strong>orange</strong> 对应的 <strong>one-hot</strong> 编码，e6257：<strong>orange</strong> 对应的嵌入向量</p>
<ul>
<li>e6257=E⋅O6257</li>
</ul>
<blockquote>
<p>用大量的矩阵和向量相乘来计算它，效率是很低下的， 因为 <strong>one-hot</strong> 向量是一个维度非常高的向量，并且几乎所有元素都是 0</p>
</blockquote>
<p>实际上，会使用一个专门的函数来单独查找矩阵 E 的某列，而不是用通常的矩阵乘法来做</p>
<h2 id="学习词嵌入learning-word-embeddings"><a class="anchor" href="#学习词嵌入learning-word-embeddings">#</a> 学习词嵌入（Learning Word Embeddings）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240522195424528.png?lastModify=1726287112" alt="image-20240522195424528" /></p>
<blockquote>
<p>使用神经网络来预测序列中的下一个单词</p>
</blockquote>
<p>将该 7 个 300 维的嵌入向量全部放进神经网络中，最后预测出 <strong>softmax</strong> 结果</p>
<p>实际上有一个固定的历史窗口</p>
<blockquote>
<p>例如：总是想预测给定 4（超参数）个单词后的下一个单词。</p>
<ul>
<li>如何适应长，段句子</li>
</ul>
<p>如果你一直使用一个 4 个词的历史窗口， 这就意味着你的神经网络会输入一个 1200 维的特征变量到这个层中， 然后再通过 <strong>softmax</strong> 来预测输出</p>
</blockquote>
<ul>
<li>
<p>可以处理任意长度的句子</p>
<blockquote>
<p>因为输入的维度总是固定的。</p>
</blockquote>
</li>
</ul>
<p>上述模型的参数就是矩阵 E</p>
<ul>
<li>
<p>所有的单词用的都是同一个矩阵 E</p>
<blockquote>
<p>可以用反向传播来进行梯度下降来最大化训练集似然函数</p>
</blockquote>
</li>
</ul>
<p>通过序列中给定的 4 个单词去重复地预测出语料库中下一个单词什么。</p>
<p>对于 <strong>apple juice</strong> 和 <strong>orange juice</strong>（训练集），后面是 <strong>juice</strong> 的概率都很高</p>
<ul>
<li>所以在梯度下降的 E 中，这些词的特征向量会非常的相似</li>
</ul>
<p>如果你要建立一个语言模型，那么一般选取目标词之前的几个词作为上下文。</p>
<p>但如果你的目标<strong>不是</strong>学习语言模型本身的话，那么你可以选择其他的上下文。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240522215150602.png?lastModify=1726287112" alt="image-20240522215150602" /></p>
<h2 id="word2vec"><a class="anchor" href="#word2vec">#</a> Word2Vec</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240522215618258.png?lastModify=1726287112" alt="image-20240522215618258" /></p>
<p>在 <strong>Skip-Gram</strong> 模型中，我们要做的是<strong>抽取</strong>上下文和目标词配对，来构造一个监督学习问题。</p>
<ul>
<li>
<p>下文不一定总是目标单词之前离得最近的四个单词，或最近的 𝑛 个单词。</p>
<p>我们要的做的是随机选一个词作为上下文词</p>
<blockquote>
<p>例如上述 <strong>orange</strong>，随机在词前后 5 个词内或者前后 10 个词内，这个范围内选择目标词</p>
</blockquote>
</li>
</ul>
<p>于是它给定上下文词，要求你<strong>预测</strong>在这个词正负 10 个词距或者正负 5 个词距内随机选择的某个目标词。</p>
<p>构造上述监督学习问题的目标并不是想要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个好的词嵌入模型。</p>
<p><strong>Skip-Gram</strong> 模型：<strong>生成</strong>词嵌入</p>
<blockquote>
<p>它使用一个中心单词来预测其周围的上下文单词</p>
<p>过这种方式学习到单词的词嵌入表示。</p>
</blockquote>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240522220840752.png?lastModify=1726287112" alt="image-20240522220840752" /></p>
<p>Oc→E→ec→o⏟softmax→y^</p>
<p><strong>Softmax:</strong>  p(t|c)=eθcTec∑j=110,000eθjTec</p>
<blockquote>
<p>省略了偏差项</p>
</blockquote>
<ul>
<li>
<p>θt：与输出 t 有关的参数（<strong>神经网络的参数矩阵</strong>）</p>
<blockquote>
<p>即某个词 t 和标签相符的概率是多少</p>
</blockquote>
</li>
</ul>
<p>L(y<sup>,y)=−∑i=110,000yilog⁡yi</sup></p>
<blockquote>
<p>y,y^：<strong>one-hot</strong> 表示</p>
</blockquote>
<p>实际上使用这个算法会遇到一些问题，首要的问题就是<strong>计算速度。</strong></p>
<ul>
<li>尤其是在 <strong>softmax</strong> 模型中，每次你想要计算这个概率，你需要对你词汇表中的所有 10,000 个词做求和计算</li>
</ul>
<p>有一些解决方案，如分级（hierarchical）的 <strong>softmax</strong> 分类器和<strong>负采样</strong>（Negative  Sampling）。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240522223750788.png?lastModify=1726287112" alt="image-20240522223750788" /></p>
<blockquote>
<p>如果你有一个分类器（左图），它告诉你目标词是在词汇表的前 5000 个中还是在词汇表的后 5000 个词中， 假如这个二分类器告诉你这个词在前 5000 个词中，然后第二个分类器会告诉你这个词在词汇表的前 2500 个词中，或者在词汇表的第二组 2500 个词中...</p>
</blockquote>
<p>在实践中一般使用哈夫曼树</p>
<ul>
<li>树上的每一个节点都可以是一个二分类器或者 <strong>logistic</strong> 回归分类器</li>
</ul>
<blockquote>
<p>常用的在浅层，不常用的在深层（durian）</p>
</blockquote>
<p>如何对 <strong>c</strong> 进行采样，一旦你对上下文 <strong>c</strong> 进行采样，那么目标词 <strong>t</strong> 就会在上下文 <strong>c</strong> 的正负 10 个词距内进行采样。 但是你要如何选择上下文 <strong>c</strong>？</p>
<ul>
<li>
<p>随机采样</p>
<blockquote>
<p>像 the、of、a、and、to 诸如此类是出现得相当频繁，orange、apple 或 durian 就不会那么频繁地出现了。</p>
</blockquote>
<p>可能不会想要你的训练集都是这些出现得很频繁的词，因为这会导致你花大部分的力气来更新这些<strong>频繁</strong>出现的单词的 ec</p>
<p>但你想要的是花时间来更新像 <strong>durian</strong> 这些更少出现的词的嵌入，即 edurian</p>
</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240522225619646.png?lastModify=1726287112" alt="image-20240522225619646" /></p>
<blockquote>
<p><strong>CBOW</strong> 是从原始语句推测目标字词；</p>
<p>而 <strong>Skip-Gram</strong> 正好相反，是从目标字词推测出原始语句</p>
</blockquote>
<h2 id="负采样"><a class="anchor" href="#负采样">#</a> 负采样</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523160549971.png?lastModify=1726287112" alt="image-20240523160549971" /></p>
<blockquote>
<p>构造一个<strong>新</strong>的监督学习问题，那么问题就是给定一对单词， 比如 <strong>orange</strong> 和 <strong>juice</strong>，我们要去预测这是否是一对上下文词 - 目标词（<strong>context-target</strong>）。</p>
</blockquote>
<p>对于上下文词 <strong>orange</strong>：</p>
<ol>
<li>
<p>从上下文中抽取一个词为<strong>正样本</strong>（1）</p>
</li>
<li>
<p>然后剩余从词汇表中抽取为<strong>负样本</strong>（0）</p>
<blockquote>
<p>无论是否在其中</p>
</blockquote>
</li>
</ol>
<p>学习算法输入 x，预测目标的标签 y</p>
<blockquote>
<p>因此问题就是给定一对词，像 orange 和 juice</p>
<ul>
<li>你觉得它们会一起出现么？</li>
<li>你觉得这两个词是通过对靠近的两个词采样获得的吗？</li>
<li>或者你觉得我是分别在文本和字典中随机选取得到的？</li>
</ul>
</blockquote>
<p>这个算法就是要分辨这<strong>两种不同</strong>的采样方式，这就是如何生成训练集的方法。</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523162154214.png?lastModify=1726287112" alt="image-20240523162154214" /></p>
<p>定义一个逻辑回归模型：P (y=1|c,t)=σ(θtTec)</p>
<blockquote>
<p>用这个公式估计 y=1 的概率</p>
</blockquote>
<p>每一个正样本都有 K 个对应的负样本来训练一个 <a href="test%5Ctest.py">test.py</a> 类似逻辑回归的模型。</p>
<p>若输入词是 <strong>orange</strong>，那么就得到了 10,000 个可能的逻辑回归分类问题。</p>
<blockquote>
<p>诸如此类，<strong>预测</strong>词汇表中这些可能的单词。把这些看作 10,000 个二分类逻辑回归分类器</p>
</blockquote>
<p>但只训练其中的 5 个</p>
<ul>
<li>只需更新 K+1 个逻辑单元，K+1 个二分类问题，相对而言每次迭代的成本比更新 10,000 维的 <strong>softmax</strong> 分类器成本低。</li>
</ul>
<blockquote>
<p>训练对应真正目标词那一个分类器，再训练 4 个随机选取的负样本</p>
</blockquote>
<ul>
<li>所以不使用一个巨大的 10,000 维度的 <strong>softmax</strong>，因为计算成本很高，而是把它转变为 10,000 个<strong>二分类</strong>问题，每个都很容易计算</li>
</ul>
<p>每次训练针对的是 K+1 个预测，其中 K 是负样本的数量，加上一个正样本。</p>
<ul>
<li>相比之下，传统的 <strong>softmax</strong> 分类器需要同时处理整个词汇表的 10,000 个预测，这会导致计算成本非常高昂。</li>
</ul>
<blockquote>
<p>通过将问题转化为多个二分类问题，每次训练只需处理 K+1 个预测，而不是整个词汇表的预测。</p>
</blockquote>
<p>对于负样本的采样</p>
<ol>
<li>通过词出现的频率进行采样</li>
<li>随机抽取</li>
</ol>
<p>而一般采用中间的方法</p>
<p>P(wi)=f(wi)34∑j=110,000f(wj)34</p>
<p>进行采样</p>
<ul>
<li>f (wi) ：观测到的在语料库中的某个英文词的词频</li>
</ul>
<h2 id="glove-词向量glove-word-vectors"><a class="anchor" href="#glove-词向量glove-word-vectors">#</a> GloVe 词向量（GloVe Word Vectors） ？</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523173957676.png?lastModify=1726287112" alt="image-20240523173957676" /></p>
<p>定义 Xij：单词 i 在单词 j 上下文中出现的次数（共现次数）</p>
<blockquote>
<p>Xij=Xtc</p>
</blockquote>
<ul>
<li>也可以遍历你的训练集，然后 数出单词𝑖在不同单词𝑗上下文中出现的个数，单词 𝑡 在不同单词 𝑐 的上下文中共出现多少次。</li>
</ul>
<p>根据上下文和目标词的定义 Xij 可能会等于 Xji</p>
<ul>
<li>如果你将上下文 和目标词的范围定义为出现于左右各 10 词以内的话，那么就会有一种对称关系。</li>
<li>上下文总是目标词前一个单词的话，那么就不会像这样对称了。</li>
</ul>
<p>将他们之间的差距进行最小化处理：</p>
<p>minimize ∑i=110,000∑j=110,000f(Xij)(θiTej+bi+bj′−log⁡Xij)2</p>
<ul>
<li>
<p>θiTej 和 θtTec 功能一样</p>
<blockquote>
<p>告诉 i 与 j 的联系程度，也就是同时出现的频率</p>
<ul>
<li>由 Xij 影响</li>
</ul>
</blockquote>
</li>
</ul>
<p>对于加权项 f (Xij)</p>
<ol>
<li>
<p>若 Xij=0，那么 log⁡Xij 负无穷大，约定 0log⁡0=0 （先不要进行求和）</p>
<p>log⁡0 就是不相关项</p>
</li>
<li>
<p>对于一些共现次数较高的词对，给予更高的权重，而对于共现次数较低的词对，则给予更低的权重。这样做的目的是为了在计算损失函数时，更多地关注那些出现频率较高的词对，因为它们对模型的训练影响更大。</p>
</li>
</ol>
<h2 id="情感分类sentiment-classification"><a class="anchor" href="#情感分类sentiment-classification">#</a> 情感分类（Sentiment Classification）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523182151015.png?lastModify=1726287112" alt="image-20240523182151015" /></p>
<blockquote>
<p>如果你能训练一个从 𝑥 到 𝑦 的映射，基于这样的标记的数据集，那么你就可以用来搜集 大家对你运营的餐馆的评价。</p>
</blockquote>
<p>情感分类一个最大的挑战就是可能标记的训练集没有那么多。</p>
<ul>
<li>词嵌入能够带来更好的效果，尤其是只有很小的训练集时。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523182948940.png?lastModify=1726287112" alt="image-20240523182948940" /></p>
<blockquote>
<p>求和或者平均它们</p>
</blockquote>
<p>但有个问题：没考虑次序</p>
<blockquote>
<p>例如上述的负面评价</p>
</blockquote>
<ul>
<li><strong>good</strong> 出现了很多次，忽略词序，仅仅把所有单词的词嵌入加起来或者平均下来，你最后的特征向量会有很多 <strong>good</strong> 的表示，你的分类器很可能认为这是一个好的评论，尽管事实上这是一个差评，只有一星的评价。</li>
</ul>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523183432156.png?lastModify=1726287112" alt="image-20240523183432156" /></p>
<p>若你训练一个这样的算法，最后会得到一个很合适的情感分类的算法。由于你的词嵌入是在一个更大的数据集里训练的，这样效果会更好，更好的泛化一些没有见过的新的单词。</p>
<blockquote>
<p>比如其他人可能会说，&quot;Completely absent of good taste, good service, and good  ambiance.&quot;，</p>
<ul>
<li>
<p>即使 absent 这个词不在标记的训练集里，如果是在一亿或者一百亿单词集里训练词嵌入</p>
<p>它仍然可以正确判断，并且泛化的很好</p>
<p>甚至这些词是在训练集中用于训练词嵌入的，但是可以不在专门用来做情感分类问题的标记的训练集中。</p>
</li>
</ul>
</blockquote>
<blockquote>
<p>一旦你学习到或者从网上下载词嵌入，你就可以很快构建一个很有效的 NLP 系统。</p>
</blockquote>
<h2 id="词嵌入除偏debiasing-word-embeddings"><a class="anchor" href="#词嵌入除偏debiasing-word-embeddings">#</a> 词嵌入除偏（Debiasing Word Embeddings）</h2>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523185008007.png?lastModify=1726287112" alt="image-20240523185008007" /></p>
<p>因此根据训练模型所使用的文本，词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见</p>
<p><img loading="lazy" data-src="https://gitcode.net/qq_67720621/typora/-/raw/master/typora-user-images-new/image-20240523192536249.png?lastModify=1726287112" alt="image-20240523192536249" /></p>
<blockquote>
<p>偏见趋势（1 D），无偏见趋势 （299 D）</p>
</blockquote>
<ol>
<li>
<p>找出想要消除的偏见特征维度</p>
<blockquote>
<p>对于性别歧视这种情况，能做的 ehe−eshe ... 取平均</p>
<ul>
<li>实际上它会用一个更加复杂的算法叫做 SVU（奇异值分解）</li>
</ul>
</blockquote>
</li>
<li>
<p>将其与所有需要消除偏见的词汇在该维度上中心化</p>
<ul>
<li>对于那些定义不确切的词可以将其处理一下，避免偏见。</li>
</ul>
<blockquote>
<p>例如：<strong>doctor，babysitter</strong> （性取向中立等等）</p>
<p>减少或是消除他们的性别歧视趋势的成分，也就是说减少他们在这个<strong>水平</strong>方向上的距离</p>
<hr />
<p><img loading="lazy" data-src="https://cdn.kesci.com/upload/image/q213u8msks.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>
</blockquote>
</li>
<li>
<p>均衡步</p>
<blockquote>
<p>例如 <strong>grandmother</strong> 与 <strong>grandfather</strong></p>
<p><strong>babysitter</strong> 和 <strong>grandmother</strong> 之间的距离或者说是相似度实际上是小于 <strong>babysitter</strong> 和 <strong>grandfather</strong> 之间的，</p>
<ul>
<li>因此这可能会加重不良状态，或者可能是非预期的偏见，</li>
</ul>
<p>也就是说 <strong>grandmother</strong> 相比于 <strong>grandfather</strong> 最终更有可能输出 <strong>babysitting</strong>。</p>
<hr />
<p>主要做的的就是将 <strong>grandmother，grandfather</strong>  移至与中间轴线等距的一对点上</p>
<ul>
<li>现在性别歧视的影响也就是这两个词与 <strong>babysitter</strong> 的距离就完全相同了</li>
</ul>
<hr />
<p><img loading="lazy" data-src="https://cdn.kesci.com/upload/image/q213xc9vkc.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>
</blockquote>
</li>
<li>
<p>手工找出那些不需要消除偏见的词汇，不进行处理</p>
</li>
</ol>
<div class="tags"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i>深度学习</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i></span><span class="text">更新于</span><time title="修改时间：2024-09-14 12:20:01" itemprop="dateModified" datetime="2024-09-14T12:20:01+08:00">2024-09-14</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i>赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img loading="lazy" data-src="/assets/wechatpay.png" alt="htired 微信支付"/><p>微信支付</p></div><div><img loading="lazy" data-src="/assets/alipay.png" alt="htired 支付宝"/><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>htired：</strong>htired<i class="ic i-at"><em>@</em></i>何必要叹气呢？</li><li class="link"><strong>本文链接：</strong><a href="https://www.htired.top/2024/09/14/deeplearning/deeplearning/" title="deeplearning">https://www.htired.top/2024/09/14/deeplearning/deeplearning/</a></li><li class="license"><strong>版权声明：</strong>本站所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/09/10/algorithm/%E7%88%AC%E6%A5%BC%E6%A2%AF/" rel="prev" itemprop="url" data-background-image="https:&#x2F;&#x2F;github.com&#x2F;htired&#x2F;MyPic&#x2F;blob&#x2F;main&#x2F;img&#x2F;4fc551ee880a11ebb6edd017c2d2eca2.jpg?raw&#x3D;true" title="爬楼梯"><span class="type">上一篇</span><span class="category"><i class="ic i-flag"></i>入门 dp</span><h3>爬楼梯</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text"> 深度学习概论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text"> 什么是神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.</span> <span class="toc-text"> 用神经网络进行监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%85%B4%E8%B5%B7"><span class="toc-number">1.3.</span> <span class="toc-text"> 深度学习为什么会兴起</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9B%A0%E4%B8%80"><span class="toc-number">1.3.1.</span> <span class="toc-text"> 原因一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9B%A0%E4%BA%8C"><span class="toc-number">1.3.2.</span> <span class="toc-text"> 原因二</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E7%A8%8B%E5%A4%A7%E7%BA%B2"><span class="toc-number">1.4.</span> <span class="toc-text"> 课程大纲</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">2.</span> <span class="toc-text"> 神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E5%88%86%E7%B1%BB"><span class="toc-number">2.1.</span> <span class="toc-text"> 二分分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.1.</span> <span class="toc-text"> 概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7"><span class="toc-number">2.1.2.</span> <span class="toc-text"> 符号</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-%E5%9B%9E%E5%BD%92"><span class="toc-number">2.2.</span> <span class="toc-text"> logistic 回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.</span> <span class="toc-text"> logistic 回归损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.0.1.</span> <span class="toc-text"> 损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.0.2.</span> <span class="toc-text"> 成本函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">2.4.</span> <span class="toc-text"> 梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">2.5.</span> <span class="toc-text"> 计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE%E6%B1%82%E5%AF%BC"><span class="toc-number">2.6.</span> <span class="toc-text"> 使用计算图求导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">2.7.</span> <span class="toc-text"> logistic 回归中的梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#m-%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.8.</span> <span class="toc-text"> m 个样本的梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96vectorization"><span class="toc-number">2.9.</span> <span class="toc-text"> 向量化（Vectorization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E7%9A%84%E6%9B%B4%E5%A4%9A%E4%BE%8B%E5%AD%90"><span class="toc-number">2.10.</span> <span class="toc-text"> 向量化的更多例子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%82%E5%92%8C"><span class="toc-number">2.10.1.</span> <span class="toc-text"> 求和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E6%95%B0"><span class="toc-number">2.10.2.</span> <span class="toc-text"> 指数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logistic-%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.10.3.</span> <span class="toc-text"> logistic 回归的梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96-logistic-%E5%9B%9E%E5%BD%92"><span class="toc-number">2.11.</span> <span class="toc-text"> 向量化 logistic 回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96-logistic-%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%BE%93%E5%87%BA"><span class="toc-number">2.12.</span> <span class="toc-text"> 向量化 logistic 回归的梯度输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#python-%E4%B8%AD%E7%9A%84%E5%B9%BF%E6%92%AD"><span class="toc-number">2.13.</span> <span class="toc-text"> Python 中的广播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-python-_-numpy-%E5%90%91%E9%87%8F%E7%9A%84%E8%AF%B4%E6%98%8E"><span class="toc-number">2.14.</span> <span class="toc-text"> 关于 python _ numpy 向量的说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">2.15.</span> <span class="toc-text"> logistic 损失函数的解释</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text"> 浅层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.</span> <span class="toc-text"> 神经网络概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.2.</span> <span class="toc-text"> 神经网络表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%93%E5%87%BA"><span class="toc-number">3.3.</span> <span class="toc-text"> 计算一个神经网络的输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E6%A0%B7%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">3.4.</span> <span class="toc-text"> 多样本向量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">3.5.</span> <span class="toc-text"> 向量化实现的解释</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.6.</span> <span class="toc-text"> 激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.7.</span> <span class="toc-text"> 为什么需要非线性激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-number">3.8.</span> <span class="toc-text"> 激活函数的导数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid-activation-function"><span class="toc-number">3.8.1.</span> <span class="toc-text"> sigmoid activation function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tanh-activation-function"><span class="toc-number">3.8.2.</span> <span class="toc-text"> Tanh activation function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rectified-linear-unit-relu"><span class="toc-number">3.8.3.</span> <span class="toc-text"> Rectified Linear Unit (ReLU)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#leaky-linear-unit-leaky-relu"><span class="toc-number">3.8.4.</span> <span class="toc-text"> Leaky linear unit (Leaky ReLU)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9F%A9%E9%98%B5%E9%93%BE%E5%BC%8F%E6%B1%82%E5%AF%BC%E8%B0%83%E6%95%B4%E7%BB%B4%E5%BA%A6"><span class="toc-number">3.9.</span> <span class="toc-text"> 神经网络的梯度下降 (矩阵链式求导调整维度)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.10.</span> <span class="toc-text"> 直观理解反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#logistic-regression"><span class="toc-number">3.10.1.</span> <span class="toc-text"> Logistic regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E5%8F%8C%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%B4%E5%BA%A6%E5%8C%B9%E9%85%8D"><span class="toc-number">3.10.2.</span> <span class="toc-text"> 对于双层神经网络（维度匹配）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0-%E9%99%A4%E4%BB%A5-m-%E8%A7%A3%E9%87%8A"><span class="toc-number">3.10.3.</span> <span class="toc-text"> 向量化实现 (除以 M 解释)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.11.</span> <span class="toc-text"> 随机初始化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text"> 深层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-2"><span class="toc-number">4.1.</span> <span class="toc-text"> 深层神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.2.</span> <span class="toc-text"> 深层网络中的前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%AF%B9%E7%9F%A9%E9%98%B5%E7%9A%84%E7%BB%B4%E6%95%B0"><span class="toc-number">4.3.</span> <span class="toc-text"> 核对矩阵的维数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E4%B8%AA%E6%A0%B7%E6%9C%AC"><span class="toc-number">4.3.1.</span> <span class="toc-text"> 单个样本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">4.3.2.</span> <span class="toc-text"> 向量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.3.3.</span> <span class="toc-text"> 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.4.</span> <span class="toc-text"> 为什么使用深层表示？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97"><span class="toc-number">4.5.</span> <span class="toc-text"> 搭建神经网络块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.6.</span> <span class="toc-text"> 前向传播和后向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#forward-propagation"><span class="toc-number">4.6.1.</span> <span class="toc-text"> Forward propagation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#backward-propagation"><span class="toc-number">4.6.2.</span> <span class="toc-text"> Backward propagation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#summary"><span class="toc-number">4.6.3.</span> <span class="toc-text"> Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0-vs-%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">4.7.</span> <span class="toc-text"> 参数 VS 超参数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.</span> <span class="toc-text"> 改善深层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%AA%8C%E8%AF%81%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">5.1.</span> <span class="toc-text"> 训练，验证，测试集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E4%B8%83%E5%88%86"><span class="toc-number">5.1.1.</span> <span class="toc-text"> 三七分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE"><span class="toc-number">5.1.2.</span> <span class="toc-text"> 大数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-number">5.1.3.</span> <span class="toc-text"> 总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D"><span class="toc-number">5.1.4.</span> <span class="toc-text"> 分布不匹配</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE"><span class="toc-number">5.2.</span> <span class="toc-text"> 偏差、方差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%AF%E5%B7%AE%E6%9C%80%E4%BC%98%E8%AF%AF%E5%B7%AE"><span class="toc-number">5.2.1.</span> <span class="toc-text"> 贝叶斯误差（最优误差）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-number">5.2.2.</span> <span class="toc-text"> 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">5.3.</span> <span class="toc-text"> 机器学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.4.</span> <span class="toc-text"> 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#logistic-%E5%9B%9E%E5%BD%92-2"><span class="toc-number">5.4.1.</span> <span class="toc-text"> logistic 回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.4.2.</span> <span class="toc-text"> 神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E6%9C%89%E5%88%A9%E4%BA%8E%E9%A2%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88%E5%91%A2"><span class="toc-number">5.5.</span> <span class="toc-text"> 为什么正则化有利于预防过拟合呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.6.</span> <span class="toc-text"> dropout 正则化？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%A7%A3-dropout"><span class="toc-number">5.7.</span> <span class="toc-text"> 理解 dropout</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">5.8.</span> <span class="toc-text"> 其他正则化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%89%A9%E5%A2%9E"><span class="toc-number">5.8.1.</span> <span class="toc-text"> 数据扩增</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#early-stopping"><span class="toc-number">5.8.2.</span> <span class="toc-text"> early stopping</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#early-stopping-%E7%BC%BA%E7%82%B9"><span class="toc-number">5.8.2.1.</span> <span class="toc-text"> early stopping 缺点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">5.8.2.2.</span> <span class="toc-text"> 优点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5normalizing-inputs"><span class="toc-number">5.9.</span> <span class="toc-text"> 归一化输入（Normalizing inputs）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8vanishing-exploding-gradients"><span class="toc-number">5.10.</span> <span class="toc-text"> 梯度消失 &#x2F; 梯度爆炸（Vanishing &#x2F; Exploding gradients）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96weight-initialization-for-deep-networks"><span class="toc-number">5.11.</span> <span class="toc-text"> 神经网络的权重初始化（Weight Initialization for Deep  Networks）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#xavier-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">5.11.1.</span> <span class="toc-text"> Xavier 初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91numerical-approximation-of-gradients"><span class="toc-number">5.12.</span> <span class="toc-text"> 梯度的数值逼近（Numerical approximation of gradients）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8Cgradient-checking"><span class="toc-number">5.13.</span> <span class="toc-text"> 梯度检验（Gradient checking）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E5%BA%94%E7%94%A8%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9gradient-checking-implementation-notes"><span class="toc-number">5.14.</span> <span class="toc-text"> 梯度检验应用的注意事项（Gradient Checking  Implementation Notes）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">5.15.</span> <span class="toc-text"> Mini-batch 梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%A7%A3-mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">5.16.</span> <span class="toc-text"> 理解 mini-batch 梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8Flearning-rate-decay"><span class="toc-number">5.17.</span> <span class="toc-text"> 学习率衰减 (Learning rate decay)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E6%95%B0exponentially-weighted-averages"><span class="toc-number">5.18.</span> <span class="toc-text"> 指数加权平均数（Exponentially weighted averages）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E6%95%B0understanding-exponentially-weighted-averages"><span class="toc-number">5.19.</span> <span class="toc-text"> 理解指数加权平均数（Understanding exponentially  weighted averages）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E7%9A%84%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3bias-correction-in-exponentially-weighted-averages"><span class="toc-number">5.20.</span> <span class="toc-text"> 指数加权平均的偏差修正（Bias correction in  exponentially weighted averages）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95gradient-descent-with-momentum"><span class="toc-number">5.21.</span> <span class="toc-text"> 动量梯度下降法（Gradient descent with Momentum）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rmsprop"><span class="toc-number">5.22.</span> <span class="toc-text"> RMSprop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#adam-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95adam-optimization-algorithm"><span class="toc-number">5.23.</span> <span class="toc-text"> Adam 优化算法 (Adam optimization algorithm)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E7%9A%84%E9%97%AE%E9%A2%98the-problem-of-local-optima"><span class="toc-number">5.24.</span> <span class="toc-text"> 局部最优的问题 (The problem of local optima)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E8%AF%95%E5%A4%84%E7%90%86tuning-process"><span class="toc-number">5.25.</span> <span class="toc-text"> 调试处理（Tuning process）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E8%B6%85%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%8C%83%E5%9B%B4using-an-appropriate-scale-to-pick-hyperparameters"><span class="toc-number">5.26.</span> <span class="toc-text"> 为超参数选择合适的范围（Using an appropriate scale to  pick hyperparameters）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E5%AE%9E%E8%B7%B5pandas-vs-caviarhyperparameters-tuning-in-practice-pandas-vs-caviar"><span class="toc-number">5.27.</span> <span class="toc-text"> 超参数调试实践：Pandas VS Caviar（Hyperparameters  tuning in practice: Pandas vs. Caviar）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0normalizing-activations-in-a-network"><span class="toc-number">5.28.</span> <span class="toc-text"> 归一化网络的激活函数（Normalizing activations in a  network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86-batch-norm-%E6%8B%9F%E5%90%88%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cfitting-batch-norm-into-a-neural-network"><span class="toc-number">5.29.</span> <span class="toc-text"> 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into  a neural network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#batch-norm-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A5%8F%E6%95%88why-does-batch-norm-work"><span class="toc-number">5.30.</span> <span class="toc-text"> Batch Norm 为什么奏效？（Why does Batch Norm work?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84-batch-normbatch-norm-at-test-time"><span class="toc-number">5.31.</span> <span class="toc-text"> 测试时的 Batch Norm（Batch Norm at test time）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax-%E5%9B%9E%E5%BD%92softmax-regression"><span class="toc-number">5.32.</span> <span class="toc-text"> Softmax 回归（Softmax regression）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA-softmax-%E5%88%86%E7%B1%BB%E5%99%A8training-a-softmax-classifier"><span class="toc-number">5.33.</span> <span class="toc-text"> 训练一个 Softmax 分类器（Training a Softmax classifier）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6deep-learning-frameworks"><span class="toc-number">5.34.</span> <span class="toc-text"> 深度学习框架（Deep Learning frameworks）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorflow"><span class="toc-number">5.35.</span> <span class="toc-text"> TensorFlow</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9"><span class="toc-number">6.</span> <span class="toc-text"> 结构化机器学习项</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AFml%E7%AD%96%E7%95%A5"><span class="toc-number">6.1.</span> <span class="toc-text"> 为什么是 ML 策略？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E4%BA%A4%E5%8C%96orthogonalization"><span class="toc-number">6.2.</span> <span class="toc-text"> 正交化（Orthogonalization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E4%B8%80%E6%95%B0%E5%AD%97%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87single-number-evaluation-metric"><span class="toc-number">6.3.</span> <span class="toc-text"> 单一数字评估指标（Single number evaluation metric）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BB%A1%E8%B6%B3%E5%92%8C%E4%BC%98%E5%8C%96%E6%8C%87%E6%A0%87satisficing-and-optimizing-metrics"><span class="toc-number">6.4.</span> <span class="toc-text"> 满足和优化指标（Satisficing and optimizing metrics）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BC%80%E5%8F%91%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%92%E5%88%86traindevtest-distributions"><span class="toc-number">6.5.</span> <span class="toc-text"> 训练 &#x2F; 开发 &#x2F; 测试集划分（Train&#x2F;dev&#x2F;test distributions）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E5%A4%A7%E5%B0%8Fsize-of-dev-and-test-sets"><span class="toc-number">6.6.</span> <span class="toc-text"> 开发集和测试集的大小（Size of dev and test sets）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E8%AF%A5%E6%94%B9%E5%8F%98%E5%BC%80%E5%8F%91%E6%B5%8B%E8%AF%95%E9%9B%86%E5%92%8C%E6%8C%87%E6%A0%87"><span class="toc-number">6.7.</span> <span class="toc-text"> 什么时候该改变开发 &#x2F; 测试集和指标？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E7%9A%84%E8%A1%A8%E7%8E%B0"><span class="toc-number">6.8.</span> <span class="toc-text"> 为什么是人的表现？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E9%81%BF%E5%85%8D%E5%81%8F%E5%B7%AEavoidable-bias"><span class="toc-number">6.9.</span> <span class="toc-text"> 可避免偏差（Avoidable bias）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86-%E8%A7%A3-%E4%BA%BA-%E7%9A%84-%E8%A1%A8-%E7%8E%B0-understanding-human-level-performance"><span class="toc-number">6.10.</span> <span class="toc-text"> 理 解 人 的 表 现 （ Understanding human-level  performance）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E8%BF%87%E4%BA%BA%E7%9A%84%E8%A1%A8%E7%8E%B0surpassing-human-level-performance"><span class="toc-number">6.11.</span> <span class="toc-text"> 超过人的表现（Surpassing human- level performance）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B9%E5%96%84%E4%BD%A0%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A1%A8%E7%8E%B0improving-your-model-performance"><span class="toc-number">6.12.</span> <span class="toc-text"> 改善你的模型的表现（Improving your model  performance）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E8%A1%8C%E9%94%99%E8%AF%AF%E5%88%86%E6%9E%90carrying-out-error-analysis"><span class="toc-number">6.13.</span> <span class="toc-text"> 进行错误分析（Carrying out error analysis）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B8%85%E9%99%A4%E6%A0%87%E6%B3%A8%E9%94%99%E8%AF%AF%E7%9A%84%E6%95%B0%E6%8D%AEcleaning-up-incorrectly-labeled-data"><span class="toc-number">6.14.</span> <span class="toc-text"> 清除标注错误的数据（Cleaning up Incorrectly labeled  data）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86"><span class="toc-number">6.14.1.</span> <span class="toc-text"> 训练集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">6.14.2.</span> <span class="toc-text"> 开发集和测试集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%B3%BB%E7%BB%9F%E5%B9%B6%E8%BF%9B%E8%A1%8C%E8%BF%AD%E4%BB%A3"><span class="toc-number">6.15.</span> <span class="toc-text"> 快速搭建你的第一个系统，并进行迭代</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E5%88%92%E5%88%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%B9%B6%E6%B5%8B%E8%AF%95"><span class="toc-number">6.16.</span> <span class="toc-text"> 在不同的划分上进行训练并测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%B8%8D%E5%8C%B9%E9%85%8D%E6%97%B6%E7%9A%84%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE%E7%9A%84%E5%88%86%E6%9E%90bias-and-variance-with-mismatched-data-distributions"><span class="toc-number">6.17.</span> <span class="toc-text"> 数据分布不匹配时的偏差与方差的分析（Bias and  Variance with mismatched data distributions）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98addressing-data-mismatch"><span class="toc-number">6.18.</span> <span class="toc-text"> 处理数据不匹配问题（Addressing data mismatch）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0transfer-learning"><span class="toc-number">6.19.</span> <span class="toc-text"> 迁移学习（Transfer learning）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0multi-task-learning"><span class="toc-number">6.20.</span> <span class="toc-text"> 多任务学习（Multi-task learning）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0what-is-end-to-end-deep-learning"><span class="toc-number">6.21.</span> <span class="toc-text"> 什么是端到端的深度学习？（What is end-to-end deep  learning?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%AF%E5%90%A6%E8%A6%81%E4%BD%BF%E7%94%A8%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0whether-to-use-end-to-end-learning"><span class="toc-number">6.22.</span> <span class="toc-text"> 是否要使用端到端的深度学习？（Whether to use end to-end learning?）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cconvolutional-neural-networks"><span class="toc-number">7.</span> <span class="toc-text"> 卷积神经网络（Convolutional  Neural Networks）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89computer-vision"><span class="toc-number">7.1.</span> <span class="toc-text"> 计算机视觉（Computer vision）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E7%A4%BA%E4%BE%8Bedge-detection-example"><span class="toc-number">7.2.</span> <span class="toc-text"> 边缘检测示例（Edge detection example）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E5%A4%9A%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E5%86%85%E5%AE%B9more-edge-detection"><span class="toc-number">7.3.</span> <span class="toc-text"> 更多边缘检测内容（More edge detection）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#padding"><span class="toc-number">7.4.</span> <span class="toc-text"> Padding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%AD%A5%E9%95%BFstrided-convolutions"><span class="toc-number">7.5.</span> <span class="toc-text"> 卷积步长（Strided convolutions）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E7%BB%B4%E5%8D%B7%E7%A7%AFconvolutions-over-volumes"><span class="toc-number">7.6.</span> <span class="toc-text"> 三维卷积（Convolutions over volumes）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9Cone-layer-of-a-convolutional-network"><span class="toc-number">7.7.</span> <span class="toc-text"> 单层卷积网络（One layer of a convolutional network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%A4%BA%E4%BE%8Ba-simple-convolution-network-example"><span class="toc-number">7.8.</span> <span class="toc-text"> 简单卷积网络示例（A simple convolution network  example）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82pooling-layers"><span class="toc-number">7.9.</span> <span class="toc-text"> 池化层（Pooling layers）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A4%BA%E4%BE%8Bconvolutional-neural-network-example"><span class="toc-number">7.10.</span> <span class="toc-text"> 卷积神经网络示例（Convolutional neural network  example）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AFwhy-convolutions"><span class="toc-number">7.11.</span> <span class="toc-text"> 为什么使用卷积？（Why convolutions?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6"><span class="toc-number">7.12.</span> <span class="toc-text"> 为什么要进行实例探究？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9Cclassic-networks"><span class="toc-number">7.13.</span> <span class="toc-text"> 经典网络（Classic networks）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lenet-5"><span class="toc-number">7.13.1.</span> <span class="toc-text"> LeNet-5</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#alexnet"><span class="toc-number">7.13.2.</span> <span class="toc-text"> AlexNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vgg-16"><span class="toc-number">7.13.3.</span> <span class="toc-text"> VGG-16</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C"><span class="toc-number">7.14.</span> <span class="toc-text"> 残差网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E7%94%A8why-resnets-work"><span class="toc-number">7.15.</span> <span class="toc-text"> 残差网络为什么有用？（Why ResNets work?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E4%BB%A5%E5%8F%8A-11-%E5%8D%B7%E7%A7%AFnetwork-in-network-and-11-convolutions"><span class="toc-number">7.16.</span> <span class="toc-text"> 网络中的网络以及 1×1 卷积（Network in Network and  1×1 convolutions）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%B7%E6%AD%8C-inception-%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8Binception-network-motivation"><span class="toc-number">7.17.</span> <span class="toc-text"> 谷歌 Inception 网络简介（Inception network motivation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#inception-%E7%BD%91%E7%BB%9Cinception-network"><span class="toc-number">7.18.</span> <span class="toc-text"> Inception 网络（Inception network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0transfer-learning-2"><span class="toc-number">7.19.</span> <span class="toc-text"> 迁移学习（Transfer Learning）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BAdata-augmentation"><span class="toc-number">7.20.</span> <span class="toc-text"> 数据增强（Data augmentation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%8E%B0%E7%8A%B6the-state-of-computer-vision"><span class="toc-number">7.21.</span> <span class="toc-text"> 计算机视觉现状（The state of computer vision）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%AE%9A%E4%BD%8Dobject-localization"><span class="toc-number">7.22.</span> <span class="toc-text"> 目标定位（Object localization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%82%B9%E6%A3%80%E6%B5%8Blandmark-detection"><span class="toc-number">7.23.</span> <span class="toc-text"> 特征点检测（Landmark detection）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8Bobject-detection"><span class="toc-number">7.24.</span> <span class="toc-text"> 目标检测（Object detection）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0convolutional-implementation-of-sliding-windows"><span class="toc-number">7.25.</span> <span class="toc-text"> 滑动窗口的卷积实现（Convolutional implementation of  sliding windows）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bounding-box-%E9%A2%84%E6%B5%8Bbounding-box-predictions"><span class="toc-number">7.26.</span> <span class="toc-text"> Bounding Box 预测（Bounding box predictions）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%B9%B6%E6%AF%94intersection-over-union"><span class="toc-number">7.27.</span> <span class="toc-text"> 交并比（Intersection over union）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6non-max-suppression"><span class="toc-number">7.28.</span> <span class="toc-text"> 非极大值抑制（Non-max suppression）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#anchor-boxes"><span class="toc-number">7.29.</span> <span class="toc-text"> Anchor Boxes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-%E7%AE%97%E6%B3%95putting-it-together-yolo-algorithm"><span class="toc-number">7.30.</span> <span class="toc-text"> YOLO 算法（Putting it together: YOLO algorithm）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9Fregion-proposals"><span class="toc-number">7.31.</span> <span class="toc-text"> 候选区域（Region proposals）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB"><span class="toc-number">7.32.</span> <span class="toc-text"> 什么是人脸识别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#one-shot-%E5%AD%A6%E4%B9%A0one-shot-learning"><span class="toc-number">7.33.</span> <span class="toc-text"> One-Shot 学习（One-shot learning）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#siamese-%E7%BD%91%E7%BB%9Csiamese-network"><span class="toc-number">7.34.</span> <span class="toc-text"> Siamese 网络（Siamese network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#triplet-%E6%8D%9F%E5%A4%B1triplet-%E6%8D%9F%E5%A4%B1"><span class="toc-number">7.35.</span> <span class="toc-text"> Triplet 损失（Triplet 损失）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E8%84%B8%E9%AA%8C%E8%AF%81%E4%B8%8E%E4%BA%8C%E5%88%86%E7%B1%BBface-verification-and-binary-classification"><span class="toc-number">7.36.</span> <span class="toc-text"> 人脸验证与二分类（Face verification and binary  classification）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BBwhat-is-neural-style-transfer"><span class="toc-number">7.37.</span> <span class="toc-text"> 什么是神经风格迁移？（What is neural style transfer?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E4%BB%80%E4%B9%88what-are-deep-convnets-learning"><span class="toc-number">7.38.</span> <span class="toc-text"> 深度卷积网络学习什么？（What are deep ConvNets  learning?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0cost-function"><span class="toc-number">7.39.</span> <span class="toc-text"> 代价函数（Cost function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AE%B9%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0content-cost-function"><span class="toc-number">7.40.</span> <span class="toc-text"> 内容代价函数（Content cost function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A3%8E%E6%A0%BC%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0style-cost-function"><span class="toc-number">7.41.</span> <span class="toc-text"> 风格代价函数（Style cost function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E7%BB%B4%E5%88%B0%E4%B8%89%E7%BB%B4%E6%8E%A8%E5%B9%BF1d-and-3d-generalizations-of-models"><span class="toc-number">7.42.</span> <span class="toc-text"> 一维到三维推广（1D and 3D generalizations of models）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8Bsequence-models"><span class="toc-number">8.</span> <span class="toc-text"> 序列模型 (Sequence Models)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8Bwhy-sequence-models"><span class="toc-number">8.1.</span> <span class="toc-text"> 为什么选择序列模型？（Why Sequence Models?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7notation"><span class="toc-number">8.2.</span> <span class="toc-text"> 数学符号（Notation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">8.3.</span> <span class="toc-text"> 循环神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADbackpropagation-through-time"><span class="toc-number">8.4.</span> <span class="toc-text"> 通过时间的反向传播（Backpropagation through time）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cdifferent-types-of-rnns"><span class="toc-number">8.5.</span> <span class="toc-text"> 不同类型的循环神经网络（Different types of RNNs）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90language-model-and-sequence-generation"><span class="toc-number">8.6.</span> <span class="toc-text"> 语言模型和序列生成（Language model and sequence  generation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%96%B0%E5%BA%8F%E5%88%97%E9%87%87%E6%A0%B7sampling-novel-sequences"><span class="toc-number">8.7.</span> <span class="toc-text"> 对新序列采样（Sampling novel sequences）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1vanishing-gradients-with-rnns"><span class="toc-number">8.8.</span> <span class="toc-text"> 循环神经网络的梯度消失（Vanishing gradients with  RNNs）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gru-%E5%8D%95%E5%85%83gated-recurrent-unitgru"><span class="toc-number">8.9.</span> <span class="toc-text"> GRU 单元（Gated Recurrent Unit（GRU）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86lstmlong-short-term-memoryunit"><span class="toc-number">8.10.</span> <span class="toc-text"> 长短期记忆（LSTM（long short term memory）unit）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cbidirectional-rnn"><span class="toc-number">8.11.</span> <span class="toc-text"> 双向循环神经网络（Bidirectional RNN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cdeep-rnns"><span class="toc-number">8.12.</span> <span class="toc-text"> 深层循环神经网络（Deep RNNs）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E6%B1%87%E8%A1%A8%E5%BE%81word-representation"><span class="toc-number">8.13.</span> <span class="toc-text"> 词汇表征（Word Representation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%AF%8D%E5%B5%8C%E5%85%A5using-word-embeddings"><span class="toc-number">8.14.</span> <span class="toc-text"> 使用词嵌入（Using Word Embeddings）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%9A%84%E7%89%B9%E6%80%A7properties-of-word-embeddings"><span class="toc-number">8.15.</span> <span class="toc-text"> 词嵌入的特性（Properties of Word Embeddings）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E7%9F%A9%E9%98%B5embedding-matrix"><span class="toc-number">8.16.</span> <span class="toc-text"> 嵌入矩阵（Embedding Matrix）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%AF%8D%E5%B5%8C%E5%85%A5learning-word-embeddings"><span class="toc-number">8.17.</span> <span class="toc-text"> 学习词嵌入（Learning Word Embeddings）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#word2vec"><span class="toc-number">8.18.</span> <span class="toc-text"> Word2Vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="toc-number">8.19.</span> <span class="toc-text"> 负采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#glove-%E8%AF%8D%E5%90%91%E9%87%8Fglove-word-vectors"><span class="toc-number">8.20.</span> <span class="toc-text"> GloVe 词向量（GloVe Word Vectors） ？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BBsentiment-classification"><span class="toc-number">8.21.</span> <span class="toc-text"> 情感分类（Sentiment Classification）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E5%B5%8C%E5%85%A5%E9%99%A4%E5%81%8Fdebiasing-word-embeddings"><span class="toc-number">8.22.</span> <span class="toc-text"> 词嵌入除偏（Debiasing Word Embeddings）</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li  class="active"><a href="/2024/09/14/deeplearning/deeplearning/" rel="bookmark" title="deeplearning">deeplearning</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><img class="image" loading="lazy" decoding="async" itemprop="image" alt="htired" src="/assets/avatar.jpg"/><p class="name" itemprop="name">htired</p><div class="description" itemprop="description">送君南浦，伤如之何？</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">57</span><span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">6</span><span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">41</span><span class="name">标签</span></a></div></nav><div class="social"><a target="_blank" rel="noopener" href="https://github.com/yourname" class="item github" title="https:&#x2F;&#x2F;github.com&#x2F;yourname"><i class="ic i-github"></i></a></div><div class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="#" onclick="return false;"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li><li class="item"><a href="/collect/" rel="section"><i class="ic i-shoucang"></i>收藏</a></li></div></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/09/10/algorithm/%E7%88%AC%E6%A5%BC%E6%A2%AF/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/os/" title="分类于操作系统">操作系统</a></div><span><a href="/2023/06/02/os/5%E3%80%81%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86/">5、设备管理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/05/08/algorithm/%E5%89%8D%E5%BE%80%E7%9B%AE%E6%A0%87%E7%9A%84%E6%9C%80%E5%B0%8F%E4%BB%A3%E4%BB%B7/">前往目标的最小代价</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/06/01/algorithm/%E6%8C%89%E5%85%AC%E5%9B%A0%E6%95%B0%E8%AE%A1%E7%AE%97%E6%9C%80%E5%A4%A7%E7%BB%84%E4%BB%B6%E5%A4%A7%E5%B0%8F/">按公因数计算最大组件大小</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/10/05/algorithm/%E8%80%83%E7%A0%94%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">考研数据结构</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/05/31/algorithm/%E7%9F%A9%E9%98%B5%E4%B8%AD%E4%B8%A5%E6%A0%BC%E9%80%92%E5%A2%9E%E7%9A%84%E5%8D%95%E5%85%83%E6%A0%BC%E6%95%B0/">矩阵中严格递增的单元格数</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/06/01/algorithm/%E5%9F%83%E5%BC%8F%E7%AD%9B-%E5%B8%A6%E9%98%88%E5%80%BC%E7%9A%84%E5%9B%BE%E8%BF%9E%E9%80%9A%E6%80%A7/">埃式筛-带阈值的图连通性</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/06/08/algorithm/%E7%A7%BB%E6%8E%89-K-%E4%BD%8D%E6%95%B0%E5%AD%97/">移掉 K 位数字</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/05/29/algorithm/%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%AE%9E%E5%8A%9B%E5%80%BC/">一个小组的最大实力值</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/06/03/algorithm/%E4%B9%98%E6%B3%95%E9%80%86%E5%85%83%EF%BC%8C%E8%B4%B9%E9%A9%AC%E5%B0%8F%E5%AE%9A%E7%90%86%EF%BC%8C%E5%B9%B6%E6%9F%A5%E9%9B%86%EF%BC%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E5%B0%86%E5%AD%90%E6%95%B0%E7%BB%84%E9%87%8D%E6%96%B0%E6%8E%92%E5%BA%8F%E5%BE%97%E5%88%B0%E5%90%8C%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E6%96%B9%E6%A1%88%E6%95%B0/">乘法逆元，费马小定理， 并查集， 动态规划-将子数组重新排序得到同一个二叉搜索树的方案数</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm/" title="分类于算法">算法</a></div><span><a href="/2023/05/06/algorithm/%E5%B0%86%E6%95%B0%E7%BB%84%E6%B8%85%E7%A9%BA/">将数组清空</a></span></li></ul></div><div class="rpost pjax"><h2>最新评论</h2><ul class="leancloud-recent-comment" id="new-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2022 -<span itemprop="copyrightYear">2024</span><span class="with-love"><i class="ic i-sakura rotate"></i></span><span class="author" itemprop="copyrightHolder">htired @ Htired Love</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i></span><span title="站点总字数">835k 字</span><span class="post-meta-divider"> | </span><span class="post-meta-item-icon"><i class="ic i-coffee"></i></span><span title="站点阅读时长">12:39</span></div><div class="powered-by">基于 <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & Theme.<a target="_blank" rel="noopener" href="https://github.com/theme-shoka-x/hexo-theme-shokaX/">ShokaX</a></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL = {
    ispost: true,
        path: `2024/09/14/deeplearning/deeplearning/`,
        favicon: {
        show: `好久不见`,
        hide: `顺其自然`
    },
    search: {
        placeholder: "文章搜索",
        empty: "关于 「 ${query} 」，什么也没搜到",
        stats: "${time} ms 内找到 ${hits} 条结果"
    },
    copy_tex: true,
    katex: true,
    mermaid: false,
    audio: undefined,
    fancybox: true,
    nocopy: false,
    outime: true,
    template: `<div class="note warning"><p><span class="label warning">文章时效性提示</span><br>这是一篇发布于 {{publish}} 天前，最后一次更新在 {{updated}} 天前的文章，部分信息可能已经发生改变，请注意甄别。</p></div>`,
    quiz: {
        choice: `单选题`,
        multiple: `多选题`,
        true_false: `判断题`,
        essay: `问答题`,
        gap_fill: `填空题`,
        mistake: `错题备注`
    },
    ignores: [
        (uri) => uri.includes('#'),
        (uri) => new RegExp(LOCAL.path + '$').test(uri),
            []
    ]
};
</script><script src="https://s4.zstatic.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha384-k6YtvFUEIuEFBdrLKJ3YAUbBki333tj1CSUisai5Cswsg9wcLNaPzsTHDswp4Az8" crossorigin="anonymous" fetchpriority="high"></script><script src="https://s4.zstatic.net/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha384-ZvpUoO&#x2F;+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn&#x2F;6Z&#x2F;hRTt8+pR6L4N2" crossorigin="anonymous" fetchpriority="high"></script><script src="/js/siteInit.js?v=0.4.11" type="module" fetchpriority="high" defer></script></body></html>